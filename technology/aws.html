
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Amazon AWS - KnowledgeShop</title>
  <meta name="author" content="Mohamed Fizal Ihsan Mohamed">


<meta name="description" content="Amazon AWS AWS Fundamentals Storage and Content Delivery Service Storage Basics S3 (Simple Storage Service) S3 Overview S3 Buckets S3 Objects S3 &hellip;">
<meta name="keywords" content="big data, data science, mongodb, nosql, R, statistics">

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://fizalihsan.github.io/technology/aws.html">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <!-- Below custom CSS is for table border styling -->
  <link href="/stylesheets/data-table.css" media="screen, projection" rel="stylesheet" type="text/css" />

  <link href="" rel="alternate" title="KnowledgeShop" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">


<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
tex2jax: {
  inlineMath: [ ['$', '$'], ["\\(","\\)"] ],
  displayMath: [ ['$$', '$$'], ["\\[","\\]"] ],
  processEscapes: true,
  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
},
messageStyle: "none",
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript" /></script>

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-58187831-1']);
    _gaq.push(['_setDomainName','github.io']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">KnowledgeShop</a></h1>
  
    <h2>Learn & Share</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:fizalihsan.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  


<ul class="main-navigation">
  <li><a href="/">Home</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>


</nav>
  <div id="main">
    <div id="content">
      <div>
<article role="article">
  
  <header>
    <h1 class="entry-title">Amazon AWS</h1>
    
  </header>
  
  <ul id="markdown-toc">
  <li><a href="#aws-fundamentals">AWS Fundamentals</a></li>
  <li><a href="#storage-and-content-delivery-service">Storage and Content Delivery Service</a>    <ul>
      <li><a href="#storage-basics">Storage Basics</a></li>
      <li><a href="#s3-simple-storage-service">S3 (Simple Storage Service)</a>        <ul>
          <li><a href="#s3-overview">S3 Overview</a></li>
          <li><a href="#s3-buckets">S3 Buckets</a></li>
          <li><a href="#s3-objects">S3 Objects</a></li>
          <li><a href="#s3-durability--availability">S3 Durability &amp; Availability</a></li>
          <li><a href="#s3-storage-classes">S3 Storage Classes</a></li>
          <li><a href="#s3-data-protection">S3 Data Protection</a></li>
          <li><a href="#advanced-topics">Advanced Topics</a></li>
        </ul>
      </li>
      <li><a href="#amazon-glacier">Amazon Glacier</a></li>
      <li><a href="#amazon-elastic-block-store-ebs">Amazon Elastic Block Store (EBS)</a>        <ul>
          <li><a href="#ebs-volume-types">EBS Volume Types</a></li>
          <li><a href="#ebs-snapshots">EBS Snapshots</a></li>
        </ul>
      </li>
      <li><a href="#aws-storage-gateway">AWS Storage Gateway</a></li>
      <li><a href="#amazon-cloudfront">Amazon CloudFront</a></li>
    </ul>
  </li>
  <li><a href="#compute-and-network-services">Compute and Network Services</a>    <ul>
      <li><a href="#ec2">EC2</a>        <ul>
          <li><a href="#instance-types">Instance Types</a></li>
          <li><a href="#securing-an-instance">Securing an Instance</a></li>
          <li><a href="#lifecycle-of-instances">Lifecycle of Instances</a></li>
          <li><a href="#instance-pricing-options">Instance Pricing Options</a></li>
          <li><a href="#tenancy-options">Tenancy options</a></li>
        </ul>
      </li>
      <li><a href="#aws-lambda">AWS Lambda</a></li>
      <li><a href="#auto-scaling">Auto Scaling</a>        <ul>
          <li><a href="#auto-scaling-plans">Auto Scaling Plans</a></li>
          <li><a href="#auto-scaling-components">Auto Scaling Components</a></li>
        </ul>
      </li>
      <li><a href="#elastic-load-balancing-elb">Elastic Load Balancing (ELB)</a>        <ul>
          <li><a href="#types-of-load-balancers">Types of Load Balancers</a>            <ul>
              <li><a href="#internet-facing-load-balancers">Internet-Facing Load Balancers</a></li>
              <li><a href="#internal-load-balancers">Internal Load Balancers</a></li>
              <li><a href="#https-load-balancers">HTTPS Load Balancers</a></li>
              <li><a href="#listeners">Listeners</a></li>
            </ul>
          </li>
          <li><a href="#elb-configurations">ELB Configurations</a>            <ul>
              <li><a href="#idle-connection-timeout">Idle Connection Timeout</a></li>
              <li><a href="#cross-zone-load-balancing">Cross-Zone Load Balancing</a></li>
              <li><a href="#connection-draining">Connection Draining</a></li>
              <li><a href="#proxy-protocol">Proxy Protocol</a></li>
              <li><a href="#sticky-sessions">Sticky Sessions</a></li>
              <li><a href="#health-checks">Health Checks</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#aws-elastic-beanstalk">AWS Elastic Beanstalk</a></li>
      <li><a href="#aws-virtual-private-cloud-vpc">AWS Virtual Private Cloud (VPC)</a>        <ul>
          <li><a href="#subnets">Subnets</a></li>
          <li><a href="#route-tables">Route Tables</a></li>
          <li><a href="#igw-internet-gateways">IGW (Internet Gateways)</a></li>
          <li><a href="#dhcp">DHCP</a></li>
          <li><a href="#eips-elastic-ip-addresses">EIPs (Elastic IP Addresses)</a></li>
          <li><a href="#enis-elastic-network-interfaces">ENIs (Elastic Network Interfaces)</a></li>
          <li><a href="#endpoints">Endpoints</a></li>
          <li><a href="#peering">Peering</a></li>
          <li><a href="#security-groups">Security Groups</a></li>
          <li><a href="#network-acls-access-control-lists">Network ACLs (Access Control Lists)</a></li>
          <li><a href="#nat-instances-network-address-translation">NAT Instances (Network Address Translation)</a></li>
          <li><a href="#nat-gateways">NAT Gateways</a></li>
          <li><a href="#vpgs-cgws-and-vpns">VPGs, CGWs and VPNs</a></li>
        </ul>
      </li>
      <li><a href="#aws-direct-connect">AWS Direct Connect</a></li>
      <li><a href="#amazon-route-53">Amazon Route 53</a>        <ul>
          <li><a href="#supported-record-types">Supported Record Types</a></li>
          <li><a href="#route-53-enables-resiliency">Route 53 Enables Resiliency</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#database-service">Database service</a>    <ul>
      <li><a href="#amazon-relational-database-service-rds">Amazon Relational Database Service (RDS)</a></li>
      <li><a href="#amazon-redshift">Amazon Redshift</a></li>
      <li><a href="#amazon-dynamodb">Amazon DynamoDB</a>        <ul>
          <li><a href="#writing-and-reading-data">Writing and Reading Data</a></li>
        </ul>
      </li>
      <li><a href="#amazon-elasticcache">Amazon ElasticCache</a></li>
    </ul>
  </li>
  <li><a href="#management-tools">Management Tools</a>    <ul>
      <li><a href="#amazon-cloudwatch">Amazon CloudWatch</a></li>
      <li><a href="#aws-cloudformation">AWS CloudFormation</a></li>
      <li><a href="#aws-cloudtrail">AWS CloudTrail</a></li>
      <li><a href="#aws-config">AWS Config</a></li>
    </ul>
  </li>
  <li><a href="#security-and-identity">Security and Identity</a>    <ul>
      <li><a href="#aws-identity-and-access-manager-iam">AWS Identity and Access Manager (IAM)</a>        <ul>
          <li><a href="#principals">Principals</a></li>
          <li><a href="#authentication">Authentication</a></li>
          <li><a href="#authorization">Authorization</a></li>
          <li><a href="#other-key-features">Other Key Features</a></li>
        </ul>
      </li>
      <li><a href="#aws-key-management-service-kms">AWS Key Management Service (KMS)</a></li>
      <li><a href="#aws-directory-service">AWS Directory Service</a></li>
      <li><a href="#aws-certificate-manager">AWS Certificate Manager</a></li>
      <li><a href="#aws-web-application-firewall-waf">AWS Web Application Firewall (WAF)</a></li>
    </ul>
  </li>
  <li><a href="#application-services">Application Services</a>    <ul>
      <li><a href="#amazon-api-gateway">Amazon API Gateway</a></li>
      <li><a href="#amazon-elastic-transcoder">Amazon Elastic Transcoder</a></li>
      <li><a href="#amazon-simple-notification-service-sns">Amazon Simple Notification Service (SNS)</a></li>
      <li><a href="#amazon-simple-email-service-ses">Amazon Simple Email Service (SES)</a></li>
      <li><a href="#amazon-simple-workflow-service-swf">Amazon Simple Workflow Service (SWF)</a></li>
      <li><a href="#amazon-simple-queue-service-sqs">Amazon Simple Queue Service (SQS)</a></li>
    </ul>
  </li>
  <li><a href="#terminology">Terminology</a></li>
  <li><a href="#references">References</a></li>
</ul>

<p><img src="/technology/aws-services.png" /></p>

<h1 id="aws-fundamentals">AWS Fundamentals</h1>

<ul>
  <li><strong>Cloud Computing - Deployment Models</strong>
    <ul>
      <li><em>All-in cloud-based application</em> is fully deployed in the cloud, with all components of the app running in the cloud</li>
      <li><em>Hybrid deployment</em> is a common approach taken by many enterprises that connects infrastructure and apps between cloud-based resources and existing resources, typically in an existing data center.</li>
    </ul>
  </li>
</ul>

<p><img class="right" src="/technology/aws-regions-availability-zones.jpg" /></p>

<ul>
  <li><strong>Global Infrastructure</strong>
    <ul>
      <li>Each <em>region</em> is a separate geographic area.</li>
      <li>Each region has multiple, isolated locations known as <em>Availability Zones</em>.</li>
      <li>Resources aren’t replicated across regions unless organizations choose to do so.</li>
      <li>For fault tolerance and stability, each region is completely isolated from each other.</li>
      <li>Each Availability Zone is also isolated, but they are connected through low-latency links.</li>
      <li>Each Availability Zone is located in lower-risk flood plains using a discrete UPS and on-site backup generators. They are each fed via different grids from independent utilities to reduce single points of failures further.</li>
      <li>Organizations retain complete control and ownership over the region in which their data is physically located, allowed them to meet regional compliance and data residency requirements.</li>
      <li>www.cloudping.info - to estimate the latency from your browser to each AWS region</li>
    </ul>
  </li>
  <li><strong>Accessing the platform</strong>
    <ul>
      <li>via Amazon console</li>
      <li>via CLI</li>
      <li>via SDK</li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="storage-and-content-delivery-service">Storage and Content Delivery Service</h1>

<h2 id="storage-basics">Storage Basics</h2>

<ul>
  <li><strong>Traditional Storage Types</strong>
    <ul>
      <li><strong><em>Block storage</em></strong>:
        <ul>
          <li>operates at low-level - the raw storage device level - and manages data as a set of numbered, fixed-size blocks</li>
          <li>accessed over a network in the form of a <em>Storage Area Network (SAN)</em> using protocols such as iSCSI or Fiber channel</li>
          <li>very closely associated with the server and the OS</li>
          <li>E.g., Amazon Elastic Block Storage (EBS)</li>
        </ul>
      </li>
      <li><strong><em>File storage</em></strong>:
        <ul>
          <li>operates at higher-level - the operating system level - and manages data as a named hierarchy of files and folders</li>
          <li>accessed over a network as a <em>Network Attached Storage (NAS)</em> file server or “filer” using protocols such as <em>Common Internet File System (CIFS)</em> or <em>Network File System (NFS)</em></li>
          <li>very closely associated with the server and the OS</li>
          <li>E.g., AWS Elastic File System (EFS)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Cloud Storage
    <ul>
      <li><strong><em>Object storage</em></strong>
        <ul>
          <li>Independent of a server and is accessed over the Internet.</li>
          <li>Instead of managing data as blocks or files using SCSI, CIFS or NFS protocols, data is managed as objects using REST API</li>
          <li>E.g., Amazon S3</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="s3-simple-storage-service">S3 (Simple Storage Service)</h2>

<h3 id="s3-overview">S3 Overview</h3>

<ul>
  <li>S3 is a highly-durable and highly-scalable cloud object store</li>
  <li>By default, every object is world-readable</li>
  <li>Scalability: S3 automatically partitions buckets to support very high request rates and simultaneous access by many clients</li>
</ul>

<h3 id="s3-buckets">S3 Buckets</h3>

<ul>
  <li>Buckets are a simple flat folder with no file system hierarchy.</li>
  <li>Limitations
    <ul>
      <li>It cannot have a sub-bucket within a bucket.</li>
      <li>Bucket name: max 63 lowercase letters, numbers, hyphens and periods</li>
      <li>Each bucket can hold an unlimited number of objects.</li>
      <li>Max 100 buckets per account.</li>
    </ul>
  </li>
</ul>

<h3 id="s3-objects">S3 Objects</h3>

<ul>
  <li>Objects reside in containers called <em>buckets</em></li>
  <li>An object can virtually store any kind of data in any formats</li>
  <li>Max object size = 5TB</li>
  <li>Each object has
    <ul>
      <li>data (the file itself)</li>
      <li>metadata - set of name/value pairs
        <ul>
          <li><em>System metadata</em> - created and used by Amazon S3. e.g., last modified date, MD5, object size, etc</li>
          <li><em>User metadata</em> - optional</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Object Key</strong>
    <ul>
      <li>Each object is identified by a unique <em>key</em> (similar to a file name).</li>
      <li>Combination of <code>bucket + key + optional version ID</code> uniquely identifies an S3 object</li>
      <li>Limitations
        <ul>
          <li>Max 1024 bytes of UTF-8 characters (including slashes, backslashes, dots, dashes)</li>
          <li>Keys must be unique within a single bucket</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>You cannot incrementally update portions of the object as you would with a file</li>
  <li><strong>ARNs (Amazon Resource Name)</strong> uniquely identify AWS resources. (http://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html)
    <ul>
      <li>Object in an Amazon S3 bucket <code>arn:aws:s3:::my_bucket/exampleobject.png</code></li>
      <li>General formats
        <ul>
          <li><code>arn:partition:service:region:account-id:resource</code></li>
          <li><code>arn:partition:service:region:account-id:resourcetype/resource</code></li>
          <li><code>arn:partition:service:region:account-id:resourcetype:resource</code></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="s3-durability--availability">S3 Durability &amp; Availability</h3>

<ul>
  <li><em>Durability</em>
    <ul>
      <li>answers the question <em>will my data still be there in the future?</em></li>
      <li>S3 provides 99.999999999% durability - meaning for 10K objects stored, you can on average expect to incur a loss of 1 object every 10,000,000 years</li>
      <li>Durability achieved by
        <ul>
          <li>automatically storing data redundantly on multiple devices in multiple facilities within a region.</li>
          <li>design to sustain concurrent loss of data in 2 facilities without loss of user data</li>
          <li>highly durable infrastructure</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><em>Availability</em>
    <ul>
      <li>answers the question <em>can I access my data right now?</em></li>
      <li>S3 provides 99.99% availability</li>
      <li>S3 is an <em>eventually consistent</em> system
        <ul>
          <li>S3 offers <em>read-after-write</em> consistency for <code>PUT</code>s to new objects. <em>Read-after-write</em> guarantees immediate visibility of new data to all clients.</li>
          <li><code>PUT</code>s to existing objects and <code>DELETE</code>s are offered at <em>eventual consistency</em></li>
          <li>Updates to a single key are atomic - for eventual-consistency reads, you will get the new data or the old data, but never an inconsistent mix of data.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="s3-storage-classes">S3 Storage Classes</h3>

<ul>
  <li><em>Hot</em> - frequently accessed data</li>
  <li><em>Warm</em> - less frequently accessed data as it ages</li>
  <li><em>Cold</em> - long-term backup or archive data before eventual deletion</li>
</ul>

<table>
  <thead>
    <tr>
      <th> </th>
      <th><strong>S3 Standard</strong></th>
      <th><strong>S3 Standard - Infrequent Access (IA)</strong></th>
      <th><strong>Reduced Redundancy Storage (RRS)</strong></th>
      <th><strong>Amazon Glacier</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Well-suited for</td>
      <td>short-term or long-term storage of frequently accessed data</td>
      <td>long-lived, less frequently accessed data that is stored for longer than 30 days</td>
      <td>derived data that can be easily reproduced (e.g., thumbnails)</td>
      <td>data that does not require real-time access, such as archives and long-term backups,<br />where a retrieval time of several hours is suitable</td>
    </tr>
    <tr>
      <td>Offers</td>
      <td>high durability (99.999999999%),<br />high availability,<br />high performance,<br />first low-byte latency,<br />high throughput</td>
      <td>same durability,<br />low latency,<br />high throughput</td>
      <td>slightly lower durability (99.9999%)</td>
      <td>secure,<br />high durability (99.999999999%)</td>
    </tr>
    <tr>
      <td>Cost</td>
      <td> </td>
      <td>Has lower per GB-month cost than Standard<br />Price model also includes a minimum object size (128KB), minimum duration (30 days), and per-GB retrieval costs.</td>
      <td>Reduced cost than Standard or Standard-IA</td>
      <td>Extremely low-cost</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>Reduce storage costs by automatically transitioning data from one storage class to another or eventually deleting data after a period of time. For example,
    <ul>
      <li>Store backup data initially in S3 Standard</li>
      <li>After 30 days, move to Standard-IA</li>
      <li>After 90 days, transition to Glacier,</li>
      <li>After 3 years, delete</li>
    </ul>
  </li>
  <li>Lifecycle configurations are attached to bucket or specific objects</li>
</ul>

<h3 id="s3-data-protection">S3 Data Protection</h3>

<ul>
  <li>Bucket-level permissioning - existing permissioning policies can be copied or generated using ‘Bucket policy generator’</li>
  <li>By default, all S3 objects and buckets are private and can only be accessed by the owner.</li>
  <li><strong>Access Control</strong>
    <ul>
      <li><em>Coarse-grained access controls</em>
        <ul>
          <li><strong>S3 Access Control Lists (ACLs)</strong>
            <ul>
              <li>legacy mechanism, created before IAM</li>
              <li>Limited to read, write, or full-control at object/bucket level</li>
              <li>Best used for enabling bucket logging or making a bucket that hosts a static website be readable</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><em>Fine-grained access controls</em>
        <ul>
          <li><strong>S3 bucket policies</strong>
            <ul>
              <li>Recommended access control mechanism</li>
              <li>Can specify
                <ul>
                  <li>who can access the bucket</li>
                  <li>from where (by <em>CIDR</em> block or IP address)</li>
                  <li>at what time of the day</li>
                </ul>
              </li>
              <li>Bucket policies allows to assign cross-account access to S3 resources</li>
              <li>Similar to IAM but
                <ul>
                  <li>associated with bucket resource instead of an IAM principal</li>
                  <li>includes an explicit reference to the IAM principal in the policy. This principal can be associated with a different AWS account</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>AWS Identity and Access Management (IAM) policies</li>
          <li>Query String Authentication</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Encryption</strong>
    <ul>
      <li>Encrypt data in-flight
        <ul>
          <li>Transmit data using HTTPS protocol to SSL API endpoints</li>
        </ul>
      </li>
      <li>Encrypt data at rest
        <ul>
          <li><strong><em>Server-Side Encryption (SSE)</em></strong>
            <ul>
              <li>S3 encrypts data at the object level as it writes it to disks and decrypts it for you when you access it</li>
              <li><em>SSE-S3 (AWS Managed Keys)</em>
                <ul>
                  <li>AWS handles the key management and key protection</li>
                  <li>Every object is encrypted with a unique key</li>
                  <li>The object encryption key is then encrypted using a separate master key</li>
                  <li>A new master key is issued at least monthly with AWS rotating the keys.</li>
                  <li>Encrypted data, keys and master keys are all stored separately on secure hosts</li>
                </ul>
              </li>
              <li><em>SSE-KMS (AWS KMS Keys)</em>
                <ul>
                  <li>AWS handles management and protection of <em>your</em> key</li>
                  <li>SSE-KMS offers additional benefits
                    <ul>
                      <li>provides audit of who used the key to access, which object, when.</li>
                      <li>Allows to view failed attempts to access data from users who did not have permissions</li>
                    </ul>
                  </li>
                </ul>
              </li>
              <li><em>SSE-C (Customer Provided Keys)</em>
                <ul>
                  <li>You maintain your own encryption keys, but don’t want to manage or implement your own client-side encryption library</li>
                  <li>AWS will do the encryption/decryption of your objects while you maintain full control of the keys</li>
                </ul>
              </li>
            </ul>
          </li>
          <li><strong><em>Client-Side Encryption (CSE)</em></strong>
            <ul>
              <li>You encrypt the data before sending it to S3</li>
              <li>2 options for using data encryption keys
   	* Use an AWS KMS-managed customer master key
                <ul>
                  <li>Use a client-side master key</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Versioning</strong>
    <ul>
      <li>If an object is accidentally changed/deleted, one can restore the object to its original state by referencing the version ID in addition to the bucket and object key</li>
      <li>Versioning is turned on at the bucket level. Once enabled, versioning cannot be removed from a bucket; it can only be suspended.</li>
    </ul>
  </li>
  <li><strong>Multi-Factor Authentication (MFA) Delete</strong>
    <ul>
      <li>requires additional authentication in order to permanently delete an object version or change the versioning state of a bucket.</li>
      <li>MFA delete can only be enabled by the root account</li>
    </ul>
  </li>
  <li><strong>Pre-Signed URLs</strong>
    <ul>
      <li>All S3 objects are by default private.</li>
      <li>Owner can share objects with others by creating <em>pre-signed URL</em>, using their own security credentials to grant time-limited permission to download the objects.</li>
      <li>To create a <em>pre-signed URL</em>, you must provide: your security credentials, bucket name, object key, the HTTP method (GET for download) and an expiration date and time.</li>
      <li>Useful to protect against ‘content scraping’ of web content such as media files stored in S3</li>
    </ul>
  </li>
</ul>

<h3 id="advanced-topics">Advanced Topics</h3>

<ul>
  <li><em>Multipart Upload</em>
    <ul>
      <li>To support uploading/copying large objects</li>
      <li>Allows to pause and resume. Has ability to upload objects where the size is initially unknown</li>
      <li>Object size &gt; 100 MB - you <em>should</em> use multipart upload</li>
      <li>Object size &gt; 5 GB - you <em>must</em> use multipart upload</li>
      <li>Using high-level APIs and high-level S3 command in CLI (<code>aws s3 cp</code>, <code>aws s3 mv</code>, <code>aws s3 sync</code>), multipart upload is automatically performed for large objects</li>
      <li>Using low-level APIs, you must break the file into parts and keep track of the parts.</li>
      <li>If a multipart upload is incomplete after specified number of days, you can set an object lifecycle policy on a bucket to abort to minimize the storage costs.</li>
    </ul>
  </li>
  <li><em>Range GETs</em>
    <ul>
      <li>is used to download (GET) only a portion of an object in S3 or Glacier.</li>
    </ul>
  </li>
  <li><em>Cross-Region Replication</em>
    <ul>
      <li>allows to asynchronously replicate all <em>new</em> objects from one region to another.</li>
      <li>Metadata and ACLs of the object also is replicated.</li>
      <li>Only new objects are replicated. Existing objects must be copied via a separate command.</li>
      <li>To enable <em>Cross-Region Replication</em>, versioning must be turned-on in both source and target buckets.</li>
      <li>An IAM policy must be used to give S3 permission to replicate objects on your behalf.</li>
      <li>Commonly used to reduce the latency to access objects by placing them closer to a set of users or store backup data at a certain distance from the original source data</li>
    </ul>
  </li>
  <li><em>Logging</em>
    <ul>
      <li>Logging is off by default. While enabling, you must choose the bucket where the logs will be stored.</li>
    </ul>
  </li>
  <li><em>Event Notifications</em>
    <ul>
      <li>Set up at bucket level to notify when an object is created, removed or lost in RRS</li>
      <li>Notification messages can be sent to Amazon SQS or SNS or AWS Lambda</li>
    </ul>
  </li>
</ul>

<h2 id="amazon-glacier">Amazon Glacier</h2>

<ul>
  <li>Extremely low-cost storage service for data archiving and long-term backup. Optimized for infrequently accessed data where a retrieval time of several hours is suitable.</li>
  <li>To retrieve an object, issue a restore command using S3 API; 3 to 5 hours later, it is copied to S3 RRS. Restore simply creates a copy; the original data is retained in Glacier until explicitly deleted.</li>
  <li><strong>Archives</strong>
    <ul>
      <li>Data is stored in <em>archives</em></li>
      <li>Max 40 TB of data per archive</li>
      <li>Unlimited archives can be created</li>
      <li>Each archive is assigned a unique archive ID at the creation time</li>
      <li>Archives are automatically encrypted</li>
      <li>Archives are immutable</li>
    </ul>
  </li>
  <li><strong>Vaults</strong>
    <ul>
      <li>Vaults are containers for archives.</li>
      <li>Max 1000 vaults per account</li>
      <li>Control access to vaults using IAM policies or vault access policies</li>
      <li><em>Vault lock policy</em>: can specify controls such as Write Once Read Many (WORM) in a vault local policy to lock the policy from future edits.</li>
      <li>Retrieving up to 5% of stored data is free per month.</li>
    </ul>
  </li>
</ul>

<h2 id="amazon-elastic-block-store-ebs">Amazon Elastic Block Store (EBS)</h2>

<p><img class="right" src="/technology/aws-ebs.PNG" width="200" height="200" /></p>

<ul>
  <li>Persistent block-level storage volumes for use with EC2 instances. Each EBS volume is automatically replicated within its Availability Zone to protect from component failure, offering HA and durability.</li>
  <li>Each EBS volume is automatically replicated within its Availability Zone</li>
  <li>Each EBS volume can be attached to a single EC2 instances</li>
  <li>A single EC2 instance can be attached to one or more EBS volumes</li>
</ul>

<h3 id="ebs-volume-types">EBS Volume Types</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: left"><strong>Magnetic Volume</strong></th>
      <th style="text-align: left"><strong>General-Purpose SSD</strong></th>
      <th style="text-align: left"><strong>Provisioned IOPS SSD</strong></th>
      <th style="text-align: left"><strong>Throughput-Optimized HDD Volumes</strong></th>
      <th style="text-align: left"><strong>Cold HDD Volumes</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>Use Cases</strong></td>
      <td style="text-align: left">Workloads where data is accessed infrequently, Sequential reads, Situations where low-cost storage is a requirement</td>
      <td style="text-align: left">System boot volumes, Virtual Desktops, small-to-medium size databases, Dev to Test environments</td>
      <td style="text-align: left">I/O intensive workloads, particularly large database workloads,<br />Critical business apps that require sustained IOPS performance or more than 10,000 IOPS or 160 MB of throughput per volume</td>
      <td style="text-align: left">Low-cost HDD volumes designed for frequent-access, throughput-intensive workloads such as big data, data warehouses, and log processing.</td>
      <td style="text-align: left">Designed for less frequently accessed workloads, such as colder data requiring fewer scans per day.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Volume Size</strong></td>
      <td style="text-align: left">1 GB to 1 TB</td>
      <td style="text-align: left">1 GB to 16 TB</td>
      <td style="text-align: left">4 GB to 16 TB</td>
      <td style="text-align: left">up to 16 TB</td>
      <td style="text-align: left">Max 16 TB</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Max throughput</strong></td>
      <td style="text-align: left">40 - 90 MB</td>
      <td style="text-align: left">160 MB</td>
      <td style="text-align: left">320 MB</td>
      <td style="text-align: left">500 MB/s</td>
      <td style="text-align: left">250 MB/s</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>IOPS Performance</strong></td>
      <td style="text-align: left">Average 100 IOPS with the ability to burst to 100s of IOPS</td>
      <td style="text-align: left">Baseline performance of 3 IOPS/GB (max 10,000 IOPS) with the ability to burst to 3000 IOPS for volumes under 1000 GB</td>
      <td style="text-align: left">Consistently performs at provisioned level, up to 20,000 IOPS max</td>
      <td style="text-align: left">max 500</td>
      <td style="text-align: left">Max 250</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Cost</strong></td>
      <td style="text-align: left">Cheapest</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">Based on the volume size provisioned (not space used) + number of IOPS provisioned, whether they are consumed or not</td>
      <td style="text-align: left">significantly less expensive than general-purpose SSD volumes.</td>
      <td style="text-align: left">significantly less expensive than Throughput-Optimized HDD volumes</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Misc</strong></td>
      <td style="text-align: left">Lowest Performance</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">When you provision a <em>Provisioned IOPS SSD</em> volume, you should specify both the size and the desired number of IOPS<br />You can stripe multiple volumes together in a <em>RAID 0</em> configuration for larger size and greater performance</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
  </tbody>
</table>

<p><strong>EBS-Optimized Instances</strong></p>

<ul>
  <li>Uses an optimized configuration stack and provides additional, dedicated capacity for EBS I/O. This optimization provides the best performance for EBS volumes by minimizing contention between EBS I/O and other traffic from EC2 instance.</li>
</ul>

<h3 id="ebs-snapshots">EBS Snapshots</h3>

<ul>
  <li>Backup EBS volume data by taking snapshots through AWS console, CLI, API or setting up a schedule for regular snapshots</li>
  <li>Snapshots are incremental backups, which means that only the blocks on the device that have changed since your most recent snapshot are saved.</li>
  <li>Snapshot data is stored in S3.</li>
  <li>The action of taking a snapshot is free. You pay only the storage costs for the snapshot data.</li>
  <li>When you request a snapshot, the point-in-time snapshot is created immediately and the volume may continue to be used, but the snapshot may remain in pending status until all the modified blocks have been transferred to S3.</li>
  <li>Snapshots are stored in AWS-controlled S3 storage; not in your account’s S3 buckets. This means you cannot manipulate them like other Amazon S3 objects. Rather, you must use the EBS snapshot features to manage them.</li>
  <li>Snapshots are constrained to the region in which they are created, meaning you can use them to create new volumes only in the same region. If you need to restore a snapshot in a different region, you can copy a snapshot to another region.</li>
</ul>

<p><strong>Creating a Volume from a Snapshot</strong></p>

<ul>
  <li>To use a snapshot, you create a new EBS volume from the snapshot. When you do this, the volume is created immediately but the data is loaded lazily. This means that the volume can be accessed upon creation, and if the data being requested has not yet been restored, it will be restored upon first request. Because of this, it is a best practice to initialize a volume created from a snapshot by accessing all the blocks in the volume.</li>
  <li>Snapshots can also be used to increase the size of an EBS volume. To increase the size of an EBS volume
    <ul>
      <li>take a snapshot of the volume</li>
      <li>create a new volume of the desired size from the snapshot</li>
      <li>Replace the original volume with the new volume.</li>
    </ul>
  </li>
</ul>

<p><strong>Recovering Volumes</strong></p>

<ul>
  <li>If an Amazon EBS-backed instance fails and there is data on the boot drive, it is relatively straightforward to detach the volume from the instance.</li>
  <li>Unless the <code>DeleteOnTermination</code> flag for the volume has been set to false, the volume should be detached before the instance is terminated.</li>
  <li>The volume can then be attached as a data volume to another instance and the data read and recovered.</li>
</ul>

<p><strong>Encryption Options</strong></p>

<ul>
  <li>Amazon EBS offers native encryption on all volume types</li>
  <li>When you launch an encrypted EBS volume, Amazon uses the <em>AWS KMS</em> to handle key management. A new master key will be created unless you select a master key that you created separately in the service.</li>
  <li>Data and associated keys are encrypted using the industry-standard <em>AES-256</em> algorithm.</li>
  <li>The encryption occurs on the servers that host EC2 instances, so the data is actually encrypted in transit between the host and the storage media and also on the media.</li>
  <li>Encryption is transparent, so all data access is the same as unencrypted volumes, and you can expect the same IOPS performance on encrypted volumes as you would with unencrypted volumes, with a minimal effect on latency.</li>
  <li>Snapshots that are taken from encrypted volumes are automatically encrypted, as are volumes that are created from encrypted snapshots.</li>
</ul>

<h2 id="aws-storage-gateway">AWS Storage Gateway</h2>

<ul>
  <li>is a service connecting an on-premise s/w with cloud-based storage to provide seamless and secure integration. It provides low-latency performance by maintaining a cache of frequently accessed data on-premises while securely storing all of your data encrypted in S3 or Glacier.</li>
</ul>

<h2 id="amazon-cloudfront">Amazon CloudFront</h2>

<ul>
  <li>is a content delivery web service - an easy way to distribute content with low-latency, high data transfer speeds - can be used to deliver websites including dynamic, static, streaming and interactive content, using a global network of edge locations. Request for content are automatically routed to the nearest edge location for best performance.</li>
</ul>

<hr />

<h1 id="compute-and-network-services">Compute and Network Services</h1>

<h2 id="ec2">EC2</h2>

<h3 id="instance-types">Instance Types</h3>

<ul>
  <li>2 key concepts to an instance:
    <ul>
      <li>the amount of virtual hardware dedicated to the Instance</li>
      <li>the software loaded on the instance</li>
    </ul>
  </li>
  <li>Instance types vary in following dimensions:
    <ul>
      <li>Virtual CPUs (vCPUs)</li>
      <li>Memory</li>
      <li>Storage</li>
      <li>Network performance</li>
    </ul>
  </li>
  <li>Instance types are grouped into families</li>
  <li>The ratio of vCPUs to memory is constant as the size scale linearly</li>
  <li>The hourly price for each size scales linearly. e.g., <code>Cost(m4.xlarge) = 2 x Cost(m4.large)</code></li>
  <li>Optimized Instance Type Family
    <ul>
      <li><code>m4</code> family provides a balance of compute, memory and n/w resources</li>
      <li><code>c4</code> <strong>compute optimized</strong> for workloads requiring significant processing</li>
      <li><code>r3</code> <strong>memory optimized</strong> for memory-intensive workloads</li>
      <li><code>i2</code> <strong>storage optimized</strong> for workloads requiring high amounts of fast SSD storage</li>
      <li><code>g2</code> <strong>GPU-based instances</strong> for graphics and general-purpose GPU compute workloads</li>
    </ul>
  </li>
  <li>Network performance
    <ul>
      <li>AWS publishes a relative measure of n/w performance: low, moderate, high</li>
      <li><em>Enhanced Networking</em>:
        <ul>
          <li>For workloads requiring greater n/w performance, use instance types supporting <em>enhanced networking</em>.</li>
          <li>It reduces the impact of virtualization on n/w performance enabling a capability called <strong><em>Single Root I/O Virtualization (SR-IOV)</em></strong></li>
          <li>This results in more <em>packets per second (PPS)</em>, lower latency and less jitter</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Amazon Machine Image (AMI)</strong></p>

<ul>
  <li>defines the initial software that will be on an instance when it is launched: <em>OS + initial state of patches + application/system software</em></li>
  <li>All AMIs are based on x86 OS (either Linux or Windows)</li>
  <li><strong>4 Sources of AMIs</strong>
    <ul>
      <li><em>Published by AWS</em>: you should apply patches upon launch</li>
      <li><em>AWS Marketplace</em></li>
      <li><em>Generated from Existing instances</em>: published AMI + corporate standard software added</li>
      <li><em>Uploaded Virtual Servers</em>: Using AWS VM Import/Export to create images from formats: VHD, VMDK, OVA</li>
    </ul>
  </li>
</ul>

<p><strong>Addressing an Instance</strong></p>

<p>There are several ways that an instance may be addressed over the web upon creation:</p>

<ul>
  <li><strong><em>Public DNS Name</em></strong>
    <ul>
      <li>DNS name is generated automatically and cannot be specified by the customer.</li>
      <li>This DNS name persists only while the instance is running and cannot be transferred to another instance.</li>
    </ul>
  </li>
  <li><strong><em>Public IP</em></strong>
    <ul>
      <li>This IP address is assigned from the addresses reserved by AWS and cannot be specified.</li>
      <li>This IP address is unique on the Internet, persists only while the instance is running, and cannot be transferred to another instance.</li>
    </ul>
  </li>
  <li><strong><em>Elastic IP</em></strong>
    <ul>
      <li>An elastic IP address is an address unique on the Internet that you reserve independently and associate with an instance.</li>
      <li>This IP address persists until the customer releases it and is not tied to the lifetime or state of an individual instance.</li>
      <li>Because it can be transferred to a replacement instance in the event of an instance failure, it is a public address that can be shared externally without coupling clients to a particular instance.</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>Private IP addresses and Elastic Network Interfaces (ENIs) are additional methods of addressing instances that are available in the context of an VPC.</p>
</blockquote>

<h3 id="securing-an-instance">Securing an Instance</h3>

<p><strong>Initial Access</strong></p>

<ul>
  <li>EC2 uses <em>public-key cryptography</em> to encrypt and decrypt login information.</li>
  <li><em>Public-key cryptography</em> uses a public key to encrypt a piece of data and an associated private key to decrypt the data. These two keys together are called a <em>key pair</em>.</li>
  <li>Key pairs can be created through the AWS Management Console, CLI, or API, or customers can upload their own key pairs.</li>
  <li>AWS stores the public key in <code>~/.ssh/authorized_keys</code> folder</li>
  <li>Private keys are kept by the customer. The private key is essential to acquiring secure access to an instance for the first time.</li>
  <li>When launching a Windows instance, EC2 generates a random password for the local administrator account and encrypts the password using the public key. Initial access to the instance is obtained by decrypting the password with the private key, either in the console or through the API.</li>
</ul>

<p><strong>Virtual Firewall Protection</strong></p>

<ul>
  <li>AWS allows you to control traffic in and out of your instances through virtual firewalls called <strong>security groups</strong>.</li>
  <li>Security groups allow you to control traffic based on port, protocol, and source/destination.</li>
  <li>Security groups are applied at the instance level, as opposed to a traditional on-premises firewall that protects at the perimeter. To breach a single perimeter to access all instances, one has to breach the security groups repeatedly for each individual instances</li>
  <li>Security groups have different capabilities :
    <ul>
      <li><em>EC2-Classic Security Groups</em> - Control outgoing instance traffic</li>
      <li><em>VPC Security Groups</em>	- Control outgoing and incoming instance traffic</li>
    </ul>
  </li>
  <li>Every instance must have at least one security group but can have more</li>
  <li><em>Changing Security Group</em>
    <ul>
      <li>If an instance is running in an VPC, you can change which security groups are associated with an instance while the instance is running.</li>
      <li>For instances outside of an VPC (called EC2-Classic), the association of the security groups cannot be changed after launch.</li>
    </ul>
  </li>
  <li>A security group is <em>default deny</em> - it does not allow any traffic that is not explicitly allowed by a security group rule.</li>
  <li>A <em>rule</em> is defined by 3 attributes:
    <ul>
      <li>Port</li>
      <li>Protocol. e.g., HTTP, RDP, etc.</li>
      <li>Source/Destination- can be defined in 2 ways:
        <ul>
          <li>CIDR block</li>
          <li>Security group — Includes any instance that is associated with the given security group. This helps prevent coupling security group rules with specific IP addresses</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>When an instance is associated with multiple security groups, the rules are aggregated and all traffic allowed by each of the individual groups is allowed. For example, if security group A allows RDP traffic from 72.58.0.0/16 and security group B allows HTTP and HTTPS traffic from 0.0.0.0/0 and your instance is associated with both groups, then both the RDP and HTTP/S traffic will be allowed in to your instance.</li>
  <li>A security group is a <strong>stateful firewall</strong>; that is, an outgoing message is remembered so that the response is allowed through the security group without an explicit inbound rule being required.</li>
</ul>

<h3 id="lifecycle-of-instances">Lifecycle of Instances</h3>

<p><strong>Bootstrapping</strong></p>

<ul>
  <li>Ability to configure instances and install applications programmatically when an instance is launched. The process of providing code to be run on an instance at launch is called <em>bootstrapping</em>.</li>
  <li>When an instance is launched for the first time, a string-valued parameter <code>UserData</code> is passed to the OS to be executed as part of the launch process. On Linux, this can be a shell script, performing tasks like
    <ul>
      <li>Applying patches and updates to the OS</li>
      <li>Installing application software</li>
      <li>Copying a longer script or program from storage to be run on the instance</li>
      <li>Installing Chef or Puppet and assigning the instance a role so the configuration management software can configure the instance</li>
    </ul>
  </li>
  <li><code>UserData</code> is stored with the instance and is not encrypted. DO NOT include any secrets such as passwords or keys in it.</li>
</ul>

<p><strong>VM Import/Export</strong></p>

<ul>
  <li>Import VMs from your existing environment as an EC2 instance and export them back to your on-premises environment.</li>
  <li>You can only export previously imported EC2 instances. Instances launched within AWS from AMIs cannot be exported.</li>
</ul>

<p><strong>Instance Metadata</strong></p>

<ul>
  <li>Instance metadata is data about your instance that you can use to configure or manage the running instance. This is unique in that it is a mechanism to obtain AWS properties of the instance from within the OS without making a call to the AWS API.</li>
  <li>An HTTP call to http://169.254.169.254/latest/meta-data/ will return the top node of the instance metadata tree.</li>
  <li>Instance metadata includes:
    <ul>
      <li>The associated security groups</li>
      <li>The instance ID</li>
      <li>The instance type</li>
      <li>The AMI used to launch the instance</li>
    </ul>
  </li>
</ul>

<p><strong>Instance Tags</strong></p>

<ul>
  <li>Tags are key/value pairs you can associate with your instance or other service.</li>
  <li>Tags can be used to identify attributes of an instance like project, environment (dev, test, and so on), billable department, and so forth.</li>
  <li>Max 10 tags per instance.</li>
</ul>

<p><strong>Modifying Instance Type</strong></p>

<ul>
  <li>If the compute needs prove to be higher or lower than expected, the instances can be changed to a different size more appropriate to the workload.</li>
  <li>Stop instance -&gt; Change instance type -&gt; Restart instance</li>
</ul>

<p><strong>Termination Protection</strong></p>

<ul>
  <li>To prevent accidental termination, when <em>Termination Protection</em> is enabled, calls to terminate the instance will fail.</li>
  <li>It does not prevent termination triggered
    <ul>
      <li>by an OS shutdown command,</li>
      <li>termination from an Auto Scaling group, or</li>
      <li>termination of a Spot Instance due to Spot price changes</li>
    </ul>
  </li>
</ul>

<h3 id="instance-pricing-options">Instance Pricing Options</h3>

<ul>
  <li><strong>On-Demand Instances</strong>
    <ul>
      <li>Least cost-effective due to the flexibility it allows customers to save by provisioning a variable level of compute for unpredictable workloads</li>
      <li>The price is per hour for each instance type</li>
      <li>No up-front commitment, and the customer has control over when the instance is launched and when it is terminated</li>
    </ul>
  </li>
  <li><strong>Reserved Instances</strong>
    <ul>
      <li>75% less cost compared to On-Demand hourly rate</li>
      <li>When purchasing a reservation, the customer specifies the instance type and Availability Zone</li>
      <li>Capacity in the data centers is reserved for that customer.</li>
      <li>2 factors determining the cost:
        <ul>
          <li><em>Term commitment</em> - the duration of the reservation</li>
          <li><em>Payment option</em>
            <ul>
              <li><em>All Upfront</em> —Pay for the entire reservation up front. There is no monthly charge for the customer during the term.</li>
              <li><em>Partial Upfront</em> —Pay a portion of the reservation charge up front and the rest in monthly installments for the duration of the term.</li>
              <li><em>No Upfront</em> —Pay the entire reservation charge in monthly installments for the duration of the term.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Modifying Reservation: You can modify your whole reservation, or just a subset, in one or more of the following ways:
        <ul>
          <li>Switch Availability Zones within the same region.</li>
          <li>Change between EC2-VPC and EC2-Classic.</li>
          <li>Change the instance type within the same instance family (Linux instances only).</li>
        </ul>
      </li>
      <li>One may purchase two Reserved Instances to handle the average traffic, but depend on On-Demand Instances to fulfill compute needs during the peak times.</li>
    </ul>
  </li>
  <li><strong>Spot Instances</strong>
    <ul>
      <li>offers the greatest discount</li>
      <li>For workloads that are not time critical and are tolerant of interruption</li>
      <li>With Spot Instances, when the customer’s bid price is above the current Spot price, the customer will receive the requested instance(s). These instances will operate like all other EC2 instances, and the customer will only pay the Spot price for the hours that instance(s) run.</li>
      <li>The instances will run until:
        <ul>
          <li>The customer terminates them.</li>
          <li>The Spot price goes above the customer’s bid price.</li>
          <li>There is not enough unused capacity to meet the demand for Spot Instances.</li>
          <li>If EC2 needs to terminate a Spot Instance, the instance will receive a termination notice providing a two-minute warning prior to EC2 terminating the instance.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="tenancy-options">Tenancy options</h3>

<ul>
  <li><strong>Shared Tenancy</strong>
    <ul>
      <li>a single host machine may house instances from different customers</li>
      <li>Default model</li>
      <li>AWS does not use overprovisioning and fully isolates instances from other instances on the same host, this is a secure tenancy model.</li>
    </ul>
  </li>
  <li><strong>Dedicated Instance</strong>
    <ul>
      <li>runs on hardware that’s dedicated to a single customer.</li>
    </ul>
  </li>
  <li><strong>Dedicated Host</strong>
    <ul>
      <li>Dedicated Host is a physical server with EC2 instance capacity fully dedicated to a single customer’s use.</li>
      <li>The customer has complete control over which specific host runs an instance at launch. This differs from Dedicated Instances in that a Dedicated Instance can launch on any hardware that has been dedicated to the account.</li>
      <li>can help address licensing requirements and reduce costs by allowing to use existing server-bound software licenses.</li>
    </ul>
  </li>
  <li><strong>Placement Groups</strong>
    <ul>
      <li>is a logical grouping of instances within a single Availability Zone.</li>
      <li>enable applications to participate in a low-latency, 10 Gbps network.</li>
      <li>recommended for applications that benefit from low network latency, high network throughput, or both. This represents network connectivity between instances.</li>
      <li>To fully use this network performance for your placement group, choose an instance type that supports enhanced networking and 10 Gbps network performance.</li>
    </ul>
  </li>
</ul>

<p><strong>Instance Stores</strong></p>

<ul>
  <li>An instance store provides temporary block-level storage (<em>ephemeral storage</em>)</li>
  <li>Data in the instance store is lost when:
    <ul>
      <li>the underlying disk drive fails</li>
      <li>the instance stops</li>
      <li>the instance terminates</li>
    </ul>
  </li>
</ul>

<h2 id="aws-lambda">AWS Lambda</h2>

<ul>
  <li>a zero-administration compute platform for back-end web developers that runs your code for you on the AWS cloud (for high availability)</li>
</ul>

<h2 id="auto-scaling">Auto Scaling</h2>

<ul>
  <li>
    <p>allows to scale EC2 capacity up or down automatically according to conditions defined for the particular workload (for scaling in and out)</p>
  </li>
  <li>Advantage of deploying applications to the cloud is the ability to launch and then release servers in response to variable workloads.</li>
  <li>
    <p>Provisioning servers on demand and then releasing when not needed can provide significant cost savings. E.g., an end-of-month data-input system, a retail shopping site supporting flash sales, etc.</p>
  </li>
  <li><strong>Embrace the Spike</strong>
    <ul>
      <li>Many web applications have unplanned load increases based on events outside of your control.</li>
      <li>Setting up Auto Scaling in advance will allow to embrace and survive such fast increase in the number of requests. It will scale up your site to meet the increased demand and then scale down when the event subsides.</li>
    </ul>
  </li>
</ul>

<h3 id="auto-scaling-plans">Auto Scaling Plans</h3>

<ul>
  <li><strong>Maintain Current Instance Levels</strong>
    <ul>
      <li>Configure your Auto Scaling group to maintain a minimum or specified number of running instances at all times.</li>
      <li>To maintain the current instance levels, Auto Scaling performs a periodic health check on running instances within an Auto Scaling group.</li>
      <li>When Auto Scaling finds an unhealthy instance, it terminates that instance and launches a new one.</li>
      <li>Steady state workloads that need a consistent number of EC2 instances at all times can use Auto Scaling to monitor and keep that specific number of EC2 instances running.</li>
    </ul>
  </li>
  <li><strong>Manual Scaling</strong>
    <ul>
      <li>This is the most basic way to scale your resources. You just need to specify the change in the maximum, minimum, or desired capacity of your Auto Scaling group.</li>
      <li>Auto Scaling manages the process of creating or terminating instances to maintain the updated capacity.</li>
      <li>Manual scaling out can be very useful to increase resources for an infrequent event, such as movie release dates.</li>
      <li>For extremely large-scale events, even the ELB load balancers can be pre-warmed by working with your local solutions architect or AWS Support.</li>
    </ul>
  </li>
  <li><strong>Scheduled Scaling</strong>
    <ul>
      <li>When you have a recurring schedule or a predictable schedule of when you will need to increase or decrease the number of instances in your group, e.g., end-of-month, or end-of-year processing, schedule scaling is useful.</li>
      <li>Scaling actions are performed automatically as a function of time and date.</li>
    </ul>
  </li>
  <li><strong>Dynamic Scaling</strong>
    <ul>
      <li>Lets you define parameters that control the Auto Scaling process in a scaling policy. E.g., create a policy that adds more EC2 instances to the web tier when the network bandwidth, measured by CloudWatch, reaches a certain threshold.</li>
    </ul>
  </li>
</ul>

<h3 id="auto-scaling-components">Auto Scaling Components</h3>

<p><strong>Launch Configuration</strong></p>

<ul>
  <li>A launch configuration is the template used to create new instances, and it has the following:
    <ul>
      <li>Name (e.g., <em>myLC</em>)</li>
      <li>AMI (e.g., <em>ami-0535d66c</em>)</li>
      <li>Instance type (e.g., <em>m3.medium</em>)</li>
      <li>Security groups (e.g., <em>sg-f57cde9d</em>)</li>
      <li>Instance key pair (e.g., <em>myKeyPair</em>)</li>
    </ul>
  </li>
  <li>Each Auto Scaling group can have only one launch configuration at a time.</li>
  <li>Security groups for instances launched in EC2-Classic may be referenced by security group name or by security group IDs. Security group ID is recommended.</li>
  <li><em>Limits</em>
    <ul>
      <li>Default limit for launch configurations = <strong>100 per region</strong>. If you exceed this limit, the call to create-launch-configuration will fail. To update this limit: <code>aws autoscaling describe-account-limits</code></li>
      <li>Auto Scaling may cause you to reach limits of other services, such as the default number of EC2 instances you can currently launch = <strong>20 per region</strong>.</li>
      <li>When you run a command using the CLI and it fails,
        <ul>
          <li>check your syntax first. If that checks out,</li>
          <li>verify the limits for the command you are attempting, and check to see that you have not exceeded a limit.</li>
          <li>To raise the limits, in cases allowed, create a support case at the AWS Support Center online and then choose <em>Service Limit Increase under Regarding</em>.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Auto Scaling Group</strong></p>

<ul>
  <li>An Auto Scaling group is a collection of EC2 instances managed by the Auto Scaling service.</li>
  <li>Each Auto Scaling group contains configuration options that control when Auto Scaling should launch new instances and terminate existing instances.</li>
  <li>An Auto Scaling group must contain
    <ul>
      <li>a name</li>
      <li>a minimum and maximum number of instances that can be in the group.</li>
      <li>(optional) desired capacity, which is the number of instances that the group must have at all times. If not specified, the default desired capacity = the minimum number of instances.</li>
    </ul>
  </li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class=""><span class="line">Name: myASG
</span><span class="line">Launch configuration: myLC
</span><span class="line">Availability Zones: us-east-1a and us-east-1c
</span><span class="line">Minimum size: 1
</span><span class="line">Desired capacity: 3
</span><span class="line">Maximum capacity: 10
</span><span class="line">Load balancers: myELB</span></code></pre></td></tr></table></div></figure></notextile></div>

<ul>
  <li>An Auto Scaling group can use either On-Demand (default) or Spot instances, but not both.</li>
  <li>Spot Instances
    <ul>
      <li>Bid price can be modified</li>
      <li>If instances are available at or below your bid price, they will be launched in your Auto Scaling group.</li>
    </ul>
  </li>
</ul>

<p><strong>Spot On!</strong></p>

<ul>
  <li>Spot Instances can be useful when hosting sites to provide additional compute capacity but are price constrained. E.g., a “freemium” site model where some basic functionality to users are free and additional functionality is for premium users.</li>
  <li>It can be used for providing the basic functionality when available by referencing a maximum bid price in the launch configuration (<code>—spot-price "0.15"</code>) associated with the Auto Scaling group.</li>
</ul>

<p><strong>Scaling Policy</strong></p>

<p><img class="right" src="/technology/aws-auto-scaling.png" /></p>

<ul>
  <li>CloudWatch alarms and scaling policies can be associated with an Auto Scaling group to adjust Auto Scaling dynamically.</li>
  <li>When a threshold is crossed, CloudWatch sends alarms to trigger changes (scaling in or out) to the number of EC2 instances currently receiving traffic behind a load balancer.</li>
  <li>After the CloudWatch alarm sends a message to the Auto Scaling group, Auto Scaling executes the associated policy to scale your group.</li>
  <li>The policy is a set of instructions that tells Auto Scaling whether to scale out, launching new EC2 instances referenced in the associated launch configuration, or to scale in and terminate instances.</li>
  <li>Various ways to configure a scaling policy:
    <ul>
      <li>increase or decrease by a specific number of instances, or</li>
      <li>adjust based on a percentage.</li>
      <li>scale by steps and increase or decrease the current capacity of the group based on a set of scaling adjustments that vary based on the size of the alarm threshold trigger.</li>
    </ul>
  </li>
  <li>
    <p>More than one scaling policy can be associated with an Auto Scaling group. E.g., One policy to scale out if CPU Load &gt; 75% for 2 minutes. Another policy to scale in if CPU Load &lt; 40% for 20 minutes.</p>
  </li>
  <li><em>Best Practice</em>
    <ul>
      <li>Scale out quickly and scale in slowly so you can respond to bursts or spikes but avoid inadvertently terminating EC2 instances too quickly, only having to launch more EC2 instances if the burst is sustained.</li>
      <li>Auto Scaling also supports a <em>cooldown period</em>, which is a configurable setting that determines when to suspend scaling activities for a short time for an Auto Scaling group.</li>
      <li>If you start an EC2 instance, you will be billed for one full hour of running time. Partial instance hours consumed are billed as full hours. This means that if you have a permissive scaling policy that launches, terminates, and re-launches many instances an hour, you are billing a full hour for each and every instance you launch, even if you terminate some of those instances in less than hour.</li>
      <li>Bootstrapping new EC2 instances launched using Auto Scaling takes time to configure before the instance is healthy and capable of accepting traffic. Instances that start and are available for load faster can join the capacity pool more quickly.</li>
      <li>Stateless instances can enter and exit gracefully than a stateful instance in an Auto Scaling group.</li>
    </ul>
  </li>
  <li><em>Rolling Out a Patch at Scale</em>
    <ul>
      <li>In large deployments of EC2 instances, Auto Scaling can be used to make rolling out a patch to your instances easy.</li>
      <li>The launch configuration associated with the Auto Scaling group may be modified to reference a new AMI and even a new EC2 instance if needed. Then you can deregister or terminate instances one at a time or in small groups, and the new EC2 instances will reference the new patched AMI.</li>
    </ul>
  </li>
</ul>

<h2 id="elastic-load-balancing-elb">Elastic Load Balancing (ELB)</h2>

<ul>
  <li>
    <p>automatically distributes incoming application traffic across multiple EC2 instances. (for fault-tolerance)</p>
  </li>
  <li>ELB allows to distribute traffic across a group of EC2 instances in one or more Availability Zones, to achieve high availability in your applications.</li>
  <li>ELB supports routing and load balancing of HTTP, HTTPS, TCP, and SSL traffic to EC2 instances.</li>
  <li>ELB provides a stable, single <em>Canonical Name record (CNAME)</em> entry point for DNS configuration and supports both Internet-facing and internal application-facing load balancers.</li>
  <li>ELB supports health checks for EC2 instances to ensure traffic is not routed to unhealthy or failing instances. It can scale automatically based on collected metrics.</li>
  <li>Long-running applications will eventually need to be maintained and updated with a newer version of the application. When using EC2 instances running behind an ELB load balancer, you may deregister these long-running EC2 instances associated with a load balancer manually and then register newly launched EC2 instances that you have started with the new updates installed.</li>
  <li><strong>Advantages of ELB</strong>
    <ul>
      <li>Because ELB is a managed service, it scales in and out automatically to meet the demands of increased application traffic and is highly available within a region itself as a service.</li>
      <li>ELB helps you achieve high availability for your applications by distributing traffic across healthy instances in multiple Availability Zones.</li>
      <li>ELB seamlessly integrates with the Auto Scaling service to automatically scale the EC2 instances behind the load balancer.</li>
      <li>ELB is secure, working with VPC to route traffic internally between application tiers, allowing you to expose only Internet-facing public IP addresses.</li>
      <li>ELB also supports integrated certificate management and SSL termination.</li>
    </ul>
  </li>
</ul>

<h3 id="types-of-load-balancers">Types of Load Balancers</h3>

<h4 id="internet-facing-load-balancers">Internet-Facing Load Balancers</h4>

<ul>
  <li>Takes requests from clients over the Internet and distributes them to EC2 instances that are registered with the load balancer.</li>
  <li>When you configure a load balancer, it receives a public DNS name that clients can use to send requests to your application. The DNS servers resolve the DNS name to your load balancer’s public IP address, which can be visible to client applications.</li>
  <li>Because ELB scales in and out to meet traffic demand, it is not recommended to bind an application to an IP address that may no longer be part of a load balancer’s pool of resources.</li>
  <li>Best practice: always refer a load balancer by its DNS name, instead of by the IP address, in order to provide a single, stable entry point.</li>
  <li>ELB in VPC supports IPv4 addresses only.</li>
  <li>ELB in EC2-Classic supports both IPv4 and IPv6 addresses.</li>
</ul>

<h4 id="internal-load-balancers">Internal Load Balancers</h4>

<ul>
  <li>In a multi-tier application, it is often useful to load balance between the tiers of the application. For example, an Internet-facing load balancer might receive and balance external traffic to the presentation or web tier whose EC2 instances then send its requests to a load balancer sitting in front of the application tier.</li>
  <li>Internal load balancers can be used to route traffic to EC2 instances in VPCs with private subnets.</li>
</ul>

<h4 id="https-load-balancers">HTTPS Load Balancers</h4>

<ul>
  <li>Load balancer that uses the SSL/TLS protocol for encrypted connections (also known as <strong><em>SSL offload</em></strong>).</li>
  <li>This feature enables traffic encryption between your load balancer and the clients that initiate HTTPS sessions, and for connections between your load balancer and your back-end instances.</li>
  <li>Elastic Load Balancing provides security policies that have predefined SSL negotiation configurations to use to negotiate connections between clients and the load balancer. In order to use SSL, you must install an SSL certificate on the load balancer that it uses to terminate the connection and then decrypt requests from clients before sending requests to the back-end EC2 instances.</li>
  <li>You can optionally choose to enable authentication on your back-end instances.</li>
  <li>ELB does not support <em>Server Name Indication (SNI)</em> on your load balancer. This means that if you want to host multiple websites on a fleet of EC2 instances behind ELB with a single SSL certificate, you will need to add a <em>Subject Alternative Name (SAN)</em> for each website to the certificate to avoid site users seeing a warning message when the site is accessed.</li>
</ul>

<h4 id="listeners">Listeners</h4>

<ul>
  <li>A listener is a process that checks for connection requests—for example, a CNAME configured to the A record name of the load balancer.</li>
  <li>Every load balancer must have one or more listeners configured.</li>
  <li>Every listener is configured with
    <ul>
      <li>a protocol and a port (client to load balancer) for a front-end connection</li>
      <li>a protocol and a port for the back-end (load balancer to EC2 instance) connection.</li>
    </ul>
  </li>
  <li>ELB supports protocols operating at two different Open System Interconnection (OSI) layers.
    <ul>
      <li>In the OSI model, Layer 4 is the transport layer that describes the TCP connection between the client and your back-end instance through the load balancer. Layer 4 is the lowest level that is configurable for your load
balancer.</li>
      <li>Layer 7 is the application layer that describes the use of HTTP and HTTPS connections from clients to the load balancer and from the load balancer to your back-end instance.</li>
    </ul>
  </li>
</ul>

<h3 id="elb-configurations">ELB Configurations</h3>

<h4 id="idle-connection-timeout">Idle Connection Timeout</h4>

<ul>
  <li>For each request that a client makes through a load balancer, the load balancer maintains two connections.
    <ul>
      <li>Connection 1: with the client</li>
      <li>Connection 2: to the back-end instance</li>
    </ul>
  </li>
  <li>For each connection, the load balancer manages an idle timeout that is triggered when no data is sent over the connection for a specified time period. After the idle timeout period has elapsed, if no data has been sent or received, the load balancer closes the connection.</li>
  <li>Default idle timeout for both connections = 60 seconds. This can be modified.</li>
  <li>If an HTTP request doesn’t complete within the idle timeout period, the load balancer closes the connection, even if data is still being transferred.</li>
  <li><strong><em>Keep-alive</em></strong>
    <ul>
      <li>If you use HTTP and HTTPS listeners, we recommend that you enable the <em>keep-alive</em> option for your EC2 instances. You can enable keep-alive in your web server settings or in the kernel settings for your EC2 instances.</li>
      <li>Keep-alive, when enabled, allows the load balancer to reuse connections to your back-end instance, which reduces CPU utilization.</li>
      <li>To ensure that the load balancer is responsible for closing the connections to your back-end instance, make sure that the value you set for the keep-alive time is greater than the idle timeout setting on your load balancer.</li>
    </ul>
  </li>
</ul>

<h4 id="cross-zone-load-balancing">Cross-Zone Load Balancing</h4>

<ul>
  <li>To ensure that request traffic is routed evenly across all back-end instances for your load balancer, regardless of the Availability Zone in which they are located, you should enable <em>cross-zone load balancing</em> on your load balancer.</li>
  <li>Cross-zone load balancing reduces the need to maintain equivalent numbers of back-end instances in each Availability Zone and improves your application’s ability to handle the loss of one or more back-end instances.</li>
  <li>However, it is still recommended that you maintain approximately equivalent numbers of instances in each Availability Zone for higher fault tolerance.</li>
  <li>For environments where clients cache DNS lookups, incoming requests might favor one of the Availability Zones. Using cross-zone load balancing, this imbalance in the request load is spread across all available back-end instances in the region, reducing the impact of misconfigured clients.</li>
</ul>

<h4 id="connection-draining">Connection Draining</h4>

<ul>
  <li>To ensure that the load balancer stops sending requests to instances that are deregistering or unhealthy, while keeping the existing connections open. This enables the load balancer to complete in-flight requests made to these instances.</li>
  <li>When you enable connection draining, you can specify a maximum time for the load balancer to keep connections alive before reporting the instance as deregistered.</li>
  <li>The maximum timeout value can be set between 1 and 3,600 seconds (the default is 300 seconds).</li>
  <li>When the maximum time limit is reached, the load balancer forcibly closes connections to the deregistering instance.</li>
</ul>

<h4 id="proxy-protocol">Proxy Protocol</h4>

<ul>
  <li>When you use TCP or SSL for both front-end and back-end connections, your load balancer forwards requests to the back-end instances without modifying the request headers.</li>
  <li>If you enable <em>Proxy Protocol</em>, a human-readable header is added to the request header with connection information such as the source IP address, destination IP address, and port numbers. The header is then sent to the back-end instance as part of the request.</li>
  <li>Before using Proxy Protocol, verify that your load balancer is not behind a proxy server with Proxy Protocol enabled. If Proxy Protocol is enabled on both the proxy server and the load balancer, the load balancer adds another header to the request, which already has a header from the proxy server. Depending on how your back-end instance is configured, this duplication might result in errors.</li>
</ul>

<h4 id="sticky-sessions">Sticky Sessions</h4>

<ul>
  <li>By default, a load balancer routes each request independently to the registered instance with the smallest load. However, you can use the sticky session feature (also known as session affinity), which enables the load balancer to bind a user’s session to a specific instance. This ensures that all requests from the user during the session are sent to the same instance.</li>
  <li>The key to managing sticky sessions is to determine how long your load balancer should consistently route the user’s request to the same instance. If your application has its own session cookie, you can configure ELB so that the session cookie follows the duration specified by the application’s session cookie.</li>
  <li>If your application does not have its own session cookie, you can configure ELB to create a session cookie by specifying your own stickiness duration.</li>
  <li>ELB creates a cookie named <em>AWSELB</em> that is used to map the session to the instance.</li>
</ul>

<h4 id="health-checks">Health Checks</h4>

<ul>
  <li>ELB supports health checks to test the status of the EC2 instances behind an ELB load balancer.</li>
  <li>If healthy, status = <em>InService</em>, else <em>OutOfService</em>.</li>
  <li>The load balancer performs health checks on all registered instances to determine whether the instance is in a healthy state or an unhealthy state.</li>
  <li>A health check is a ping, a connection attempt, or a page that is checked periodically.</li>
  <li>You can set the time interval between health checks and also the amount of time to wait to respond in case the health check page includes a computational aspect.</li>
  <li>Finally, you can set a threshold for the number of consecutive health check failures before an instance is marked as unhealthy.</li>
</ul>

<h2 id="aws-elastic-beanstalk">AWS Elastic Beanstalk</h2>

<ul>
  <li>Developers can simply upload their application code, and the service automatically handles all the details, such as resource provisioning, load balancing, Auto scaling, and monitoring.</li>
</ul>

<h2 id="aws-virtual-private-cloud-vpc">AWS Virtual Private Cloud (VPC)</h2>

<p><img class="right" src="/technology/aws-vpc-sample.png" /></p>

<ul>
  <li>VPC is a custom-defined virtual network within AWS cloud</li>
  <li>VPC is the networking layer for EC2</li>
  <li>2 networking platforms in AWS
    <ul>
      <li>EC2-Classic (old) - single, flat n/w shared with other customers</li>
      <li>EC2-VPC</li>
    </ul>
  </li>
  <li>VPC consists of
    <ul>
      <li>Subnets</li>
      <li>Route tables</li>
      <li>DHCP option sets</li>
      <li>Security groups</li>
      <li>Network ACLs</li>
      <li>Internet Gateways (<em>optional</em>)</li>
      <li>EIP addresses (<em>optional</em>)</li>
      <li>Elastic Network Interfaces ENIs (<em>optional</em>)</li>
      <li>Endpoints (<em>optional</em>)</li>
      <li>Peering (<em>optional</em>)</li>
      <li>NAT instances and NAT gateways (<em>optional</em>)</li>
      <li>Virtual Private Gateways (VPGs), Customer Gateways (CGWs), VPNs</li>
      <li>(<em>optional</em>)</li>
    </ul>
  </li>
  <li>VPC allows to
    <ul>
      <li>select your own IP address range</li>
      <li>create your own subnets</li>
      <li>configure your own route tables, n/w gateways and security settings</li>
    </ul>
  </li>
  <li>Within a region, you can create multiple VPCs. Each VPC is logically isolated even if it shared its IP address space</li>
  <li>VPC can span across Availability Zones</li>
  <li>To create a VPC, choose CIDR block
    <ul>
      <li>which cannot be changed after creation</li>
      <li>As large as <em>/16 (65,636 addresses)</em> or as small as <em>/28 (16 addresses)</em>.</li>
      <li>Should not overlap</li>
    </ul>
  </li>
  <li>Every account has a default VPC created in each region with a default subnet created in each Availability Zone.
    <ul>
      <li>The assigned CIDR block of the VPC will be <em>172.31.0.0/16</em></li>
      <li>Default VPCs contain one public subnet in every Availability Zone with the region, with a netmask of <em>/20</em></li>
    </ul>
  </li>
  <li>Orgs can extend their corporate data center n/w to AWS by using h/w or s/w VPN connections or dedicated circuits by using AWS Direct Connect.</li>
</ul>

<h3 id="subnets">Subnets</h3>

<ul>
  <li>A subnet is a segment of VPC’s IP address range where you can launch EC2 instances, RDS databases, etc.</li>
  <li>CIDR blocks defines subnets</li>
  <li>Smallest subnet you can create is a <em>/28 (16 addresses)</em>.
    <ul>
      <li>AWS reserves the first 4 addresses and the last IP address of every subnet for internal networking purposes. So, <code>16-4-1 = 11</code> IP addresses is the smallest subnet.</li>
    </ul>
  </li>
  <li>Subnets reside within one Availability Zone. CANNOT span across zones or regions.</li>
  <li>One Availability Zone can have multiple subnets</li>
  <li>Internal IP address range of the subnet is always <em>private</em></li>
  <li>Subnets can be classified as one of the below
    <ul>
      <li><strong>Public</strong> : route table directs the subnet’s traffic to VPC’s IGW</li>
      <li><strong>Private</strong>: route table DOES NOT direct the traffic to VPC’s IGW</li>
      <li><strong>VPN-only</strong>: route table DOES NOT direct the traffic to VPC’s IGW, but to the VPC’s VPG</li>
    </ul>
  </li>
</ul>

<h3 id="route-tables">Route Tables</h3>

<ul>
  <li>contains a set of rules called <em>rules</em> that are applied to subnets to determine where the network traffic is directed</li>
  <li>allows EC2 instances in different subnets within a VPC to communicate with each other</li>
  <li>Route tables can be used to specify
    <ul>
      <li>which subnets are public (by directing Internet traffic to IGW)</li>
      <li>which subnets are private (by not having a route that directs to IGW)</li>
    </ul>
  </li>
  <li>Each route table
    <ul>
      <li>has a default route called the <em>local route</em>, which enables communication within the VPC. This route cannot be modified or removed.</li>
      <li>custom routes can be added to exit the VPC via IGW, VPG or the NAT instance</li>
    </ul>
  </li>
  <li>Each VPC comes with a main router table that can be modified.</li>
  <li>Custom route tables can be added, and subsequent new subnets created will be automatically associated with this custom route table</li>
  <li>Each subnet must be associated with a route table</li>
  <li>Each route in a table specifies a destination CIDR.</li>
</ul>

<h3 id="igw-internet-gateways">IGW (Internet Gateways)</h3>

<p><img class="right" src="/technology/aws-igw.png" /></p>

<ul>
  <li>IGW allows communication between instances in VPC and the Internet.</li>
  <li>IGW is horizontally scaled, redundant, and highly available VPC component</li>
  <li>To route traffic subnets to Internet, route tables must have a route targeting the IGW</li>
  <li>To enable an EC2 instance to send/receive traffic from the Internet, assign a public IP address or EIP address</li>
  <li>IGW maintains the 1-to-1 map of the EC2 instance’s private IP and public IP.</li>
  <li>EC2 instances within a VPC are only aware of their private IPs. When traffic is sent from the instance to the Internet, the IGW translates the reply address to the public IP address of the instance.</li>
  <li>To create a public subnet with Internet access
    <ul>
      <li>Attach an IGW to your VPC</li>
      <li>Create a subnet route table<br />
        <ul>
          <li>rule to route all non-local traffic (<em>0.0.0.0/0</em>) to the IGW</li>
          <li>(optional) rule to route all destinations not explicitly defined in the route table to IGW</li>
        </ul>
      </li>
      <li>Configure your n/w ACLs and security group rules to allow relevant traffic flow to and from your instance</li>
    </ul>
  </li>
</ul>

<h3 id="dhcp">DHCP</h3>

<ul>
  <li>DHCP - a standard for passing configuration information to hosts on a TCP/IP n/w</li>
  <li>Following options can be configured within a DHCP message:
    <ul>
      <li><strong>DNS servers</strong> - IP addresses of up to 4 DNS servers.</li>
      <li><strong>Domain name</strong> - e.g., <em>mycompany.com</em></li>
      <li><strong>ntp-servers</strong> - IP addresses of up to 4 NTP (Network Time Protocol) servers</li>
      <li><strong>netbios-name-servers</strong> - IP addresses of up to 4 NetBIOS name servers</li>
      <li><strong>netbios-node-type</strong> - set this to 2</li>
    </ul>
  </li>
  <li>Upon a VPC creation, AWS automatically associates a DHCP option set to it, and sets 2 options
    <ul>
      <li>DNS servers (defaulted to <em>AmazonProvidedDNS</em>. This option enables DNS for instances that need to communicate over the VPC’s IGW)</li>
      <li>Domain name (defaulted to domain name of your region)</li>
    </ul>
  </li>
  <li>To assign your own domain name to your instances, create a custom DHCP option set and assign it to your VPC</li>
  <li>Each VPC must have only one DHCP option set assigned to it.</li>
</ul>

<h3 id="eips-elastic-ip-addresses">EIPs (Elastic IP Addresses)</h3>

<ul>
  <li>An Elastic IP Addresses (EIP) is a static, public IP address in the pool for the region that you can allocate to your account (pull from the pool) and release (return to the pool).</li>
  <li>EIPs allow you to maintain a set of IP addresses that remain fixed while the underlying infrastructure may change over time.</li>
  <li>You must first allocate an EIP for use within a VPC and then assign it to an instance.</li>
  <li>EIPs are specific to a region (that is, an EIP in one region cannot be assigned to an instance within an VPC in a different region).</li>
  <li>There is a one-to-one relationship between network interfaces and EIPs.</li>
  <li>You can move EIPs from one instance to another, either in the same VPC or a different VPC within the same region.</li>
  <li>EIPs remain associated with your AWS account until you explicitly release them.</li>
  <li>There are charges for EIPs allocated to your account, even when they are not associated with a resource.</li>
</ul>

<h3 id="enis-elastic-network-interfaces">ENIs (Elastic Network Interfaces)</h3>

<ul>
  <li>An ENI is a virtual network interface that you can attach to an instance in an VPC.</li>
  <li>ENIs are only available within an VPC, and they are associated with a subnet upon creation.</li>
  <li>They can have 1 public IP address, 1 primary private IP and multiple non-primary private IPs.</li>
  <li>Assigning a second network interface to an instance via an ENI allows it to be <em>dual-homed</em> (have network presence in different subnets).</li>
  <li>An ENI created independently of a particular instance persists regardless of the lifetime of any instance to which it is attached; if an underlying instance fails, the IP address may be preserved by attaching the ENI to a replacement instance.</li>
</ul>

<h3 id="endpoints">Endpoints</h3>

<ul>
  <li>An VPC endpoint enables you to create a private connection between your VPC and another AWS service without requiring access over the Internet or through a NAT instance, VPN connection, or AWS Direct Connect.</li>
  <li>Multiple endpoints can be created for a single service, and use different route tables to enforce different access policies from different subnets to the same service.</li>
  <li>To create an VPC endpoint:
    <ul>
      <li>Specify the <strong>VPC</strong>.</li>
      <li>Specify the <strong>service</strong>. A service is identified by a prefix list of the form <code>com.amazonaws.&lt;region&gt;.&lt;service&gt;</code>.</li>
      <li>Specify the <strong>policy</strong>. You can allow full access or create a custom policy. This policy can be changed at any time.</li>
      <li>Specify the <strong>route tables</strong>. A route will be added to each specified route table, which will state the service as the destination and the endpoint as the target.</li>
    </ul>
  </li>
</ul>

<p>Below route table directs all Internet traffic (0.0.0.0/0) to an IGW. Any traffic destined to another service will also be sent to the IGW in order to reach that service.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Destination</th>
      <th style="text-align: left">Target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">10.0.0.0/16</td>
      <td style="text-align: left">Local</td>
    </tr>
    <tr>
      <td style="text-align: left">0.0.0.0/0</td>
      <td style="text-align: left">igw-1a2b3c4d</td>
    </tr>
  </tbody>
</table>

<p>Below route table adds the route to direct S3 traffic to the VPC Endpoint</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Destination</th>
      <th style="text-align: left">Target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">10.0.0.0/16</td>
      <td style="text-align: left">Local</td>
    </tr>
    <tr>
      <td style="text-align: left">0.0.0.0/0</td>
      <td style="text-align: left">igw-1a2b3c4d</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>p1-1a2b3c4d</strong></td>
      <td style="text-align: left"><strong>vpce-1a2b3c4d</strong></td>
    </tr>
  </tbody>
</table>

<h3 id="peering">Peering</h3>

<ul>
  <li>An VPC peering connection is a networking connection between two VPCs that enables instances in either VPC to communicate with each other as if they are within the same network.</li>
  <li>Peering connection can be created
    <ul>
      <li>between your own VPCs or</li>
      <li>with an VPC in another AWS account within a single region.</li>
    </ul>
  </li>
  <li>A peering connection is neither a gateway nor an Amazon VPN connection and does not introduce a single point of failure for communication.</li>
  <li>Peering connections are created through a <em>request/accept</em> protocol. The owner of the requesting VPC sends a request to peer to the owner of the peer VPC.
    <ul>
      <li>If the peer VPC is within the same account, it is identified by its <em>VPC ID</em>.</li>
      <li>If the peer VPC is within a different account, it is identified by <em>Account ID</em> and <em>VPC ID</em>.</li>
    </ul>
  </li>
  <li>The owner of the peer VPC has one week to accept or reject the request before it expires.</li>
  <li>A VPC may have multiple peering connections, and peering is a one-to-one relationship between VPC.</li>
  <li>Peering connections DO NOT support <em>transitive routing</em>. In other words, if there are peering connections between VPC A &amp; B, and VPC B &amp; C, it does NOT mean VPC A &amp; C can peer directly with each other.</li>
  <li>A peering connection CANNOT be created between VPCs that have matching or overlapping CIDR blocks.</li>
  <li>A peering connection CANNOT be created between VPCs in different regions.</li>
  <li>There CANNOT have more than one peering connection between the same two VPCs at the same time.</li>
</ul>

<h3 id="security-groups">Security Groups</h3>

<ul>
  <li>A security group is a virtual stateful firewall that controls inbound and outbound network traffic to AWS resources and EC2 instances.</li>
  <li>All EC2 instances MUST be launched into a security group.</li>
  <li><strong>Default Security Group</strong>
    <ul>
      <li>If an SG is not specified at launch, then the instance will be launched into the <em>default SG</em> for the VPC.</li>
      <li>The default SG allows communication between all resources within the security group,
        <ul>
          <li>allows all outbound traffic, and</li>
          <li>denies all other traffic.</li>
        </ul>
      </li>
      <li>Default SG CANNOT be deleted.</li>
      <li>Rules in default SG can be changed.</li>
    </ul>
  </li>
  <li>Up to 500 SGs can be created per VPC.</li>
  <li>Up to 50 inbound and 50 outbound rules can be added per SG.</li>
  <li>Up to 5 SGs can be associated per Network Interface.</li>
  <li>To apply more than 100 rules to an instance, associate multiple SGs.</li>
  <li>You can specify <em>allow rules</em>, but NOT <em>deny rules</em>. This is an important difference between SGs and ACLs.</li>
  <li>can specify separate rules for inbound and outbound traffic.</li>
  <li>By default, no inbound traffic is allowed until added.</li>
  <li>By default, new SGs have an outbound rule that allows all outbound traffic. You can remove the rule and add your custom rules.</li>
  <li>SGs are stateful. This means that responses to allowed inbound traffic are allowed to flow outbound regardless of outbound rules and vice versa. This is an important difference between security groups and network ACLs.</li>
  <li>Instances associated with the same security group CANNOT talk to each other unless you add rules allowing it (with the exception being the default security group).</li>
  <li>SGs associated with an instance can be changed after launch, and changes will take effect immediately.</li>
</ul>

<p><strong>Sample Security Group</strong></p>

<p><strong>Inbound</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Inbound</th>
      <th style="text-align: left">Source</th>
      <th style="text-align: left">Protocol</th>
      <th style="text-align: left">Port Range</th>
      <th style="text-align: left">Comments</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"> </td>
      <td style="text-align: left">sg-xxxxxxxx</td>
      <td style="text-align: left">All</td>
      <td style="text-align: left">All</td>
      <td style="text-align: left">Allow inbound traffic from instances within the same SG</td>
    </tr>
    <tr>
      <td style="text-align: left"> </td>
      <td style="text-align: left">0.0.0.0/0</td>
      <td style="text-align: left">TCP</td>
      <td style="text-align: left">8</td>
      <td style="text-align: left">Allow inbound traffic from the Internet port 80</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Outbound</th>
      <th style="text-align: left">Destination</th>
      <th style="text-align: left">Protocol</th>
      <th style="text-align: left">Port Range</th>
      <th style="text-align: left">Comments</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"> </td>
      <td style="text-align: left">0.0.0.0/0</td>
      <td style="text-align: left">All</td>
      <td style="text-align: left">All</td>
      <td style="text-align: left">Allow all outbound traffic</td>
    </tr>
  </tbody>
</table>

<h3 id="network-acls-access-control-lists">Network ACLs (Access Control Lists)</h3>

<ul>
  <li>ACL is a stateless firewall on a subnet level.</li>
  <li>This is another layer of security in addition to SGs.</li>
  <li>A network ACL is a numbered list of rules that AWS evaluates in order, starting with the lowest numbered rule, to determine whether traffic is allowed in or out of any subnet associated with the network ACL.</li>
  <li>VPCs are created with a modifiable default network ACL associated with every subnet that allows all inbound and outbound traffic.</li>
  <li>When you create a custom network ACL, its initial configuration will deny all inbound and outbound traffic</li>
  <li>You may set up network ACLs with rules similar to your security groups in order to add a layer of security to your VPC, or you may choose to use the default network ACL that does not filter traffic traversing the subnet boundary.</li>
  <li>Overall, every subnet must be associated with a network ACL.</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Security Group</th>
      <th style="text-align: left">Network ACL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Operates at the instance level (1st layer of defense)</td>
      <td style="text-align: left">Operates at the subnet level (2nd layer of defense)</td>
    </tr>
    <tr>
      <td style="text-align: left">Supports <em>allow</em> rules only</td>
      <td style="text-align: left">Supports both <em>allow</em> and <em>deny</em> rules</td>
    </tr>
    <tr>
      <td style="text-align: left">Stateful firewall: Return traffic automatically allowed</td>
      <td style="text-align: left">Stateless firewall: Return traffic must be explicitly allowed by rules</td>
    </tr>
    <tr>
      <td style="text-align: left">AWS evaluates all rules before deciding whether to allow traffic</td>
      <td style="text-align: left">AWS processes rules in number order when deciding whether to allow traffic</td>
    </tr>
    <tr>
      <td style="text-align: left">Applied selectively to individual instances</td>
      <td style="text-align: left">Automatically applied to all instances in the associated subnets; this is a backup layer of defense, so you don’t have to rely on someone specifying the security group</td>
    </tr>
  </tbody>
</table>

<h3 id="nat-instances-network-address-translation">NAT Instances (Network Address Translation)</h3>

<ul>
  <li>NAT instances and NAT gateways allow instances deployed in private subnets to gain Internet access.</li>
  <li>By default, any instance that you launch into a private subnet in an VPC is not able to communicate with the Internet through the IGW.</li>
  <li>NAT instance is an Amazon Linux AMI designed to accept traffic from instances within a private subnet, translate the source IP address to the public IP address of the NAT instance, and forward the traffic to the IGW.</li>
  <li>In addition, the NAT instance maintains the state of the forwarded traffic in order to return response traffic from the Internet to the proper instance in the private subnet.</li>
  <li>These instances have the string <code>amzn-ami-vpc-nat</code> in their names, which is searchable in the EC2 console.</li>
  <li>To allow instances within a private subnet to access Internet resources through the IGW via a NAT instance, you must do the following:
    <ul>
      <li>Create a security group for the NAT with outbound rules that specify the needed Internet resources by port, protocol, and IP address.</li>
      <li>Launch an Amazon Linux NAT AMI as an instance in a public subnet and associate it with the NAT security group.</li>
      <li>Disable the Source/Destination Check attribute of the NAT.</li>
      <li>Configure the route table associated with a private subnet to direct Internet-bound traffic to the NAT instance (for example, i-1a2b3c4d).</li>
      <li>Allocate an EIP and associate it with the NAT instance.</li>
      <li>This configuration allows instances in private subnets to send outbound Internet communication, but it prevents the instances from receiving inbound traffic initiated by someone on the Internet.</li>
    </ul>
  </li>
</ul>

<h3 id="nat-gateways">NAT Gateways</h3>

<ul>
  <li>A NAT gateway is an Amazon managed resource that is designed to operate just like a NAT instance, but it is simpler to manage, requires less administrative effort and highly available within an Availability Zone.</li>
  <li>For common use cases, use a NAT gateway instead of a NAT instance.</li>
  <li>To allow instances within a private subnet to access Internet resources through the IGW via a NAT gateway, you must do the following:
    <ul>
      <li>Configure the route table associated with the private subnet to direct Internet-bound traffic to the NAT gateway (for example, nat-1a2b3c4d).</li>
      <li>Allocate an EIP and associate it with the NAT gateway.</li>
    </ul>
  </li>
  <li>Like a NAT instance, this managed service allows outbound Internet communication and prevents the instances from receiving inbound traffic initiated by someone on the Internet.</li>
  <li>To create an Availability Zone-independent architecture, create a NAT gateway in each Availability Zone and configure your routing to ensure that resources use the NAT gateway in the same Availability Zone.</li>
</ul>

<h3 id="vpgs-cgws-and-vpns">VPGs, CGWs and VPNs</h3>

<p><img class="right" src="/technology/aws-vpg-vpn.PNG" /></p>

<ul>
  <li>VPC offers two ways to connect a corporate network to a VPC: VPG and CGW.</li>
  <li><strong>VPG (Virtual Private Gateway)</strong>
    <ul>
      <li>is the virtual private network (VPN) concentrator on the AWS side of the VPN connection between the two networks.</li>
      <li>The VPG is the AWS end of the VPN tunnel.</li>
      <li>VPGs support both dynamic routing with <em>Border Gateway Protocol (BGP)</em> and static routing.</li>
    </ul>
  </li>
  <li><strong>Customer gateway (CGW)</strong>
    <ul>
      <li>The CGW is a hardware or software application on the customer’s side of the VPN tunnel.</li>
      <li>VPC will provide the information needed by the network administrator to configure the CGW and establish the VPN connection with the VPG.</li>
      <li>VPC also supports multiple CGWs, each having a VPN connection to a single VPG (many-to-one design). In order to support this topology, the CGW IP addresses must be unique within the region.</li>
    </ul>
  </li>
  <li><strong>Virtual Private Network (VPN)</strong>
    <ul>
      <li>After these VPG and CGW are created, the last step is to create a VPN tunnel.</li>
      <li>VPN tunnel MUST be initiated from the CGW to the VPG.</li>
      <li>You must specify the type of routing that you plan to use when you create a VPN connection.
        <ul>
          <li>If the CGW supports <em>Border Gateway Protocol (BGP)</em>, then configure the VPN connection for dynamic routing.</li>
          <li>Otherwise, configure the connections for static routing. If you will be using static routing, you must enter the routes for your network that should be communicated to the VPG. Routes will be propagated to the VPC to allow your resources to route network traffic back to the corporate network through the VGW and across the VPN tunnel.</li>
        </ul>
      </li>
      <li>The VPN connection consists of two <em>Internet Protocol Security (IPSec)</em> tunnels for higher availability to the VPC.</li>
    </ul>
  </li>
</ul>

<h2 id="aws-direct-connect">AWS Direct Connect</h2>

<ul>
  <li>allows organizations to establish a dedicated n/w connection from their data center to AWS.</li>
</ul>

<h2 id="amazon-route-53">Amazon Route 53</h2>

<p><a href="/technology/networking.html">Read about DNS basics here</a></p>

<ul>
  <li>is a highly available and scalable DNS web service. Also serves as domain registrar, allowing you to purchase and manage domains directly from AWS</li>
  <li>Amazon Route 53 performs 3 main functions. You can use none or any combination of these functions:
    <ul>
      <li>Domain registration</li>
      <li>DNS service</li>
      <li>Health checking</li>
    </ul>
  </li>
  <li><em>Domain Registration</em>
    <ul>
      <li>a registered website can be transfered to Route 53</li>
      <li>It isn’t required to use Route 53 as your DNS service or to configure health checking for your resources.</li>
      <li>Route 53 supports a wide variety of generic TLDs (for example, .com and .org) and geographic TLDs (for example, .be and .us).</li>
    </ul>
  </li>
  <li><em>DNS Service</em>
    <ul>
      <li>Route 53 responds to DNS queries using a global network of authoritative DNS servers, which reduces latency.</li>
      <li>To comply with DNS standards, responses sent over UDP protocol are limited to <em>512 bytes</em> in size.</li>
      <li>Responses exceeding 512 bytes are truncated, and the resolver must re-issue the request over TCP.</li>
      <li>If you register a new domain name with Route 53, then Route 53 will be automatically configured as the DNS service for the domain, and a <strong><em>hosted zone</em></strong> will be created for your domain.</li>
      <li>You add resource record sets to the hosted zone, which define how you want Route 53 to respond to DNS queries for your domain (for example, with the IP address for a web server, the IP address for the nearest CloudFront edge location, or the IP address for an Elastic Load Balancing load balancer).</li>
      <li>If you registered your domain with another domain registrar, that registrar is probably providing the DNS service for your domain. You can transfer DNS service to Route 53, with or without transferring registration for the domain.</li>
    </ul>
  </li>
  <li><em>Health Check</em>
    <ul>
      <li>Route 53 sends automated requests over the Internet to your application to verify that it’s reachable, available, and functional.</li>
      <li>Route 53 health checks monitor the health of your resources such as web servers and email servers.</li>
      <li>You can configure CloudWatch alarms for your health checks so that you receive notification when a resource becomes unavailable.</li>
      <li>You can also configure Route 53 to route Internet traffic away from resources that are unavailable.</li>
      <li>High Availability and Resiliency
        <ul>
          <li>Health checks and DNS failover are major tools in the Route 53 feature set that help make your application highly available and resilient to failures.</li>
          <li>If you deploy an application in multiple Availability Zones and multiple AWS regions, with Route 53 health checks attached to every endpoint, Route 53 can send back a list of healthy endpoints only.</li>
          <li>Health checks can automatically switch to a healthy endpoint with minimal disruption to your clients and without any configuration changes.</li>
          <li>You can use this automatic recovery scenario in <em>active-active</em> or <em>active-passive</em> setups, depending on whether your additional endpoints are always hit by live traffic or only after all primary endpoints have failed.</li>
          <li>Using health checks and automatic failovers, Route 53 improves your service uptime, especially when compared to the traditional <strong><em>monitor-alert-restart</em></strong> approach of addressing failures.</li>
          <li>Route 53 health checks are not triggered by DNS queries; they are run periodically by AWS, and results are published to all DNS servers. This way, name servers can be aware of an unhealthy endpoint and route differently within approximately 30 seconds of a problem (after three failed tests in a row), and new DNS results will be known to clients a minute later (assuming your TTL is 60 seconds), bringing complete recovery time to about a minute and a half in total in this scenario.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><em>Hosted Zones</em>
    <ul>
      <li>A hosted zone is a collection of resource record sets hosted by Route 53.</li>
      <li>Like a traditional DNS zone file, a hosted zone represents resource record sets that are managed together under a single domain name.</li>
      <li>Each hosted zone has its own metadata and configuration information.</li>
      <li>2 types of hosted zones:
        <ul>
          <li>Private hosted zone: is a container that holds information about how you want to route traffic for a domain and its subdomains within one or more VPCs.</li>
          <li>Public hosted zone: is a container that holds information about how you want to route traffic on the Internet for a domain (for example, <code>example.com</code>) and its subdomains (for example, <code>apex.example.com</code> and <code>acme.example.com</code>).</li>
        </ul>
      </li>
      <li>The resource record sets contained in a hosted zone must share the same suffix. e.g., the <code>example.com</code> hosted zone can contain resource record sets for the <code>www.example.com</code> and <code>www.aws.example.com</code> subdomains, but it cannot contain resource record sets for a <code>www.example.ca</code> subdomain.</li>
      <li>You can use S3 to host your static website at the hosted zone (for example, <code>domain.com</code>) and redirect all requests to a subdomain (for example, <code>www.domain.com</code>). Then, in Route 53, you can create an alias resource record that sends requests for the root domain to the S3 bucket.</li>
      <li>DO NOT use CNAME for your hosted zone. Use <em>an alias record</em>. CNAMEs are not allowed for hosted zones in Route 53.</li>
      <li>DO NOT use <em>A records</em> for subdomains (for example, <code>www.domain.com</code>), as they refer to hardcoded IP addresses. Use Route 53 <em>alias records</em> or traditional CNAME records to always point to the right resource, wherever your site is hosted, even when the physical server has changed its IP address.</li>
    </ul>
  </li>
</ul>

<h3 id="supported-record-types">Supported Record Types</h3>

<ul>
  <li>Route 53 supports the following DNS resource record types:
    <ul>
      <li><code>A</code></li>
      <li><code>AAAA</code></li>
      <li><code>CNAME</code></li>
      <li><code>MX</code></li>
      <li><code>NS</code></li>
      <li><code>PTR</code></li>
      <li><code>SOA</code></li>
      <li><code>SPF</code></li>
      <li><code>SRV</code></li>
      <li><code>TXT</code></li>
    </ul>
  </li>
  <li>When you access  Route 53 using the API, you will see examples of how to format the <code>Value</code> element for each record type.</li>
</ul>

<p><strong>Routing Policies</strong></p>

<ul>
  <li>When you create a resource record set, you choose a routing policy, which determines how Route 53 responds to queries.</li>
  <li>Routing policy options: simple, weighted, latency-based, failover, and geolocation.</li>
  <li>When specified, Route 53 evaluates a resource’s relative weight, the client’s network latency to the resource, or the client’s geographical location when deciding which resource to send back in a DNS response.</li>
  <li>Routing policies can be associated with health checks, so resource health status is considered before it even becomes a candidate in a conditional decision tree.</li>
  <li><strong><em>Simple</em></strong>
    <ul>
      <li>Default routing policy when a new resource is created.</li>
      <li>Use a simple routing policy when you have a single resource that performs a given function for your domain (for example, one web server that serves content for the <code>example.com</code> website). In this case, Route 53 responds to DNS queries based only on the values in the resource record set (for example, the IP address in an <em>A record</em>).</li>
    </ul>
  </li>
  <li><strong><em>Weighted</em></strong>
    <ul>
      <li>With weighted DNS, you can associate multiple resources (such as EC2 instances or ELB load balancers) with a single DNS name.</li>
      <li>Use the weighted routing policy when you have multiple resources that perform the same function (such as web servers that serve the same website), and you want Route 53 to route traffic to those resources in proportions that you specify. For example, you may use this for load balancing between different AWS regions or to test new versions of your website (you can send 10 percent of traffic to the test environment and 90 percent of traffic to the older version of your website).</li>
      <li>To create a group of weighted resource record sets, you need to create two or more resource record sets that have the same DNS name and type. You then assign each resource record set a unique identifier and a relative weight.</li>
      <li>When processing a DNS query, Route 53 searches for a resource record set or a group of resource record sets that have the same name and DNS record type (such as an A record). Route 53 then selects one record from the group.</li>
      <li>The probability of any resource record set being selected is governed by the following formula: <code>Weight for a given resource record set / Sum of the weights for the resource record sets in the group</code></li>
    </ul>
  </li>
  <li><strong><em>Latency-Based</em></strong>
    <ul>
      <li>Latency-based routing allows you to route your traffic based on the lowest network latency for your end user (for example, using the AWS region that will give them the fastest response time).</li>
      <li>Use the latency routing policy when you have resources that perform the same function in multiple AWS Availability Zones or regions and you want Route 53 to respond to DNS queries using the resources that provide the best latency. For example, suppose you have ELB load balancers in the U.S. East region and in the Asia Pacific region, and you created a latency resource record set in Route 53 for each load balancer.</li>
      <li>A user in London enters the name of your domain in a browser, and DNS routes the request to an Route 53 name server. Route 53 refers to its data on latency between London and the Singapore region and between London and the Oregon region. If latency is lower between London and the Oregon region, Route 53 responds to the user’s request with the IP address of your load balancer in Oregon. If latency is lower between London and the Singapore region, Route 53 responds with the IP address of your load balancer in Singapore.</li>
    </ul>
  </li>
  <li><strong><em>Failover</em></strong>
    <ul>
      <li>Use a failover routing policy to configure active-passive failover, in which one resource takes all the traffic when it’s available and the other resource takes all the traffic when the first resource isn’t available.</li>
      <li>Note: you can’t create failover resource record sets for private hosted zones.</li>
      <li>For example, you might want your primary resource record set to be in U.S. West (N. California) and your secondary, Disaster Recovery (DR) resource(s) to be in U.S. East (N. Virginia). Route 53 will monitor the health of your primary resource endpoints using a health check.</li>
      <li>A health check tells Route 53 how to send requests to the endpoint whose health you want to check: which protocol to use (HTTP, HTTPS, or TCP), which IP address and port to use, and, for HTTP/HTTPS health checks, a domain name and path.</li>
      <li>After you have configured a health check, Amazon will monitor the health of your selected DNS endpoint. If your health check fails, then failover routing policies will be applied and your DNS will fail over to your DR site.</li>
    </ul>
  </li>
  <li><strong><em>Geolocation</em></strong>
    <ul>
      <li>Geolocation routing lets you choose where Route 53 will send your traffic based on the geographic location of your users (the location from which DNS queries originate). For example, you might want all queries from Europe to be routed to a fleet of EC2 instances that are specifically configured for your European customers, with local languages and pricing in Euros.</li>
      <li>can also use geolocation routing to restrict distribution of content to only the locations in which you have distribution rights. Another possible use is for balancing load across endpoints in a predictable, easy-to-manage way so that each user location is consistently routed to the same endpoint.</li>
      <li>can specify geographic locations by continent, by country, or even by state in the United States.</li>
      <li>You can also create separate resource record sets for overlapping geographic regions, and priority goes to the smallest geographic region. For example, you might have one resource record set for Europe and one for the UK. This allows you to route some queries for selected countries (in this example, the United Kingdom) to one resource and to route queries for the rest of the continent (in this example, Europe) to a different resource.</li>
      <li>Geolocation works by mapping IP addresses to locations. You should be cautious, however, as some IP addresses aren’t mapped to geographic locations. Even if you create geolocation resource record sets that cover all seven continents, Route 53 will receive some DNS queries from locations that it can’t identify. In this case, you can create a default resource record set that handles both queries from IP addresses that aren’t mapped to any location and queries that come from locations for which you haven’t created geolocation resource record sets.</li>
      <li>If you don’t create a default resource record set, Route 53 returns a “no answer” response for queries from those locations.</li>
      <li>You cannot create two geolocation resource record sets that specify the same geographic location.</li>
      <li>You also cannot create geolocation resource record sets that have the same values for “Name” and “Type” as the “Name” and “Type” of non-geolocation resource record sets.</li>
    </ul>
  </li>
</ul>

<h3 id="route-53-enables-resiliency">Route 53 Enables Resiliency</h3>

<p>When pulling these concepts together to build an application that is highly available and resilient to failures, consider these building blocks:</p>

<ul>
  <li>In every AWS region, an ELB load balancer is set up with cross-zone load balancing and connection draining. This distributes the load evenly across all instances in all Availability Zones, and it ensures requests in flight are fully served before an EC2 instance is disconnected from an ELB load balancer for any reason.</li>
  <li>Each ELB load balancer delegates requests to EC2 instances running in multiple Availability Zones in an auto-scaling group. This protects the application from Availability Zone outages, ensures that a minimal amount of instances is always running, and responds to changes in load by properly scaling each group’s EC2 instances.</li>
  <li>Each ELB load balancer has health checks defined to ensure that it delegates requests only to healthy instances.</li>
  <li>Each ELB load balancer also has an Route 53 health check associated with it to ensure that requests are routed only to load balancers that have healthy EC2 instances.</li>
  <li>The application’s production environment (for example, <code>prod.domain.com</code>) has Route 53 alias records that point to ELB load balancers. The production environment also uses a latency-based routing policy that is associated with ELB health checks. This ensures that requests are routed to a healthy load balancer, thereby providing minimal latency to a client.</li>
  <li>The application’s failover environment (for example, <code>fail.domain.com</code>) has an Route 53 alias record that points to a CloudFront distribution of an S3 bucket hosting a static version of the application.</li>
  <li>The application’s subdomain (for example, <code>www.domain.com</code>) has an Route 53 alias record that points to <code>prod.domain.com</code> (as primary target) and <code>fail.domain.com</code> (as secondary target) using a failover routing policy. This ensures <code>www.domain.com</code> routes to the production load balancers if at least one of them is healthy or the “fail whale” if all of them appear to be unhealthy.</li>
  <li>The application’s hosted zone (for example, domain.com) has an Route 53 alias record that redirects requests to www.domain.com using an S3 bucket of the same name.</li>
  <li>Application content (both static and dynamic) can be served using  CloudFront. This ensures that the content is delivered to clients from CloudFront edge locations spread all over the world to provide minimal latency. Serving dynamic content from a CDN, where it is cached for short periods of time (that is, several seconds), takes the load off of the application and further improves its latency and responsiveness.</li>
  <li>The application is deployed in multiple AWS regions, protecting it from a regional outage.</li>
</ul>

<hr />

<h1 id="database-service">Database service</h1>

<ul>
  <li><strong>Data Warehouses</strong>
    <ul>
      <li>A data warehouse is a central repository for data that can come from one or more sources.</li>
      <li>RDS - used for OLTP, but can be used for OLAP</li>
      <li>Redshift is a high-performance data warehouse designed specifically for OLAP.</li>
      <li>It is also common to combine RDS with Redshift in the same application and periodically extract recent transactions and load them into a reporting database.</li>
    </ul>
  </li>
  <li><strong>NoSQL databases</strong>
    <ul>
      <li>Traditional relational databases are difficult to scale beyond a single server without significant engineering and cost, but a NoSQL architecture allows for horizontal scalability on commodity hardware.</li>
      <li>NoSQL database can be installed and run on EC2 instances, or choose a managed service like Amazon DynamoDB to deal with the heavy lifting involved with building a distributed cluster spanning multiple data centers.</li>
    </ul>
  </li>
</ul>

<h2 id="amazon-relational-database-service-rds">Amazon Relational Database Service (RDS)</h2>

<ul>
  <li>RDS is a service that simplifies the setup, operations, and scaling of a relational database on AWS. With Amazon</li>
  <li>RDS offloads common tasks like backups, patching, scaling, and replication from user.</li>
  <li>RDS helps you to streamline the installation of the database software and also the provisioning of infrastructure capacity.</li>
  <li>After the initial launch, RDS simplifies ongoing maintenance by automating common administrative tasks on a recurring basis.</li>
  <li>With RDS, you can accelerate your development timelines and establish a consistent operating model for managing relational databases. For example, RDS makes it easy to replicate your data to increase availability, improve durability, or scale up or beyond a single database instance for read-heavy database workloads.</li>
  <li>RDS exposes a database endpoint to connect and execute SQL.</li>
  <li>RDS does not provide shell access to DB Instances, and restricts access to certain system procedures and tables that require advanced privileges.</li>
</ul>

<p><strong>Database Instances</strong></p>

<ul>
  <li>The RDS service itself provides an API that lets you create and manage one or more DB Instances.</li>
  <li>A DB Instance is an isolated database environment deployed in your private network segments in the cloud.</li>
  <li>Each DB Instance runs and manages a popular commercial or open source database engine on your behalf.</li>
  <li>To launch a new DB Instance: use AWS console or call <code>CreateDBInstance</code> API</li>
  <li>To change or resize existing DB Instances: <code>ModifyDBInstance</code> API.</li>
  <li>A DB Instance can contain multiple different databases, all of which you create and manage within the DB Instance itself by executing SQL commands with the RDS endpoint.</li>
  <li><em>DB Instance Class</em>
    <ul>
      <li>The compute and memory resources of a DB Instance are determined by its DB Instance class.</li>
      <li>The range of DB Instance classes extends from a <em>db.t2.micro</em> with 1 virtual CPU (vCPU) and 1 GB of memory, up to a db.r3.8xlarge with 32 vCPUs and 244 GB of memory.</li>
      <li>Instance class can changed, and RDS will migrate data to a larger or smaller instance class.</li>
      <li>Size and performance characteristics of the storage used can be controlled independent from the DB Instance class selected.</li>
    </ul>
  </li>
  <li>Many features and common configuration settings are exposed and managed using DB parameter groups and DB option groups.</li>
  <li><em>DB parameter group</em>
    <ul>
      <li>A DB parameter group acts as a container for engine configuration values that can be applied to one or more DB Instances.</li>
      <li>Change the DB parameter group for an existing instance requires a reboot.</li>
    </ul>
  </li>
  <li><em>DB option group</em>
    <ul>
      <li>A DB option group acts as a container for engine features, which is empty by default.</li>
      <li>To enable specific features of a DB engine (for example, Oracle Statspack, Microsoft SQL Server Mirroring), create a new DB option group and configure the settings accordingly.</li>
    </ul>
  </li>
  <li><em>Data migration</em>
    <ul>
      <li>Existing databases can be migrated to RDS using native tools and techniques that vary depending on the engine. For example with MySQL, you can export a backup using mysqldump and import the file into RDS MySQL.</li>
      <li>AWS Database Migration Service gives you a graphical interface that simplifies the migration of both schema and data between databases.</li>
      <li>AWS Database Migration Service helps convert databases from one database engine to another.</li>
    </ul>
  </li>
</ul>

<p><strong>Operational Benefits</strong></p>

<ul>
  <li>RDS increases the operational reliability of your databases by applying a very consistent deployment and operational model.</li>
  <li>This level of consistency is achieved in part by limiting the types of changes that can be made to the underlying infrastructure and through the extensive use of automation.</li>
  <li>For example with RDS, you cannot use Secure Shell (SSH) to log in to the host instance and install a custom piece of software. You can, however, connect using SQL administrator tools or use DB option groups and DB parameter groups to change the behavior or feature configuration for a DB Instance.</li>
  <li>If you want full control of the OS or require elevated permissions to run, then consider installing your database on EC2 instead of RDS.</li>
  <li>RDS is designed to simplify the common tasks required to operate a relational database in a reliable manner.</li>
</ul>

<p><em>Comparison of Operational Responsibilities</em></p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Responsibility</th>
      <th style="text-align: left">Database On-Premise</th>
      <th style="text-align: left">Database on EC2</th>
      <th style="text-align: left">Database on RDS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">App Optimization</td>
      <td style="text-align: left">You</td>
      <td style="text-align: left">You</td>
      <td style="text-align: left">You</td>
    </tr>
    <tr>
      <td style="text-align: left">Scaling</td>
      <td style="text-align: left">You</td>
      <td style="text-align: left">You</td>
      <td style="text-align: left">AWS</td>
    </tr>
    <tr>
      <td style="text-align: left">High Availability</td>
      <td style="text-align: left">You</td>
      <td style="text-align: left">You</td>
      <td style="text-align: left">AWS</td>
    </tr>
    <tr>
      <td style="text-align: left">Backups</td>
      <td style="text-align: left">You</td>
      <td style="text-align: left">You</td>
      <td style="text-align: left">AWS</td>
    </tr>
    <tr>
      <td style="text-align: left">DB Engine Patches</td>
      <td style="text-align: left">You</td>
      <td style="text-align: left">You</td>
      <td style="text-align: left">AWS</td>
    </tr>
    <tr>
      <td style="text-align: left">Software Installation</td>
      <td style="text-align: left">You</td>
      <td style="text-align: left">You</td>
      <td style="text-align: left">AWS</td>
    </tr>
    <tr>
      <td style="text-align: left">OS Patches</td>
      <td style="text-align: left">You</td>
      <td style="text-align: left">You</td>
      <td style="text-align: left">AWS</td>
    </tr>
    <tr>
      <td style="text-align: left">OS Installation</td>
      <td style="text-align: left">You</td>
      <td style="text-align: left">AWS</td>
      <td style="text-align: left">AWS</td>
    </tr>
    <tr>
      <td style="text-align: left">Server Maintenance</td>
      <td style="text-align: left">You</td>
      <td style="text-align: left">AWS</td>
      <td style="text-align: left">AWS</td>
    </tr>
    <tr>
      <td style="text-align: left">Rack and Stack</td>
      <td style="text-align: left">You</td>
      <td style="text-align: left">AWS</td>
      <td style="text-align: left">AWS</td>
    </tr>
    <tr>
      <td style="text-align: left">Power and Cooling</td>
      <td style="text-align: left">You</td>
      <td style="text-align: left">AWS</td>
      <td style="text-align: left">AWS</td>
    </tr>
  </tbody>
</table>

<p><strong>Database Engines</strong></p>

<ul>
  <li>RDS supports six database engines: MySQL, PostgreSQL, MariaDB, Oracle, SQL Server, and Amazon Aurora. Features and capabilities vary slightly depending on the engine that you select.</li>
  <li><strong><em>MySQL</em></strong>
    <ul>
      <li>RDS for MySQL currently supports MySQL 5.7, 5.6, 5.5, and 5.1.</li>
      <li>The engine is running the open source Community Edition with <em>InnoDB</em> as the default and recommended database storage engine.</li>
      <li>RDS MySQL allows you to connect using standard MySQL tools such as MySQL Workbench or SQL Workbench/J.</li>
      <li>RDS MySQL supports Multi-AZ deployments for high availability and read replicas for horizontal scaling.</li>
    </ul>
  </li>
  <li><strong><em>PostgreSQL</em></strong>
    <ul>
      <li>RDS supports PostgreSQL 9.5.x, 9.4.x, and 9.3.x.</li>
      <li>RDS PostgreSQL can be managed using standard tools like <code>pgAdmin</code> and supports standard JDBC/ODBC drivers.</li>
      <li>supports Multi-AZ deployment for high availability and read replicas for horizontal scaling.</li>
    </ul>
  </li>
  <li><strong><em>MariaDB</em></strong>
    <ul>
      <li>MariaDB is an open source database engine built by the creators of MySQL and enhanced with enterprise tools and functionality.</li>
      <li>MariaDB adds features that enhance the performance, availability, and scalability of MySQL.</li>
      <li>AWS supports MariaDB 10.0.17.</li>
      <li>RDS fully supports the <code>XtraDB</code> storage engine for MariaDB DB Instances and, like RDS MySQL and PostgreSQL, has support for Multi-AZ deployment and read replicas.</li>
    </ul>
  </li>
  <li><strong><em>Oracle</em></strong>
    <ul>
      <li>RDS supports DB Instances running several editions of Oracle 11g and Oracle 12c.</li>
      <li>RDS supports access to schemas on a DB Instance using any standard SQL client application, such as Oracle SQL Plus.</li>
      <li>RDS Oracle supports 3 different editions of the popular database engine: Standard Edition One, Standard Edition, and Enterprise Edition.</li>
    </ul>
  </li>
</ul>

<p><em>RDS Oracle Editions Compared</em></p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Edition</th>
      <th style="text-align: left">Performance</th>
      <th style="text-align: left">Multi-AZ</th>
      <th style="text-align: left">Encryption</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Standard One</td>
      <td style="text-align: left">++++</td>
      <td style="text-align: left">Yes</td>
      <td style="text-align: left">KMS</td>
    </tr>
    <tr>
      <td style="text-align: left">Standard</td>
      <td style="text-align: left">++++++++</td>
      <td style="text-align: left">Yes</td>
      <td style="text-align: left">KMS</td>
    </tr>
    <tr>
      <td style="text-align: left">Enterprise</td>
      <td style="text-align: left">++++++++</td>
      <td style="text-align: left">Yes</td>
      <td style="text-align: left">KMS and TDE</td>
    </tr>
  </tbody>
</table>

<ul>
  <li><strong><em>Microsoft SQL Server</em></strong>
    <ul>
      <li>RDS allows DBAs to connect to their SQL Server DB Instance in the cloud using native tools like SQL Server Management Studio.</li>
      <li>Versions supported: SQL Server 2008 R2, SQL Server 2012, and SQL Server 2014.</li>
      <li>Editions supported: Express Edition, Web Edition, Standard Edition, and Enterprise Edition.</li>
    </ul>
  </li>
</ul>

<p><em>RDS SQL Server Editions Compared</em></p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Edition</th>
      <th style="text-align: left">Performance</th>
      <th style="text-align: left">Multi-AZ</th>
      <th style="text-align: left">Encryption</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Express</td>
      <td style="text-align: left">+</td>
      <td style="text-align: left">No</td>
      <td style="text-align: left">KMS</td>
    </tr>
    <tr>
      <td style="text-align: left">Web</td>
      <td style="text-align: left">++++</td>
      <td style="text-align: left">No</td>
      <td style="text-align: left">KMS</td>
    </tr>
    <tr>
      <td style="text-align: left">Standard</td>
      <td style="text-align: left">++++</td>
      <td style="text-align: left">Yes</td>
      <td style="text-align: left">KMS</td>
    </tr>
    <tr>
      <td style="text-align: left">Enterprise</td>
      <td style="text-align: left">++++++++</td>
      <td style="text-align: left">Yes</td>
      <td style="text-align: left">KMS and TDE</td>
    </tr>
  </tbody>
</table>

<ul>
  <li><strong><em>Licensing</em></strong>
    <ul>
      <li>RDS Oracle and Microsoft SQL Server are commercial software products that require appropriate licenses to operate in the cloud.</li>
      <li>AWS offers two licensing models: License Included and Bring Your Own License (BYOL).</li>
      <li><em>License Included Model</em>:
        <ul>
          <li>In this model, the license is held by AWS and is included in the RDS instance price.</li>
          <li>For Oracle, this provides licensing for Standard Edition One.</li>
          <li>For SQL Server, License Included provides licensing for SQL Server Express Edition, Web Edition, and Standard Edition.</li>
        </ul>
      </li>
      <li><em>Bring Your Own License (BYOL)</em>
        <ul>
          <li>In this model, you provide your own license.</li>
          <li>For Oracle, you must have the appropriate Oracle Database license for the DB Instance class and Oracle Database edition you want to run. You can bring over Standard Edition One, Standard Edition, and Enterprise Edition.</li>
          <li>For SQL Server, you provide your own license under the Microsoft License Mobility program. You can bring over Microsoft SQL Standard Edition and also Enterprise Edition. You are responsible for tracking and managing how licenses are allocated.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong><em>Amazon Aurora</em></strong>
    <ul>
      <li>Aurora offers enterprise-grade commercial database technology while offering the simplicity and cost effectiveness of an open source database.</li>
      <li>This is achieved by redesigning the internal components of MySQL to take a more service-oriented approach.</li>
      <li>Like other RDS engines, Aurora is a fully managed service, is MySQL-compatible out of the box, and provides for increased reliability and performance over standard MySQL deployments.</li>
      <li>Aurora can deliver up to 5x performance of MySQL without requiring changes to most of your existing web applications. You can use the same code, tools, and applications that you use with your existing MySQL databases with Aurora.</li>
      <li><em>DB Cluster</em>
        <ul>
          <li>When you first create an Amazon Aurora instance, you create a DB cluster.</li>
          <li>A DB cluster has one or more instances and includes a cluster volume that manages the data for those instances.</li>
          <li>An Aurora cluster volume is a virtual database storage volume that spans multiple Availability Zones, with each Availability Zone having a copy of the cluster data.</li>
          <li>An Aurora DB cluster consists of 2 different types of instances: Primary Instance and Aurora Replicate instance</li>
          <li><em>Primary Instance</em>:
            <ul>
              <li>This is the main instance, which supports both read and write workloads.</li>
              <li>When you modify your data, you are modifying the primary instance.</li>
              <li>Each Aurora DB cluster has one primary instance.</li>
            </ul>
          </li>
          <li><em>Aurora Replica</em>:
            <ul>
              <li>This is a secondary instance that supports only read operations.</li>
              <li>Each DB cluster can have up to <strong>15</strong> Aurora Replicas in addition to the primary instance.</li>
              <li>By using multiple Aurora Replicas, you can distribute the read workload among various instances, increasing performance.</li>
              <li>You can also locate your Aurora Replicas in multiple Availability Zones to increase your database availability.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Storage Options</strong></p>

<ul>
  <li>RDS is built using EBS and allows you to select the right storage option based on your performance and cost requirements.</li>
  <li>Depending on the database engine and workload, you can scale up to 4 to 6TB in provisioned storage and up to 30,000 IOPS.</li>
  <li>RDS supports 3 storage types:
    <ul>
      <li><em>Magnetic Magnetic storage</em>: also called <em>standard storage</em>, offers cost-effective storage that is ideal for applications with light I/O requirements.</li>
      <li><em>General Purpose (SSD)</em>: also called <em>gp2</em>, can provide faster access than magnetic storage. This storage type can provide burst performance to meet spikes and is excellent for small- to medium-sized databases.</li>
      <li><em>Provisioned IOPS (SSD)</em>: is designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency in random access I/O throughput.</li>
    </ul>
  </li>
  <li>For most applications, General Purpose (SSD) is the best option and provides a good mix of lower-cost and higher-performance characteristics.</li>
</ul>

<p><em>RDS Storage Types</em></p>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: left">Magnetic</th>
      <th style="text-align: left">General Purpose (SSD)</th>
      <th style="text-align: left">Provisioned IOPS (SSD)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Size</td>
      <td style="text-align: left">+++</td>
      <td style="text-align: left">+++++</td>
      <td style="text-align: left">+++++</td>
    </tr>
    <tr>
      <td style="text-align: left">Performance</td>
      <td style="text-align: left">+</td>
      <td style="text-align: left">+++</td>
      <td style="text-align: left">+++++</td>
    </tr>
    <tr>
      <td style="text-align: left">Cost</td>
      <td style="text-align: left">++</td>
      <td style="text-align: left">+++</td>
      <td style="text-align: left">+++++</td>
    </tr>
  </tbody>
</table>

<p><strong>Backup and Recovery</strong></p>

<ul>
  <li>RDS provides a consistent operational model for backup and recovery procedures across the different database engines.</li>
  <li>RDS provides two mechanisms for backing up the database: automated backups and manual snapshots.</li>
  <li>By using a combination of both techniques, you can design a backup recovery model to protect your application data.</li>
  <li><strong><em>Recovery Point Objective (RPO)</em></strong> and <strong><em>Recovery Time Objective (RTO)</em></strong>
    <ul>
      <li><em>RPO</em>
        <ul>
          <li>is defined as the maximum period of data loss that is acceptable in the event of a failure or incident.</li>
          <li>For example, many systems back up transaction logs every 15 minutes to allow them to minimize data loss in the event of an accidental deletion or hardware failure.</li>
        </ul>
      </li>
      <li><em>RTO</em>
        <ul>
          <li>is defined as the maximum amount of downtime that is permitted to recover from backup and to resume processing.</li>
          <li>For large databases in particular, it can take hours to restore from a full backup.</li>
          <li>In the event of a hardware failure, you can reduce your RTO to minutes by failing over to a secondary node.</li>
          <li>You should create a recovery plan that, at a minimum, lets you recover from a recent backup.</li>
        </ul>
      </li>
      <li>Each organization typically will define a RPO and RTO for important applications based on the criticality of the application and the expectations of the users.</li>
      <li>It’s common for enterprise systems to have an RPO measured in minutes and an RTO measured in hours or even days, while some critical applications may have much lower tolerances.</li>
    </ul>
  </li>
  <li><strong><em>Backup Mechanisms</em></strong>
    <ul>
      <li><em>Automated Backups</em>
        <ul>
          <li>continuously tracks changes and backs up database.</li>
          <li>RDS creates a storage volume snapshot of the DB Instance, backing up the entire DB Instance and not just individual databases.</li>
          <li>Backup retention period can be set up while creating a DB Instance.</li>
          <li>Default backup retention period = 1 day. Max retention = 35 days.</li>
          <li>When a DB Instance is deleted, all automated backup snapshots are deleted and cannot be recovered. Manual snapshots, however, are not deleted.</li>
          <li>Automated backups will occur daily during a configurable 30-minute maintenance window called the <em>backup window</em>.</li>
          <li>DB Instance can be restored to any specific time during the retention period, creating a new DB Instance.</li>
        </ul>
      </li>
      <li><em>Manual DB Snapshots</em>
        <ul>
          <li>In addition to automated backups, manual DB snapshots can be performed at any time.</li>
          <li>A DB snapshot is initiated by you and can be created as frequently as you want.</li>
          <li>You can then restore the DB Instance to the specific state in the DB snapshot at any time.</li>
          <li>DB snapshots can be created with the RDS console or the <code>CreateDBSnapshot</code> action.</li>
          <li>Unlike automated snapshots that are deleted after the retention period, manual DB snapshots are kept until you explicitly delete them with the RDS console or the <code>DeleteDBSnapshot</code> action.</li>
          <li>For busy databases, use Multi-AZ to minimize the performance impact of a snapshot. During the backup window, storage I/O may be suspended while your data is being backed up, and you may experience elevated latency. This I/O suspension typically lasts for the duration of the snapshot. This period of I/O suspension is shorter for Multi-AZ DB deployments because the backup is taken from the standby, but latency can occur during the backup process.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong><em>Recovery</em></strong>
    <ul>
      <li>RDS allows you to recover your database quickly whether you are performing automated backups or manual DB snapshots.</li>
      <li>You cannot restore from a DB snapshot to an existing DB Instance; a new DB Instance is created when you restore.</li>
      <li>When you restore a DB Instance, only the default DB parameter and security groups are associated with the restored instance. As soon as the restore is complete, you should associate any custom DB parameter or security groups used by the instance from which you restored.</li>
      <li>When using automated backups, RDS combines the daily backups performed during your predefined maintenance window in conjunction with transaction logs to enable you to restore your DB Instance to any point during your retention period, typically up to the last 5 minutes.</li>
    </ul>
  </li>
</ul>

<p><img class="right" src="/technology/rds-multi_az.png" width="400" height="400" /></p>

<ul>
  <li><strong><em>High Availability with Multi-AZ</em></strong>
    <ul>
      <li>RDS Multi-AZ deployments allows you to create a database cluster across multiple Availability Zones.</li>
      <li>Setting up a relational database to run in a highly available and fault-tolerant fashion is a challenging task.</li>
      <li>With a single option, RDS can increase the availability of your database using replication.</li>
      <li>Multi-AZ lets you meet the most demanding RPO and RTO targets by using synchronous replication to minimize RPO and fast failover to minimize RTO to minutes.</li>
      <li>Multi-AZ allows you to place a secondary copy of your database in another Availability Zone for disaster recovery purposes.</li>
      <li>Multi-AZ deployments are available for all types of RDS database engines.</li>
      <li>When you create a Multi-AZ DB Instance, a primary instance is created in one Availability Zone and a secondary instance is created in another Availability Zone. You are assigned a database instance endpoint such as : <code>my_app_db.ch6fe7ykq1zd.us-west-2.rds.amazonaws.com</code>. This endpoint is a DNS name that AWS takes responsibility for resolving to a specific IP address. You use this DNS name when creating the connection to your database.</li>
      <li>RDS automatically replicates the data from the master database or primary instance to the slave database or secondary instance using synchronous replication.</li>
      <li>Each Availability Zone runs on its own physically distinct, independent infrastructure and is engineered to be highly reliable.</li>
      <li>RDS detects and automatically recovers from the most common failure scenarios for Multi-AZ deployments so that you can resume database operations as quickly as possible without administrative intervention.</li>
      <li>Multi-AZ deployments are for disaster recovery only; they are not meant to enhance database performance. The standby DB Instance is not available to offline queries from the primary master DB Instance. To improve database performance using multiple DB Instances, use read replicas or other DB caching technologies such as Amazon ElastiCache.</li>
      <li>_Automatic Failover__
        <ul>
          <li>RDS will automatically fail over to the standby instance without user intervention.</li>
          <li>The DNS name remains the same, but the RDS service changes the CNAME to point to the standby.</li>
          <li>The primary DB Instance switches over automatically to the standby replica if there was an Availability Zone service disruption, if the primary DB Instance fails, or if the instance type is changed.</li>
          <li>RDS automatically performs a failover in the event of any of the following:
            <ul>
              <li>Loss of availability in primary Availability Zone</li>
              <li>Loss of network connectivity to primary database</li>
              <li>Compute unit failure on primary database</li>
              <li>Storage failure on primary database</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><em>Manual Failover</em>: You can also perform a manual failover of the DB Instance. Failover between the primary and the secondary instance is fast, and the time automatic failover takes to complete is typically one to two minutes.</li>
    </ul>
  </li>
</ul>

<p><strong>Scaling Up and Out</strong></p>

<ul>
  <li>RDS allows you to scale compute and storage vertically, and for some DB engines, you can scale horizontally.</li>
  <li><strong><em>Vertical Scalability</em></strong>
    <ul>
      <li>RDS makes it easy to scale up or down your database tier to meet the demands of your application.</li>
      <li>Changes can be scheduled to occur during the next maintenance window or to begin immediately using the <code>ModifyDBInstance</code> action.</li>
      <li>To change the amount of compute and memory, you can select a different DB Instance class of the database.</li>
      <li>After you select a larger or smaller DB Instance class, RDS automates the migration process to a new class with only a short disruption and minimal effort.</li>
      <li><em>Storage Expansion</em>
        <ul>
          <li>You can also increase the amount of storage, the storage class, and the storage performance for an RDS Instance.</li>
          <li>Each database instance can scale from 5GB up to 6TB in provisioned storage depending on the storage type and engine.</li>
          <li>Storage for RDS can be increased over time as needs grow with minimal impact to the running database.</li>
          <li>Storage expansion is supported for all of the database engines except for SQL Server.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong><em>Horizontal Scalability with Partitioning</em></strong>
    <ul>
      <li>Partitioning a large relational database into multiple instances or shards is a common technique for handling more requests beyond the capabilities of a single instance.</li>
      <li>Partitioning, or sharding, allows you to scale horizontally to handle more users and requests but requires additional logic in the application layer.</li>
      <li>The application needs to decide how to route database requests to the correct shard and becomes limited in the types of queries that can be performed across server boundaries.</li>
    </ul>
  </li>
  <li><strong><em>Horizontal Scalability with Read Replicas</em></strong>
    <ul>
      <li>Another important scaling technique is to use read replicas to offload read transactions from the primary database and increase the overall number of transactions. RDS supports read replicas that allow you to scale out elastically beyond the capacity constraints of a single DB Instance for read-heavy database workloads.</li>
      <li>Use cases where deploying one or more read replica DB Instances is helpful:
        <ul>
          <li>for read-heavy workloads e.g., blog sites</li>
          <li>Handle read traffic while the source DB Instance is unavailable for backups or scheduled maintenance</li>
          <li>Offload reporting or data warehousing scenarios</li>
        </ul>
      </li>
      <li>Read replicas are currently supported in RDS for MySQL, PostgreSQL, MariaDB, and Aurora.</li>
      <li>RDS uses the MySQL, MariaDB, and PostgreSQL DB engines’ built-in replication functionality to create a special type of DB Instance, called a <em>read replica</em>, from a source DB Instance. Updates made to the source DB Instance are asynchronously copied to the read replica.</li>
      <li>You can create one or more replicas of a database within a single AWS Region or across multiple AWS Regions.</li>
      <li>To enhance your disaster recovery capabilities or reduce global latencies, you can use cross-region read replicas</li>
    </ul>
  </li>
</ul>

<p><strong>Security</strong></p>

<ul>
  <li>Securing your RDS DB Instances and relational databases requires a comprehensive plan that addresses the many layers commonly found in database-driven systems. This includes the infrastructure resources, the database, and the network.</li>
  <li><em>Infrastructure level</em>: Protect access to infrastructure resources using IAM policies limiting the actions to perform. e.g., <code>CreateDBInstance</code> and <code>DeleteDBInstance</code> actions to administrators only.</li>
  <li><em>Network level</em>
    <ul>
      <li><em>VPC</em>: deploy RDS DB Instances into a private subnet within a VPC limiting network access to the DB Instance. Before deploying into a VPC, first create a DB subnet group that predefines which subnets are available for RDS deployments.</li>
      <li><em>Security Groups &amp; ACL</em>: Along with VPC, restrict network access using network ACLs and security groups to limit inbound traffic to a short list of source IP addresses.</li>
    </ul>
  </li>
  <li><em>Database level</em>
    <ul>
      <li>create users and grant them permissions to read and write to your databases.</li>
      <li>create users with strong passwords that you rotate frequently.</li>
    </ul>
  </li>
  <li><em>Data level</em>:
    <ul>
      <li>protect the confidentiality of your data in transit and at rest with multiple encryption capabilities provided with RDS.</li>
      <li>Security features vary slightly from one engine to another, but all engines support some form of in-transit encryption and also at-rest encryption.</li>
      <li>Use SSL to protect data in transit.</li>
      <li>Encryption at rest is possible for all engines using the Amazon Key Management Service (KMS) or Transparent Data Encryption (TDE).</li>
      <li>All logs, backups, and snapshots are encrypted for an encrypted RDS instance.</li>
    </ul>
  </li>
</ul>

<h2 id="amazon-redshift">Amazon Redshift</h2>

<ul>
  <li>Redshift is a fast, powerful, fully managed, petabyte-scale data warehouse service in the cloud.</li>
  <li>Redshift is a relational database designed for OLAP scenarios and optimized for high-performance analysis and reporting of very large datasets.</li>
  <li>Traditional data warehouses are difficult and expensive to manage, especially for large datasets. Redshift lowers the cost of a data warehouse and also makes it easy to analyze large amounts of data very quickly.</li>
  <li>Redshift gives you fast querying capabilities over structured data using standard SQL commands to support interactive querying over large datasets.</li>
  <li>With connectivity via ODBC or JDBC, Redshift integrates well with various data loading, reporting, data mining, and analytics tools.</li>
  <li>Redshift is based on industry-standard PostgreSQL</li>
  <li>Redshift manages the work needed to set up, operate, and scale a data warehouse, from provisioning the infrastructure capacity to automating ongoing administrative tasks such as backups and patching. Redshift automatically monitors your nodes and drives to help you recover from failures.</li>
</ul>

<p><strong>Clusters and Nodes</strong></p>

<p><img class="right" src="/technology/aws-redshift-cluster.png" width="400" height="400" /></p>

<ul>
  <li>A cluster is composed of a leader node and one or more compute nodes.</li>
  <li>The client application interacts directly only with the leader node, and the compute nodes are transparent to external applications.</li>
  <li>Redshift currently has support for 6 different node types and each has a different mix of CPU, memory, and storage. The 6 node types are grouped into two categories:
    <ul>
      <li><em>Dense Compute</em> node types support clusters up to 326TB using fast SSDs</li>
      <li><em>Dense Storage</em> nodes support clusters up to 2PB using large magnetic disks</li>
    </ul>
  </li>
  <li>Each cluster contains one or more databases.</li>
  <li>User data for each table is distributed across the compute nodes. Your application or SQL client communicates with Redshift using standard JDBC or ODBC connections with the leader node, which in turn coordinates query execution with the compute nodes. Your application does not interact directly with the compute nodes.</li>
  <li><em>Slices</em>
    <ul>
      <li>The disk storage for a compute node is divided into a number of slices.</li>
      <li>The number of slices per node depends on the node size of the cluster and typically varies between 2 and 16.</li>
      <li>The nodes all participate in parallel query execution, working on data that is distributed as evenly as possible across the slices.</li>
    </ul>
  </li>
  <li>You can increase query performance by adding multiple nodes to a cluster. When you submit a query, Redshift distributes and executes the query in parallel across all of a cluster’s compute nodes. Redshift also spreads your table data across all compute nodes in a cluster based on a distribution strategy that you specify. This partitioning of data across multiple compute resources allows you to achieve high levels of performance.</li>
  <li>Redshift allows
    <ul>
      <li>to resize a cluster to add storage and compute capacity</li>
      <li>change the node type of a cluster and keep the overall size the same.</li>
    </ul>
  </li>
  <li>During resize, Redshift will create a new cluster and migrate data from the old cluster to the new one. During a resize operation, the database will become read-only until the operation is finished.</li>
</ul>

<p><strong>Table Design</strong></p>

<ul>
  <li>Redshift CREATE TABLE command supports specifying compression encodings, distribution strategy, and sort keys.</li>
  <li>Additional columns can be added to a table using the ALTER TABLE command; however, existing columns cannot be modified.</li>
  <li><em>Compression Encoding</em>
    <ul>
      <li>One of the key performance optimizations used by Redshift is data compression.</li>
      <li>When loading data for the first time into an empty table, Redshift will automatically sample your data and select the best compression scheme for each column.</li>
      <li>Alternatively, you can specify compression encoding on a per-column basis as part of the CREATE TABLE command.</li>
    </ul>
  </li>
  <li><em>Distribution Strategy</em>
    <ul>
      <li>One of the primary decisions when creating a table in Redshift is how to distribute the records across the nodes and slices in a cluster. You can configure the distribution style of a table to give Redshift hints as to how the data should be partitioned to best meet your query patterns. When you run a query, the optimizer shifts the rows to the compute nodes as needed to perform any joins and aggregates.</li>
      <li>The goal in selecting a table distribution style is to minimize the impact of the redistribution step by putting the data where it needs to be before the query is performed.</li>
      <li>The data distribution style that you select for your database has a big impact on query performance, storage requirements, data loading, and maintenance. By choosing the best distribution strategy for each table, you can balance your data distribution and significantly improve overall system performance.</li>
      <li><strong><em>Distribution styles</em></strong>
        <ul>
          <li><em>EVEN distribution</em>: Default option. Results in the data being distributed across the slices in a uniform fashion regardless of the data.</li>
          <li><em>KEY distribution</em>: the rows are distributed according to the values in one column. The leader node will store matching values close together and increase query performance for joins.</li>
          <li><em>ALL distribution</em>: a full copy of the entire table is distributed to every node. This is useful for lookup tables and other large tables that are not updated frequently.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><em>Sort Keys</em>
    <ul>
      <li>While creating a table, specify one or more columns as sort keys.</li>
      <li>Sorting enables efficient handling of range-restricted predicates. If a query uses a range-restricted predicate, the query processor can rapidly skip over large numbers of blocks during table scans.</li>
      <li>The sort keys for a table can be either
        <ul>
          <li>A <em>compound sort key</em> is more efficient when query predicates use a prefix, which is a subset of the sort key columns in order.</li>
          <li>An <em>interleaved sort key</em> gives equal weight to each column in the sort key, so query predicates can use any subset of the columns that make up the sort key, in any order.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><em>Loading Data</em>
    <ul>
      <li>supports INSERT and UPDATE</li>
      <li>Bulk data upload
        <ul>
          <li>use <code>COPY</code> command</li>
          <li>supports multiple types of input data sources</li>
          <li>fastest way to load data from flat files stored in an S3 bucket or from an DynamoDB table</li>
          <li>can read from multiple files from S3 at the same time - can distribute the workload to the nodes and perform the load process in parallel</li>
          <li>After each bulk data load
            <ul>
              <li>run <code>VACUUM</code> command to reorganize the data and reclaim space after deletes.</li>
              <li>recommended to run <code>ANALYZE</code> command to update table statistics.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Data Export: use <code>UNLOAD</code> command - can be used to generate delimited text files and store them in S3.</li>
    </ul>
  </li>
  <li><em>Querying Data</em>
    <ul>
      <li>supports standard SQL</li>
      <li>For complex queries, query plan can be analyzed to better optimize the access pattern.</li>
      <li>Performance of the cluster and specific queries can be monitored using CloudWatch and the Redshift web console.</li>
      <li><strong><em>Workload Management (WLM)</em></strong>
        <ul>
          <li>WLM is used queue and prioritize queries for large clusters supporting many users</li>
          <li>WLM allows you define multiple queues and set the concurrency level for each queue.</li>
          <li>For example, you might want to have one queue set up for long-running queries and limit the concurrency and another queue for short-running queries and allow higher levels of concurrency.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><em>Snapshots</em>
    <ul>
      <li>Similar to RDS, you can create point-in-time snapshots of the cluster.</li>
      <li>A snapshot can then be used to restore a copy or create a clone of your original cluster.</li>
      <li>Snapshots are durably stored internally in S3 by Redshift.</li>
      <li>Redshift supports both automated snapshots and manual snapshots.
        <ul>
          <li><em>automated snapshots</em> periodically takes snapshots of your cluster and keeps a copy for a configurable retention period.</li>
          <li><em>manual snapshots</em> can be shared across regions or even with other AWS accounts - retained until you explicitly delete them.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><em>Security</em>
    <ul>
      <li>Security plan should include controls to protect the infrastructure resources, the database schema, the records in the table, and network access.</li>
      <li><em>Infrastructure level</em>: using IAM policies that limit the actions AWS administrators can perform. e.g., permission to create and manage the lifecycle of a cluster, including scaling, backup, and recovery operations.</li>
      <li><em>Network level</em>: clusters can be deployed within the private IP address space of your VPC to restrict overall network connectivity. Fine-grained network access can be further restricted using security groups and network ACLs at the subnet level.</li>
      <li><em>Database level</em>:
        <ul>
          <li>When initially creating a Redshift cluster, create a master user account and password.</li>
          <li>The master account can be used to log in to the Redshift database and to create more users and groups. Each database user can be granted permission to schemas, tables, and other database objects. These permissions are independent from the IAM policies used to control access to the infrastructure resources and the Redshift cluster configuration.</li>
        </ul>
      </li>
      <li><em>Data level</em>:
        <ul>
          <li>Encryption of data in transit using SSL-encrypted connections, and also encryption of data at rest using multiple techniques.</li>
          <li>To encrypt data at rest, Redshift integrates with KMS and AWS CloudHSM for encryption key management services.</li>
          <li>Encryption at rest and in transit assists in meeting compliance requirements, and provides additional protections for your data.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="amazon-dynamodb">Amazon DynamoDB</h2>

<ul>
  <li>fully managed NoSQL database service - provides fast and low-latency performance that scales with ease.</li>
  <li>Developers can create a table and write an unlimited number of items with consistent latency.</li>
  <li>can provide consistent performance levels by automatically distributing the data and traffic for a table over multiple partitions.</li>
  <li>After you configure a certain read or write capacity, DynamoDB will automatically add enough infrastructure capacity to support the requested throughput levels.</li>
  <li>Read or write capacity can be adjusted as per demand after a table has been created, and DynamoDB will add or remove infrastructure and adjust the internal partitioning accordingly.</li>
  <li>To help maintain consistent, fast performance levels, all table data is stored on high-performance SSD disk drives. Performance metrics, including transactions rates, can be monitored using CloudWatch.</li>
  <li>provides automatic high-availability and durability protections by replicating data across multiple Availability Zones within an AWS Region.</li>
  <li>Applications can connect to the DynamoDB service endpoint and submit requests over HTTP/S to read and write items to a table or even to create and delete tables.</li>
  <li>provides a web service API that accepts requests in JSON format.</li>
</ul>

<p><strong>Data Model</strong></p>

<ul>
  <li>Basic components of the DynamoDB data model include tables, items, and attributes.</li>
  <li>a table is a collection of items and each item is a collection of one or more attributes.</li>
  <li>Each item has a primary key that uniquely identifies the item.</li>
  <li>No limit in number of attributes in an item. However, max item size = 400KB</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
</pre></td><td class="code"><pre><code class=""><span class="line">{
</span><span class="line">Id = 101
</span><span class="line">ProductName = "Book 101 Title"
</span><span class="line">ISBN = "123–1234567890"
</span><span class="line">Authors = [ "Author 1", "Author 2" ]
</span><span class="line">Price = 2.88
</span><span class="line">Dimensions = "8.5 x 11.0 x 0.5"
</span><span class="line">PageCount = 500
</span><span class="line">InPublication = 1
</span><span class="line">ProductCategory = "Book"
</span><span class="line">}</span></code></pre></td></tr></table></div></figure></notextile></div>

<p><strong>Data Types</strong></p>

<ul>
  <li>DynamoDB only requires a primary key attribute.</li>
  <li>When you create a table or a secondary index, you must specify the names and data types of each primary key attribute (partition key and sort key).</li>
  <li>Data types fall into three major categories:
    <ul>
      <li><em>Scalar</em>: represents exactly one value.
        <ul>
          <li><em>String</em>: Text and variable length characters up to 400KB. Supports Unicode with UTF8 encoding</li>
          <li><em>Number</em>: Positive or negative number with up to 38 digits of precision</li>
          <li><em>Binary</em>: Binary data, images, compressed objects up to 400KB in size</li>
          <li><em>Boolean</em>: Binary flag representing a true or false value</li>
          <li><em>Null</em>: Represents a blank, empty, or unknown state. String, Number, Binary, Boolean cannot be empty.</li>
        </ul>
      </li>
      <li><em>Set</em>: represent a unique list of one or more scalar values. Each value must be the same data type. Sets do not guarantee order.
        <ul>
          <li><em>String Set</em></li>
          <li><em>Number Set</em></li>
          <li><em>Binary Set</em></li>
        </ul>
      </li>
      <li><em>Document</em>: represents multiple nested attributes, similar to the structure of a JSON file - supports 2 document types: List and Map. Multiple Lists and Maps can be combined and nested to create complex structures.
        <ul>
          <li><em>List</em>: Each List can be used to store an ordered list of attributes of different data types.</li>
          <li><em>Map</em>: Each Map can be used to store an unordered list of key/value pairs.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Primary Key</strong></p>

<ul>
  <li>DynamoDB supports 2 types of primary keys, and this configuration cannot be changed after a table has been created:</li>
  <li>Primary key must be type string, number, or binary.</li>
  <li><em>Partition Key</em>
    <ul>
      <li>The primary key is made of one attribute, a partition (or hash) key.</li>
      <li>DynamoDB builds an unordered hash index on this primary key attribute.</li>
      <li>partition key is used to distribute the request to the right partition.</li>
    </ul>
  </li>
  <li><em>Partition and Sort Key</em>
    <ul>
      <li>The primary key is made of 2 attributes: partition key + sort (or range) key.</li>
      <li>Each item in the table is uniquely identified by the combination of its partition and sort key values. It is possible for two items to have the same partition key value, but those two items must have different sort key values.</li>
    </ul>
  </li>
</ul>

<p><strong>Provisioned Capacity</strong></p>

<ul>
  <li>When you create an DynamoDB table, you are required to provision a certain amount of read and write capacity to handle your expected workloads.</li>
  <li>Based on your configuration settings, DynamoDB will then provision the right amount of infrastructure capacity to meet your requirements with sustained, low-latency response times.</li>
  <li>Overall capacity is measured in read and write capacity units. These values can later be scaled up or down by using an <code>UpdateTable</code> action.</li>
  <li>Each operation against a DynamoDB table will consume some of the provisioned capacity units.</li>
  <li>The specific amount of capacity units consumed depends largely on the size of the item, but also on other factors.</li>
  <li><em>Read Operations</em>
    <ul>
      <li>For read operations, the amount of capacity consumed also depends on the read consistency selected in the request. E.g., given a table without a local secondary index, you will consume 1 capacity unit if you read an item that is 4KB or smaller.</li>
      <li>To read a 110KB item 110KB, 28 capacity units are consumed. (<code>110 / 4 = 27.5 </code> rounded up to 28).</li>
      <li>For strongly consistent read operations, twice the number of capacity units are consumed. 56 in the above example.</li>
    </ul>
  </li>
  <li><em>Write Operations</em>
    <ul>
      <li>For write operations, 1 capacity is consumed to write an item 1KB or smaller.</li>
    </ul>
  </li>
  <li><em>CloudWatch metrics</em>
    <ul>
      <li>use CloudWatch to monitor the capacity and make scaling decisions. e.g., <code>ConsumedReadCapacityUnits</code> and <code>ConsumedWriteCapacityUnits</code>.</li>
      <li>If you do exceed your provisioned capacity for a period of time, requests will be throttled and can be retried later. You can monitor and alert on the <code>ThrottledRequests</code> metric using CloudWatch to notify you of changing usage patterns.</li>
    </ul>
  </li>
</ul>

<p><strong>Secondary Indexes</strong></p>

<ul>
  <li>A secondary index lets you query the data in the table using an alternate key, in addition to queries against the primary key.</li>
  <li>allow you to search a large table efficiently and avoid an expensive scan operation to find items with specific attributes</li>
  <li>When an item is modified in a table, each secondary index is updated which also consumes write capacity units.</li>
  <li>DynamoDB supports 2 different kinds of indexes:
    <ul>
      <li><em>Global Secondary Index</em>:
        <ul>
          <li>is an index with a partition and sort key that can be different from those on the table.</li>
          <li>can create or delete a global secondary index on a table at any time.</li>
          <li>item updates maintain their own provisioned throughput settings separate from the table</li>
        </ul>
      </li>
      <li><em>Local Secondary Index</em>:
        <ul>
          <li>is an index that has the same partition key attribute as the primary key of the table, but a different sort key.</li>
          <li>can only create a local secondary index when you create a table.</li>
          <li>item updates will consume write capacity units from the main table</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="writing-and-reading-data">Writing and Reading Data</h3>

<p><strong>Writing Items</strong></p>

<ul>
  <li>DynamoDB provides 3 primary API actions to create, update, and delete items</li>
  <li><code>PutItem</code> action
    <ul>
      <li>create a new item with one or more attributes.</li>
      <li>if the primary key already exists, updates an existing item .</li>
      <li>only requires a table name and a primary key; any additional attributes are optional.</li>
    </ul>
  </li>
  <li><code>UpdateItem</code> action
    <ul>
      <li>finds existing items based on the primary key and replace the attributes.</li>
      <li>useful to only update a single attribute and leave the other attributes unchanged.</li>
      <li>can also be used to create items if they don’t already exist.</li>
      <li>also provides support for <em>atomic counters</em> which allows to increment and decrement a value and are guaranteed to be consistent across multiple concurrent requests. E.g., a counter attribute used to track the overall score of a mobile game can be updated by many clients at the same time.</li>
    </ul>
  </li>
  <li><code>DeleteItem</code> action
    <ul>
      <li>remove an item from a table by specifying a primary key.</li>
    </ul>
  </li>
  <li>All 3 actions also support <em>conditional expressions</em> that allow you to perform validation before an action is applied. This can be useful to prevent accidental overwrites or to enforce some type of business logic checks.</li>
</ul>

<p><strong>Reading Items</strong></p>

<ul>
  <li>Retrieved data through a direct lookup  using <code>GetItem</code> action or through a search using the <code>Query</code> or <code>Scan</code> action.</li>
  <li><code>GetItem</code>
    <ul>
      <li><code>GetItem</code> allows you to retrieve an item based on its primary key.</li>
      <li>All (default) or some of the item’s attributes can be queried</li>
      <li>If a PK is composed of a partition key, the entire partition key is needed to retrieve the item.</li>
      <li>If the PK is a composite of a partition key and a sort key, then both the partition and sort key is needed.</li>
      <li>Each call consumes read capacity units based on the size of the item and the consistency option selected.</li>
      <li>By default, <code>GetItem</code> reads are eventually consistent.</li>
      <li>Strongly consistent reads can be requested optionally; this will consume additional read capacity units, but it will return the most up-to-date version of the item.</li>
    </ul>
  </li>
</ul>

<p><strong>Eventual Consistency</strong></p>

<ul>
  <li>By default, <code>GetItem</code> reads are eventually consistent.</li>
  <li>Strongly consistent reads
    <ul>
      <li>can be requested optionally;</li>
      <li>will consume additional read capacity units, but it will return the most up-to-date version of the item.</li>
      <li>A strongly consistent read might be less available in the case of a network delay or outage.</li>
    </ul>
  </li>
</ul>

<p><strong>Batch Operations</strong></p>

<ul>
  <li><code>BatchGetItem</code></li>
  <li><code>BatchWriteItem</code> - up to 25 item creates or updates with a single operation - allows you to minimize the overhead of each individual call when processing large numbers of items.</li>
</ul>

<p><strong>Searching Items</strong></p>

<ul>
  <li><code>Query</code> and <code>Scan</code> actions used to search a table or an index.</li>
  <li><code>Query</code> operation
    <ul>
      <li>is the primary search operation to find items in a table or a secondary index using only primary key attribute values.</li>
      <li>Each Query requires a partition key attribute name and a distinct value to search.</li>
      <li>can optionally provide a sort key value and use a comparison operator to refine the search results.</li>
      <li>Results are automatically sorted by the primary key and are limited to 1MB.</li>
    </ul>
  </li>
  <li><code>Scan</code> operation
    <ul>
      <li>will read every item in a table or a secondary index, then it filters out values to provide the desired result</li>
      <li>By default, a Scan operation returns all of the data attributes for every item in the table or index.</li>
      <li>Each request can return up to 1MB of data.</li>
      <li>Items can be filtered out using expressions, but this can be a resource-intensive operation.</li>
    </ul>
  </li>
  <li>If the result set for a <code>Query</code> or a <code>Scan</code> exceeds 1MB, you can page through the results in 1MB increments.</li>
</ul>

<p><strong>Scaling and Partitioning</strong></p>

<ul>
  <li>DynamoDB tables can scale horizontally through the use of partitions</li>
  <li>Each individual partition represents a unit of compute and storage capacity.</li>
  <li>DynamoDB stores items for a single table across multiple partitions.</li>
  <li>DynamoDB decides which partition to store the item in based on the partition key. The partition key is used to distribute the new item among all of the available partitions, and items with the same partition key will be stored on the same partition.</li>
  <li>As the number of items in a table grows, additional partitions can be added by splitting an existing partition.</li>
  <li>The provisioned throughput configured for a table is also divided evenly among the partitions.</li>
  <li>Provisioned throughput allocated to a partition is entirely dedicated to that partition, and there is no sharing of provisioned throughput across partitions.</li>
  <li>When a table is created, DynamoDB configures the table’s partitions based on the desired read and write capacity.</li>
  <li>Limits
    <ul>
      <li>1 single partition = max 10GB of data</li>
      <li>Max 3,000 read capacity units or 1,000 write capacity units</li>
    </ul>
  </li>
  <li>For partitions that are not fully using their provisioned capacity, DynamoDB provides some burst capacity to handle spikes in traffic.</li>
  <li>A portion of your unused capacity will be reserved to handle bursts for short periods.</li>
  <li>As storage or capacity requirements change, DynamoDB can split a partition to accommodate more data or higher provisioned request rates.</li>
  <li>After a partition is split, however, it cannot be merged back together. Keep this in mind when planning to increase provisioned capacity temporarily and then lower it again.</li>
  <li>With each additional partition added, its share of the provisioned capacity is reduced.</li>
  <li>To achieve the full amount of request throughput provisioned for a table, keep your workload spread evenly across the partition key values.</li>
  <li>Distributing requests across partition key values distributes the requests across partitions. For example, if a table has 10,000 read capacity units configured but all of the traffic is hitting one partition key, you will not be able to get more than the 3,000 maximum read capacity units that one partition can support.</li>
  <li>To maximize DynamoDB throughput, create tables with a partition key that has a large number of distinct values and ensure that the values are requested fairly uniformly. Adding a random element that can be calculated or hashed is one common technique to improve partition distribution.</li>
</ul>

<p><strong>Security</strong></p>

<ul>
  <li>DynamoDB integrates with the IAM service to provide strong control over permissions using policies.</li>
  <li>can create one or more policies that allow or deny specific operations on specific tables.</li>
  <li>can also use conditions to restrict access to individual items or attributes.</li>
  <li>All operations must first be authenticated as a valid user or user session.</li>
  <li>Applications that need to read and write from DynamoDB need to obtain a set of temporary or permanent access control keys. While these keys could be stored in a configuration file, a best practice is for applications running on AWS to use <em>IAM EC2 instance profiles</em> to manage credentials. IAM EC2 instance profiles or roles allow you to avoid storing sensitive keys in configuration files that must then be secured.</li>
  <li>For mobile applications, a best practice is to use a combination of web identity federation with the AWS Security Token Service (AWS STS) to issue temporary keys that expire after a short period.</li>
  <li>DynamoDB also provides support for fine-grained access control that can restrict access to specific items within a table or even specific attributes within an item. For example, you may want to limit a user to only access his or her items within a table and prevent access to items associated with a different user. Using conditions in an IAM policy allows you to restrict which actions a user can perform, on which tables, and to which attributes a user can read or write.</li>
</ul>

<p><strong>DynamoDB Streams</strong></p>

<ul>
  <li>A common requirement for many applications is to keep track of recent changes and then perform some kind of processing on the changed records. DynamoDB Streams makes it easy to get a list of item modifications for the last 24-hour period. For example, you might need to calculate metrics on a rolling basis and update a dashboard, or maybe synchronize two tables or log activity and changes to an audit trail. With DynamoDB Streams, these types of applications become easier to build.</li>
  <li>DynamoDB Streams allows you to extend application functionality without modifying the original application.</li>
  <li>By reading the log of activity changes from the stream, you can build new integrations or support new reporting requirements that weren’t part of the original design.</li>
  <li>Each item change is buffered in a time-ordered sequence or stream that can be read by other applications.</li>
  <li>Changes are logged to the stream in <em>near real-time</em> and allow you to respond quickly or chain together a sequence of events based on a modification.</li>
  <li>Streams can be enabled or disabled for an DynamoDB table using the AWS Management Console, CLI, or SDK.</li>
  <li><em>Stream</em>
    <ul>
      <li>A stream consists of stream records.</li>
      <li>Each stream record represents a single data modification in the DynamoDB table to which the stream belongs.</li>
      <li>Each stream record is assigned a sequence number, reflecting the order in which the record was published to the stream.</li>
    </ul>
  </li>
  <li><em>Shards</em>
    <ul>
      <li>Stream records are organized into groups, also referred to as <em>shards</em>.</li>
      <li>Each shard acts as a container for multiple stream records and contains information on accessing and iterating through the records.</li>
      <li>Shards live for a maximum of 24 hours and, with fluctuating load levels, could be split one or more times before they are eventually closed.</li>
      <li>To build an application that reads from a shard, it is recommended to use the DynamoDB Streams Kinesis Adapter. The Kinesis Client Library (KCL) simplifies the application logic required to process reading records from streams and shards.</li>
    </ul>
  </li>
</ul>

<h2 id="amazon-elasticcache">Amazon ElasticCache</h2>

<ul>
  <li>a web service that simplifies scaling of an in-memory cache in the cloud - supports Memcached and Redis cache engines</li>
</ul>

<hr />

<h1 id="management-tools">Management Tools</h1>

<h2 id="amazon-cloudwatch">Amazon CloudWatch</h2>

<ul>
  <li>monitoring service for AWS cloud resources and the applications running on AWS</li>
  <li>CloudWatch is a service that you can use to monitor your AWS resources and your applications in real time.</li>
  <li>With CloudWatch, you can collect and track metrics, create alarms that send notifications, and make changes to the resources being monitored based on rules you define.</li>
  <li>For example, you might choose to monitor CPU utilization to decide when to add or remove EC2 instances in an application tier. Or, if a particular application-specific metric that is not visible to AWS is the best indicator for assessing your scaling needs, you can perform a PUT request to push that metric into CloudWatch.</li>
  <li>You can then use this custom metric to manage capacity.</li>
  <li>You can specify parameters for a metric over a time period and configure alarms and automated actions when a threshold is reached. CloudWatch supports multiple types of actions such as sending a notification to an SNS topic or executing an Auto Scaling policy.</li>
  <li>
    <p>CloudWatch offers either basic or detailed monitoring for supported AWS products. Basic monitoring sends data points to CloudWatch every five minutes for a limited number of preselected metrics at no charge.</p>
  </li>
  <li>
    <p><em>Detailed monitoring</em> sends data points to CloudWatch every minute and allows data aggregation for an additional charge. If you want to use detailed monitoring, you must enable it—basic is the default.</p>
  </li>
  <li><strong>Read Alert</strong>
    <ul>
      <li>You may have an application that leverages DynamoDB, and you want to know when read requests reach a certain threshold and alert yourself with an email. You can do this by using <code>ProvisionedReadCapacityUnits</code> for the DynamoDB table for which you want to set an alarm. You simply set a threshold value during a number of consecutive periods and then specify email as the notification type. Now, when the threshold is sustained over the number of periods, your specified email will alert you to the read activity.</li>
    </ul>
  </li>
  <li>CloudWatch metrics can be retrieved by performing a GET request. When you use detailed monitoring, you can also aggregate metrics across a length of time you specify. CloudWatch does not aggregate data across regions but can aggregate across Availability Zones within a region.</li>
  <li>AWS provides a rich set of metrics included with each service, but you can also define custom metrics to monitor resources and events AWS does not have visibility into—for example, EC2 instance memory consumption and disk metrics that are visible to the operating system of the EC2 instance but not visible to AWS or application-specific thresholds running on instances that are not known to AWS. CloudWatch supports an API that allows programs and scripts to PUT metrics into CloudWatch as name-value pairs that can then be used to create events and trigger alarms in the same manner as the default CloudWatch metrics.</li>
  <li>CloudWatch Logs can be used to monitor, store, and access log files from EC2 instances, AWS CloudTrail, and other sources. You can then retrieve the log data and monitor in real time for events—for example, you can track the number of errors in your application logs and send a notification if an error rate exceeds a threshold. CloudWatch Logs can also be used to store your logs in S3 or Glacier. Logs can be retained indefinitely or according to an aging policy that will delete older logs as no longer needed.</li>
  <li>A CloudWatch Logs agent is available that provides an automated way to send log data to CloudWatch Logs for EC2 instances running Amazon Linux or Ubuntu. You can use the CloudWatch Logs agent installer on an existing EC2 instance to install and configure the CloudWatch Logs agent. After installation is complete, the agent confirms that it has started and it stays running until you disable it.</li>
  <li>CloudWatch has some limits that you should keep in mind when using the service. Each AWS account is limited to <em>5,000 alarms per AWS account</em>, and metrics data is retained for two weeks by default (at the time of this writing). If you want to keep the data longer, you will need to move the logs to a persistent store like S3 or Glacier. You should familiarize yourself with the limits for CloudWatch in the CloudWatch Developer Guide.</li>
</ul>

<h2 id="aws-cloudformation">AWS CloudFormation</h2>

<ul>
  <li>defines a JSON-based templating language that can be used to describe all the AWS resources needed for a workload. When templates are submitted to the AWS CloudFormation, it will take care of provisioning and configuring those resources in appropriate order</li>
</ul>

<h2 id="aws-cloudtrail">AWS CloudTrail</h2>

<ul>
  <li>a web service that records AWS API calls for an account and delivers log files for audit and review.</li>
</ul>

<h2 id="aws-config">AWS Config</h2>

<ul>
  <li>With this, one can export an inventory of their AWS resources with all configuration details, and determine how a resource was configured at any point in time - enables compliance auditing, security analysis, resource change tracking, and troubleshooting.</li>
</ul>

<hr />

<h1 id="security-and-identity">Security and Identity</h1>

<h2 id="aws-identity-and-access-manager-iam">AWS Identity and Access Manager (IAM)</h2>

<ul>
  <li>
    <p>enables organizations to create and manage AWS users and groups and use permissions to allow and deny their access to AWS resources.</p>
  </li>
  <li>IAM uses traditional identity concepts such as users, groups, and access control policies to control who can use, what services/resources, and how.</li>
  <li>Provides granular control to limit a single user to the ability to perform
    <ul>
      <li>a single action</li>
      <li>on a specific resource</li>
      <li>from a specific IP address</li>
      <li>during a specific time window.</li>
    </ul>
  </li>
  <li>
    <p>Access can be granted whether they are running on-premises or in the cloud.</p>
  </li>
  <li><strong>IAM is not an identity store/authorization system</strong>
    <ul>
      <li>Permissions are to manipulate AWS infrastructure, not for your application.</li>
      <li>This is not a replacement to your on-premises application authentication/authorization.</li>
      <li>If your application identities are based on Active Directory, your on-premises Active Directory can be extended into the cloud.</li>
      <li><em>AWS Directory Service</em> is an Active Directory-compatible directory service that can work on its own or integrate with on-premises Active Directory.</li>
      <li>For mobile app, consider <em>Amazon Cognito</em> for identity management</li>
    </ul>
  </li>
  <li><strong>IAM is not operating system identity management</strong>
    <ul>
      <li>Under the shared responsibility model, you are in control of your operating system console and configuration. Whatever mechanism you currently use to control access to your server infrastructure will continue to work on EC2 instances, whether that is managing individual machine login accounts or a directory service such as Active Directory or LDAP.</li>
      <li>You can run an Active Directory or LDAP server on EC2, or you can extend your on-premises system into the cloud.</li>
      <li>AWS Directory Service will also work well to provide Active Directory functionality in the cloud as a service, whether standalone or integrated with your existing Active Directory.</li>
    </ul>
  </li>
  <li>Authentication Technologies
    <ul>
      <li>For OS Access - use	Active Directory LDAP Machine-specific accounts</li>
      <li>For Application Access - use User Repositories</li>
      <li>For Mobile apps - Amazon Cognito</li>
      <li>For AWS Resources - use	IAM</li>
    </ul>
  </li>
  <li>IAM is controlled through AWS Console, CLI and via AWS SDKs. In addition, the AWS Partner Network (APN) includes a rich ecosystem of tools to manage and extend IAM.</li>
</ul>

<h3 id="principals">Principals</h3>

<ul>
  <li>A principal is an IAM entity that is allowed to interact with AWS resources.</li>
  <li>A principal can be permanent or temporary, and it can represent a human or an application.</li>
  <li>
    <p>3 types of principals: root users, IAM users, and roles/temporary security tokens.</p>
  </li>
  <li><strong>1. Root User</strong>
    <ul>
      <li>Root user is automatically created when creating a new AWS account.</li>
      <li>With single sign-in principal, root user has complete access to all AWS Cloud services and resources in the account.</li>
      <li>Root user persists as long as the account is open</li>
      <li>Root user can be used for both console and programmatic access to AWS resources.</li>
      <li>Best practice: DO NOT use the root user for everyday tasks, even the administrative ones. Instead, use it only to create the first IAM user and then securely lock away the root user credentials.</li>
    </ul>
  </li>
  <li><strong>2. IAM Users</strong>
    <ul>
      <li>Users are persistent identities set up through the IAM service to represent individual people or applications.</li>
      <li>IAM users can be created by principals with IAM administrative privileges.</li>
      <li>Users are permanent entities until an IAM admin deletes them - no expiration period</li>
      <li>Users are an excellent way to enforce the <em>principle of least privilege</em>; i.e., the concept of allowing a person or process interacting with AWS resources to perform exactly the tasks they need but nothing else.</li>
      <li>Users can be associated with very granular policies that define permissions.</li>
    </ul>
  </li>
  <li><strong>3. Roles/Temporary Security Tokens</strong>
    <ul>
      <li>Roles are used to grant specific privileges to specific actors for a set duration of time.</li>
      <li>Actors can be authenticated by AWS or some trusted external system.</li>
      <li>When one of the actors assumes a role, AWS provides the actor with a temporary security token from the AWS Security Token Service (STS) that the actor can use to access AWS Cloud services.</li>
      <li>Requesting a temporary security token requires specifying how long the token will exist before it expires. The range of a temporary security token lifetime is 15 minutes to 36 hours.</li>
      <li>Roles and temporary security tokens enable a number of use cases:</li>
      <li><strong><em>3a. Amazon EC2 Roles</em></strong>
        <ul>
          <li>Grant permissions to applications running on EC2 instance.</li>
          <li>Granting permissions to an application is always tricky, as it usually requires configuring the application with some sort of credential upon installation. This leads to issues around securely storing the credential prior to use, how to access it safely during installation, and how to secure it in the configuration.</li>
          <li>Suppose that an application running on an EC2 instance needs to access an S3 bucket.
            <ul>
              <li>A policy granting permission to read and write that bucket can be created and assigned to an IAM user, and the application can use the access key for that IAM user to access the S3 bucket.</li>
              <li>The problem with this approach is that the access key for the user must be accessible to the application, probably by storing it in some sort of configuration file.</li>
              <li>The process for obtaining the access key and storing it encrypted in the configuration is usually complicated and a hindrance to agile development.</li>
              <li>Additionally, the access key is at risk when being passed around. Finally, when the time comes to rotate the access key, the rotation involves performing that whole process again.</li>
            </ul>
          </li>
          <li>Using IAM roles for Amazon EC2 removes the need to store AWS credentials in a configuration file.</li>
          <li>Alternative solution
            <ul>
              <li>Create an IAM role that grants the required access to the S3 bucket.</li>
              <li>When the Amazon EC2 instance is launched, the role is assigned to the instance.</li>
              <li>When the application running on the instance uses the API to access the S3 bucket, it assumes the role assigned to the instance and obtains a temporary token that it sends to the API.</li>
              <li>The process of obtaining the temporary token and passing it to the API is handled automatically by most of the AWS SDKs, allowing the application to make a call to access the S3 bucket without worrying about authentication.</li>
              <li>This removes any need to store an access key in a configuration file.</li>
              <li>Also, because the API access uses a temporary token, there is no fixed access key that must be rotated.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong><em>3b. Cross-Account Access</em></strong>
        <ul>
          <li>Grant permissions to users from other AWS accounts, whether you control those accounts or not.</li>
          <li>Common use case for IAM roles is to grant access to AWS resources to IAM users in other AWS accounts. These accounts may be other AWS accounts controlled by your company or outside agents like customers or suppliers.</li>
          <li>Set up an IAM role with the permissions you want to grant to users in the other account, then users in the other account can assume that role to access your resources.</li>
          <li>This is highly recommended as a best practice, as opposed to distributing access keys outside your organization.</li>
        </ul>
      </li>
      <li><strong><em>3c. Federation</em></strong>
        <ul>
          <li>Grant permissions to users authenticated by a trusted external system.</li>
          <li>Many organizations already have an identity repository outside of AWS and would rather leverage that repository than create a new and largely duplicate repository of IAM users.</li>
          <li>Similarly, web-based applications may want to leverage web-based identities such as Facebook, Google, or Login with Amazon.</li>
          <li><em>IAM Identity Providers</em> provide the ability to federate these outside identities with IAM and assign privileges to those users authenticated outside of IAM.</li>
          <li><strong>Identity Providers (IdP)</strong> IAM can integrate with 2 different types of outside Identity providers .
            <ul>
              <li><em>OpenID Connect (OIDC)</em>
                <ul>
                  <li>For federating web identities such as Facebook, Google, or Login with Amazon, IAM supports integration via OIDC.</li>
                  <li>This allows IAM to grant privileges to users authenticated with some of the major web-based IdPs.</li>
                </ul>
              </li>
              <li><em>SAML 2.0 (Security Assertion Markup Language)</em>
                <ul>
                  <li>For federating internal identities, such as Active Directory or LDAP, IAM supports integration via SAML.</li>
                  <li>A SAML-compliant IdP such as Active Directory Federation Services (ADFS) is used to federate the internal directory to IAM.</li>
                  <li>Federation works by returning a temporary token associated with a role to the IdP for the authenticated identity to use for calls to the AWS API.</li>
                  <li>The actual role returned is determined via information received from the IdP, either attributes of the user in the on-premises identity store or the user name and authenticating service of the web identity store.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="authentication">Authentication</h3>

<p>3 ways that IAM authenticates a principal:</p>

<ul>
  <li><strong>User Name/Password</strong>
    <ul>
      <li>When a principal represents a human interacting with the console, the human will provide a user name/password pair to verify their identity.</li>
      <li>IAM allows you to create a password policy enforcing password complexity and expiration.</li>
    </ul>
  </li>
  <li><strong>Access Key</strong>
    <ul>
      <li><code>Access Key = Access Key ID (20 chars) + Access Secret Key (40 chars)</code></li>
      <li>When a program is manipulating the AWS infrastructure via the API, it will use these values to sign the underlying REST calls to the services.</li>
      <li>The AWS SDKs and tools handle all the intricacies of signing the REST calls, so using an access key will almost always be a matter of providing the values to the SDK or tool.</li>
    </ul>
  </li>
  <li><strong>Access Key/Session Token</strong>
    <ul>
      <li>When a process operates under an assumed role, the temporary security token provides an access key for authentication.</li>
      <li>In addition to the access key, the token also includes a session token. Calls to AWS must include both the two-part access key and the session token to authenticate.</li>
      <li>It is important to note that when an IAM user is created, it has neither an access key nor a password, and the IAM administrator can set up either or both. This adds an extra layer of security in that console users cannot use their credentials to run a program that accesses your AWS infrastructure.</li>
    </ul>
  </li>
</ul>

<h3 id="authorization">Authorization</h3>

<ul>
  <li>The process of specifying exactly what actions a principal can and cannot perform is called authorization.</li>
  <li>
    <p>Authorization is handled in IAM by defining specific privileges in policies and associating those policies with principals.</p>
  </li>
  <li><strong>Policies</strong>
    <ul>
      <li>A policy is a JSON document that fully defines a set of permissions to access and manipulate AWS resources.</li>
      <li>Policy documents contain one or more permissions, with each permission defining:
        <ul>
          <li><em>Effect</em>: A single word: Allow or Deny.</li>
          <li><em>Service</em>: For what service does this permission apply? Most AWS Cloud services support granting access through IAM, including IAM itself.</li>
          <li><em>Resource</em>:
            <ul>
              <li>The resource value specifies the specific AWS infrastructure for which this permission applies. This is specified as an <strong><em>Amazon Resource Name (ARN)</em></strong>.</li>
              <li>The format for an ARN varies slightly between services, but the basic format is: <code>arn:aws:service:region:account-id:[resourcetype:]resource</code></li>
              <li>For some services, wildcard values are allowed; for instance, an S3 ARN could have a resource of <code>foldername\*</code> to indicate all objects in the specified folder.</li>
              <li>Sample ARN
                <ul>
                  <li>Amazon S3 Bucket:	<code>arn:aws:s3:us-east-1:123456789012:my_corporate_bucket/*</code></li>
                  <li>IAM User:	<code>arn:aws:iam:us-east-1:123456789012:user/David</code></li>
                  <li>Amazon DynamoDB Table:	<code>arn:aws:dynamodb:us-east-1:123456789012:table/tablename</code></li>
                </ul>
              </li>
            </ul>
          </li>
          <li><em>Action</em>:
            <ul>
              <li>The action value specifies the subset of actions within a service that the permission allows or denies. For instance, a permission may grant access to any read-based action for S3.</li>
              <li>A set of actions can be specified with an enumerated list or by using wildcards (Read*).</li>
            </ul>
          </li>
          <li><em>Condition</em>:
            <ul>
              <li>The condition value optionally defines one or more additional restrictions that limit the actions allowed by the permission.</li>
              <li>For instance, the permission might contain a condition that limits the ability to access a resource to calls that come from a specific IP address range.</li>
              <li>Another condition could restrict the permission only to apply during a specific time interval.</li>
              <li>A sample policy is shown in the following listing. This policy allows a principal to list the objects in a specific bucket and to retrieve those objects, but only if the call comes from a specific IP address.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
</pre></td><td class="code"><pre><code class="js"><span class="line"><span class="p">{</span>
</span><span class="line">	<span class="s2">&quot;Version&quot;</span><span class="o">:</span> <span class="s2">&quot;2012–10–17&quot;</span><span class="p">,</span>
</span><span class="line">	<span class="s2">&quot;Statement&quot;</span><span class="o">:</span> <span class="p">[</span>
</span><span class="line">		<span class="p">{</span>
</span><span class="line">			<span class="s2">&quot;Sid&quot;</span><span class="o">:</span> <span class="s2">&quot;Stmt1441716043000&quot;</span><span class="p">,</span>
</span><span class="line">			<span class="s2">&quot;Effect&quot;</span><span class="o">:</span> <span class="s2">&quot;Allow&quot;</span><span class="p">,</span>
</span><span class="line">			<span class="s2">&quot;Action&quot;</span><span class="o">:</span> <span class="p">[</span> <span class="s2">&quot;s3:GetObject&quot;</span><span class="p">,</span> <span class="s2">&quot;s3:ListBucket&quot;</span><span class="p">],</span>
</span><span class="line">			<span class="s2">&quot;Condition&quot;</span><span class="o">:</span> <span class="p">{</span>
</span><span class="line">				<span class="s2">&quot;IpAddress&quot;</span><span class="o">:</span> <span class="p">{</span>
</span><span class="line">					<span class="s2">&quot;aws:SourceIp&quot;</span><span class="o">:</span> <span class="s2">&quot;192.168.0.1&quot;</span>
</span><span class="line">				<span class="p">}</span>
</span><span class="line">			<span class="p">},</span>
</span><span class="line">			<span class="s2">&quot;Resource&quot;</span><span class="o">:</span> <span class="p">[</span>
</span><span class="line">				<span class="s2">&quot;arn:aws:s3:::my_public_bucket/*&quot;</span>
</span><span class="line">			<span class="p">]</span>
</span><span class="line">		<span class="p">}</span>
</span><span class="line">	<span class="p">]</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p><strong>Associating Policies with Principals</strong></p>

<ul>
  <li>A policy can be associated directly with an IAM user in one of two ways:
    <ul>
      <li><em>User Policy</em>:
        <ul>
          <li>These policies exist only in the context of the user to which they are attached.</li>
          <li>In the console, a user policy is entered into the user interface on the IAM user page.</li>
        </ul>
      </li>
      <li><em>Managed Policies</em>:
        <ul>
          <li>Policies are created in the ‘Policies’ tab on the IAM page and exist independently of any individual user.</li>
          <li>Same policy can be associated with many users or groups of users.</li>
          <li>There are a large number of predefined managed policies on the Policies tab of the IAM page in the AWS Console.</li>
          <li>You can write your own policies specific to your use cases.</li>
          <li>
            <ul>
              <li>Using predefined managed policies ensures that when new permissions are added for new features, your users will still have the correct access.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>IAM Groups</strong></p>

<ul>
  <li>Common method for associating policies with users is with the IAM groups feature.</li>
  <li>Groups simplify managing permissions for large numbers of users.</li>
  <li>After a policy is assigned to a group, any user who is a member of that group assumes those permissions.</li>
  <li>This is a much simpler management process than having to review what policies a new IAM user for the operations team should receive and manually adding those policies to the user.</li>
  <li>2 ways a policy can be associated with an IAM group:
    <ul>
      <li><em>Group Policy</em>
        <ul>
          <li>These policies exist only in the context of the group to which they are attached.</li>
          <li>In the AWS Console, a group policy is entered into the user interface on the IAM Group page.</li>
        </ul>
      </li>
      <li><em>Managed Policies</em>
        <ul>
          <li>can be associated with IAM users and IAM groups.</li>
          <li>A good first step is to use the root user to create a new IAM group called “IAM Administrators” and assign the managed policy, “IAMFullAccess.”</li>
          <li>Then create a new IAM user called “Administrator,” assign a password, and add it to the IAM Administrators group. At this point, you can log off as the root user and perform all further administration with the IAM user account.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Assuming a role</strong></p>

<p>The final way an actor can be associated with a policy is by assuming a role. In this case, the actor can be:</p>

<ul>
  <li>An authenticated IAM user (person or process). In this case, the IAM user must have the rights to assume the role.</li>
  <li>A person or process authenticated by a trusted service outside of AWS, such as an on-premises LDAP directory or a web authentication service. In this situation, an AWS Cloud service will assume the role on the actor’s behalf and return a token to the actor.</li>
</ul>

<p>After an actor has assumed a role, it is provided with a temporary security token associated with the policies of that role. The token contains all the information required to authenticate API calls. This information includes a standard access key plus an additional session token required for authenticating calls under an assumed role.</p>

<h3 id="other-key-features">Other Key Features</h3>

<p>Beyond the critical concepts of principals, authentication, and authorization, there are several other features of the IAM service that are important to understand to realize the full benefits of IAM.</p>

<p><strong>Multi-Factor Authentication (MFA)</strong></p>

<ul>
  <li>MFA adds an extra layer of security to your infrastructure by adding a second method of authentication beyond just a password or access key.</li>
  <li>With MFA, authentication also requires entering a <em>One-Time Password (OTP)</em> from a small device. The MFA device can be either a small hardware device you carry with you or a virtual device via an app on your smart phone (for example, the AWS Virtual MFA app).</li>
  <li>MFA requires you to verify your identity with both <strong>something you know</strong> and <strong>something you have</strong>.</li>
  <li>MFA can be assigned to any IAM user account, whether the account represents a person or application.</li>
  <li>When a person using an IAM user configured with MFA attempts to access the AWS  Console, after providing their password they will be prompted to enter the current code displayed on their MFA device before being granted access. An application using an IAM user configured with MFA must query the application user to provide the current code, which the application will then pass to the API.</li>
</ul>

<p><strong>Rotating Keys</strong></p>

<ul>
  <li>It is a security best practice to rotate access keys associated with your IAM users.</li>
  <li>IAM facilitates this process by allowing two active access keys at a time.</li>
  <li>The process to rotate keys can be conducted via the console, CLI, or SDKs:
    <ul>
      <li>Create a new access key for the user.</li>
      <li>Reconfigure all applications to use the new access key.</li>
      <li>Disable the original access key (disabling instead of deleting at this stage is critical, as it allows rollback to the original key if there are issues with the rotation).</li>
      <li>Verify the operation of all applications.</li>
      <li>Delete the original access key.</li>
    </ul>
  </li>
  <li>Access keys should be rotated on a regular schedule.</li>
</ul>

<p><strong>Resolving Multiple Permissions</strong></p>

<p>Occasionally, multiple permissions will be applicable when determining whether a principal has the privilege to perform some action. These permissions may come from multiple policies associated with a principal or resource policies attached to the AWS resource in question. It is important to know how conflicts between these permissions are resolved:</p>

<ul>
  <li>Initially the request is denied by default.</li>
  <li>All the appropriate policies are evaluated; if there is an explicit “deny” found in any policy, the request is denied and evaluation stops.</li>
  <li>If no explicit “deny” is found and an explicit “allow” is found in any policy, the request is allowed.</li>
  <li>If there are no explicit “allow” or “deny” permissions found, then the default “deny” is maintained and the request is denied.</li>
</ul>

<p>The only exception to this rule is if an <em>AssumeRole</em> call includes a role and a policy, the policy cannot expand the privileges of the role (for example, the policy cannot override any permission that is denied by default in the role).</p>

<h2 id="aws-key-management-service-kms">AWS Key Management Service (KMS)</h2>

<ul>
  <li>enables organizations to create and control the encryption keys used to encrypt their data and uses Hardware Security Modules (HSM) to protect the security of the keys.</li>
</ul>

<h2 id="aws-directory-service">AWS Directory Service</h2>

<ul>
  <li>allows organizations to set up and run Microsoft Active Directory on the AWS cloud or connect their AWS resources with an existing on-premises Microsoft Active Directory.</li>
</ul>

<h2 id="aws-certificate-manager">AWS Certificate Manager</h2>

<ul>
  <li>is a service that lets organizations easily provision, manage and deploy SSL/TLS certificates for use with AWS cloud services. It removes the time-consuming manual process of purchasing, uploading and renewing certificates - enables to quickly request a certificate, deploy it on AWS and let AWS Certificate Manager handle certificate renewals</li>
</ul>

<h2 id="aws-web-application-firewall-waf">AWS Web Application Firewall (WAF)</h2>

<ul>
  <li>helps protect web apps from common attacks</li>
</ul>

<hr />

<h1 id="application-services">Application Services</h1>

<h2 id="amazon-api-gateway">Amazon API Gateway</h2>

<ul>
  <li>a fully managed service that makes it easy to create, publish, maintain, monitor and secure APIs at any scale - handles all the tasks involved in accepting and processing up to hundreds of thousands of concurrent API calls, including traffic management, monitoring, authorization and access control, API version management.</li>
</ul>

<h2 id="amazon-elastic-transcoder">Amazon Elastic Transcoder</h2>

<ul>
  <li>media transcoder that converts media files from their source formats into versions that will play back on specific devices</li>
</ul>

<h2 id="amazon-simple-notification-service-sns">Amazon Simple Notification Service (SNS)</h2>

<p><img class="right" src="/technology/aws-sns.jpg" width="400" height="400" /></p>

<ul>
  <li>is a web service for mobile and enterprise messaging that enables you to set up, operate, and send notifications.</li>
  <li>Designed to make web-scale computing easier for developers.</li>
  <li>follows the pub-sub messaging paradigm- no need to poll periodically. e.g., you can send notifications to Apple, Android, Fire OS, and Windows devices.</li>
  <li>In China, you can send messages to Android devices with Baidu Cloud Push.</li>
  <li>can send SMS messages to mobile device users in the United States or to email recipients worldwide.</li>
  <li>messages are delivered to subscribers using different methods, such as Amazon SQS, HTTP, HTTPS, email, SMS, and AWS Lambda.</li>
  <li>When using SNS, you (as the owner) create a topic and control access to it by defining policies that determine which publishers and subscribers can communicate with the topic and via which technologies.</li>
  <li>Publishers send messages to topics that they created or that they have permission to publish to.</li>
</ul>

<p><strong>Common Scenarios</strong></p>

<ul>
  <li>SNS can support a wide variety of needs, including monitoring applications, workflow systems, time-sensitive information updates, mobile applications, and any other application that generates or consumes notifications.</li>
  <li>SNS can be used to relay events in workflow systems among distributed computer applications, move data between data stores, or update records in business systems.</li>
  <li>Event updates and notifications concerning validation, approval, inventory changes, and shipment status are immediately delivered to relevant system components and end users.</li>
  <li>Another example use for SNS is to relay time-critical events to mobile applications and devices.</li>
  <li>SNS is both highly reliable and scalable, and provides significant advantages to developers who build applications that rely on real-time events.</li>
</ul>

<p><img class="right" src="/technology/aws-sns-fanout.jpg" width="400" height="400" /></p>

<ul>
  <li><strong><em>Fanout</em></strong>
    <ul>
      <li>A fanout scenario is when an SNS message is sent to a topic and then replicated and pushed to multiple SQS queues, HTTP endpoints, or email addresses. This allows for parallel asynchronous processing.</li>
      <li>e.g., you can develop an application that sends an SNS message to a topic whenever an order is placed for a product. Then the SQS queues that are subscribed to that topic will receive identical notifications for the new order. An EC2 instance attached to one of the queues handles the processing or fulfillment of the order, while an Amazon EC2 instance attached to a parallel queue sends order data to a data warehouse application/service for analysis.</li>
      <li>Another way to use fanout is to replicate data sent to your production environment and integrate it with your development environment. You can subscribe yet another queue to the same topic for new incoming orders to flow into your development environment to improve and test your application using data received from your production environment.</li>
    </ul>
  </li>
  <li><strong><em>Application and System Alerts</em></strong>
    <ul>
      <li>Application and system alerts are SMS and/or email notifications that are triggered by predefined thresholds. For example, because many AWS Cloud services use SNS, you can receive immediate notification when an event occurs, such as a specific change to your Auto Scaling group in AWS.</li>
    </ul>
  </li>
  <li><strong><em>Push Email and Text Messaging</em></strong>
    <ul>
      <li>Push email and text messaging are two ways to transmit messages to individuals or groups via email and/or SMS.</li>
    </ul>
  </li>
  <li><strong><em>Mobile Push Notifications</em></strong>
    <ul>
      <li>Mobile push notifications enable you to send messages directly to mobile applications.</li>
    </ul>
  </li>
</ul>

<h2 id="amazon-simple-email-service-ses">Amazon Simple Email Service (SES)</h2>

<h2 id="amazon-simple-workflow-service-swf">Amazon Simple Workflow Service (SWF)</h2>

<p><img class="right" src="/technology/aws-swf.jpg" /></p>

<ul>
  <li>helps build, run and scale background jobs that have parallel or sequential steps - has fully managed state tracker and task coordinator on the cloud - provides ability to recover or retry if a task fails.</li>
  <li>SWF makes it easy to build applications that coordinate work across distributed components.</li>
  <li>In SWF, a task represents a logical unit of work that is performed by a component of your application. Coordinating tasks across the application involves managing inter-task dependencies, scheduling, and concurrency in accordance with the logical flow of the application.</li>
  <li>SWF gives you full control over implementing and coordinating tasks without worrying about underlying complexities such as tracking their progress and maintaining their state.</li>
  <li>When using SWF, you implement workers to perform tasks. These workers can run either on cloud infrastructure, such as EC2, or on your own premises.</li>
  <li>You can create long-running tasks that might fail, time out, or require restarts, or tasks that can complete with varying throughput and latency.</li>
  <li>SWF stores tasks, assigns them to workers when they are ready, monitors their progress, and maintains their state, including details on their completion.</li>
  <li>To coordinate tasks, you write a program that gets the latest state of each task from SWF and uses it to initiate subsequent tasks.</li>
  <li>SWF maintains an application’s execution state durably so that the application is resilient to failures in individual components.</li>
  <li>
    <p>With SWF, you can implement, deploy, scale, and modify these application components independently.</p>
  </li>
  <li><strong>Workflows</strong>
    <ul>
      <li>Using SWF, you can implement distributed, asynchronous applications as workflows.</li>
      <li>Workflows coordinate and manage the execution of activities that can be run asynchronously across multiple computing devices and that can feature both sequential and parallel processing.</li>
      <li>When designing a workflow, analyze your application to identify its component tasks, which are represented in SWF as activities.</li>
      <li>The workflow’s coordination logic determines the order in which activities are executed.</li>
    </ul>
  </li>
  <li><strong>Workflow Domains</strong>
    <ul>
      <li>Domains provide a way of scoping SWF resources within your AWS account.</li>
      <li>You must specify a domain for all the components of a workflow, such as the workflow type and activity types.</li>
      <li>It is possible to have more than one workflow in a domain</li>
      <li>Workflows in different domains cannot interact with one another.</li>
    </ul>
  </li>
  <li><strong>Workflow History</strong>
    <ul>
      <li>The workflow history is a detailed, complete, and consistent record of every event that occurred since the workflow execution started.</li>
      <li>An event represents a discrete change in your workflow execution’s state, such as scheduled and completed activities, task timeouts, and signals.</li>
    </ul>
  </li>
  <li><strong>Actors</strong>
    <ul>
      <li>SWF consists of a number of different types of programmatic features known as actors.</li>
      <li>Actors can be workflow starters, deciders, or activity workers. These actors communicate with SWF through its API.</li>
      <li>Actors can be developed in any programming language.</li>
      <li><strong><em>Workflow starter</em></strong>
        <ul>
          <li>is any application that can initiate workflow executions. e.g., a mobile application where a customer orders takeout food or requests a taxi.</li>
          <li>Activities within a workflow can run sequentially, in parallel, synchronously, or asynchronously.</li>
        </ul>
      </li>
      <li><strong><em>Decider</em></strong>:
        <ul>
          <li>The logic that coordinates the tasks in a workflow is called the decider.</li>
          <li>The decider schedules the activity tasks and provides input data to the activity workers.</li>
          <li>The decider also processes events that arrive while the workflow is in progress and closes the workflow when the objective has been completed.</li>
        </ul>
      </li>
      <li><strong><em>Activity Worker</em></strong>
        <ul>
          <li>is a single computer process (or thread) that performs the activity tasks in your workflow.</li>
          <li>Different types of activity workers process tasks of different activity types, and multiple activity workers can process the same type of task.</li>
          <li>When an activity worker is ready to process a new activity task, it polls SWF for tasks that are appropriate for that activity worker. After receiving a task, the activity worker processes the task to completion and then returns the status and result to SWF. The activity worker then polls for a new task.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Tasks</strong>
    <ul>
      <li>SWF provides activity workers and deciders with work assignments, given as one of 3 types of tasks:
        <ul>
          <li>activity tasks,</li>
          <li>AWS Lambda tasks, and</li>
          <li>decision tasks.</li>
        </ul>
      </li>
      <li><strong><em>Activity task</em></strong>
        <ul>
          <li>tells an activity worker to perform its function, such as to check inventory or charge a credit card.</li>
          <li>The activity task contains all the information that the activity worker needs to perform its function.</li>
        </ul>
      </li>
      <li><strong><em>AWS Lambda task</em></strong>
        <ul>
          <li>is similar to an activity task, but executes an AWS Lambda function instead of a traditional SWF activity.</li>
        </ul>
      </li>
      <li><strong><em>Decision Task</em></strong>
        <ul>
          <li>tells a decider that the state of the workflow execution has changed so that the decider can determine the next activity that needs to be performed.</li>
          <li>The decision task contains the current workflow history.</li>
          <li>SWF schedules a decision task when the workflow starts and whenever the state of the workflow changes, such as when an activity task completes.</li>
          <li>Each decision task contains a paginated view of the entire workflow execution history.</li>
          <li>The decider analyzes the workflow execution history and responds back to SWF with a set of decisions that specify what should occur next in the workflow execution. Essentially, every decision task gives the decider an opportunity to assess the workflow and provide direction back to SWF.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Task Lists</strong>
    <ul>
      <li>Task lists provide a way of organizing the various tasks associated with a workflow.</li>
      <li>Task lists are similar to dynamic queues. When a task is scheduled in SWF, you can specify a queue (task list) to put it in. Similarly, when you poll SWF for a task, you determine which queue (task list) to get the task from.</li>
      <li>Task lists provide a flexible mechanism to route tasks to workers.</li>
      <li>Task lists are dynamic in that you don’t need to register a task list or explicitly create it through an action. Ssimply scheduling a task creates the task list if it doesn’t already exist.</li>
    </ul>
  </li>
  <li><strong>Long Polling</strong>
    <ul>
      <li>Deciders and activity workers communicate with SWF using long polling.</li>
      <li>The decider or activity worker periodically initiates communication with SWF, notifying SWF of its availability to accept a task, and then specifies a task list to get tasks from.</li>
      <li>Long polling works well for high-volume task processing.</li>
      <li>Deciders and activity workers can manage their own capacity.</li>
    </ul>
  </li>
  <li><strong>Object Identifiers</strong>
    <ul>
      <li>SWF objects are uniquely identified by workflow type, activity type, decision and activity tasks, and workflow execution:</li>
      <li><strong><em>Workflow Type</em></strong>: A registered workflow type is identified by its domain, name, and version. Workflow types are specified in the call to <code>RegisterWorkflowType</code>.</li>
      <li><strong><em>Activity Type</em></strong>: A registered activity type is identified by its domain, name, and version. Activity types are specified in the call to <code>RegisterActivityType</code>.</li>
      <li><strong><em>Task Token</em></strong>
        <ul>
          <li>Each decision task and activity task is identified by a unique task token.</li>
          <li>The task token is generated by SWF and is returned with other information about the task in the response from <code>PollForDecisionTask</code> or <code>PollForActivityTask</code>.</li>
          <li>is most commonly used by the process that received the task</li>
          <li>a process could pass the token to another process, which could then report the completion or failure of the task.</li>
        </ul>
      </li>
      <li><strong><em>Workflow execution</em></strong>
        <ul>
          <li>A single execution of a workflow is identified by the domain, workflow ID, and run ID.</li>
          <li>The first two are parameters that are passed to <code>StartWorkflowExecution</code>. The run ID is returned by <code>StartWorkflowExecution</code>.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Workflow Execution Closure</strong>
    <ul>
      <li>After you start a workflow execution, it is open.</li>
      <li>An open workflow execution can be closed as completed, canceled, failed, or timed out.</li>
      <li>It can also be continued as a new execution, or it can be terminated.</li>
      <li>The decider, the person administering the workflow, or SWF can close a workflow execution.</li>
    </ul>
  </li>
</ul>

<h2 id="amazon-simple-queue-service-sqs">Amazon Simple Queue Service (SQS)</h2>

<ul>
  <li>managed message queueing service</li>
  <li>With Amazon SQS, you can offload the administrative burden of operating and scaling a highly available messaging cluster while paying a low price for only what you use.</li>
  <li>Most of the time each message will be delivered to your application exactly once, however the system should be <em>idempotent</em> by design (i.e., it must not be adversely affected if it processes the same message more than once).</li>
  <li>The service does not guarantee FIFO delivery of messages. If the system requires that order be preserved, you can place sequencing information in each message to reorder the messages when they are retrieved from the queue.</li>
</ul>

<p><strong>Message Lifecycle</strong></p>

<ul>
  <li><code>Component 1</code> sends <code>Message A</code> to a queue, and the message is redundantly distributed across the SQS servers.</li>
  <li>When <code>Component 2</code> is ready to process a message, it retrieves messages from the queue, and <code>Message A</code> is returned. While <code>Message A</code> is being processed, it remains in the queue and is not returned to subsequently receive requests for the duration of the visibility timeout.</li>
  <li><code>Component 2</code> deletes <code>Message A</code> from the queue to prevent the message from being received and processed again after the visibility timeout expires.</li>
</ul>

<p><strong>Delay Queues and Visibility Timeouts</strong></p>

<ul>
  <li><em>Delay queues</em> allow you to postpone the delivery of new messages in a queue for a specific number of seconds.</li>
  <li>Any message sent to delay queue will be invisible to consumers for the duration of the delay period.</li>
  <li>To create a delay queue, use <code>CreateQueue</code> and set the <code>DelaySeconds</code> attribute to any value between <em>0 and 900 (15 minutes)</em>.</li>
  <li>Existing queue can be turned into a delay queue by using <code>SetQueueAttributes</code> to set the queue’s <code>DelaySeconds</code> attribute (default value = 0).</li>
  <li>Delay queues are similar to visibility timeouts in that both features make messages unavailable to consumers for a specific period of time. The difference is that a delay queue hides a message when it is first added to the queue, whereas a visibility timeout hides a message only after that message is retrieved from the queue.</li>
  <li>When a message is in the queue but is neither delayed nor in a visibility timeout, it is considered to be <em>“in flight.”</em></li>
  <li>Up to 120,000 messages can be <em>in flight</em> at any given time.</li>
  <li>SQS supports up to 12 hours’ maximum visibility timeout.</li>
</ul>

<p><strong>Separate Throughput from Latency</strong></p>

<ul>
  <li>SQS is accessed through HTTP request-response, and a typical request-response takes a bit less than 20ms from EC2.</li>
  <li>This means that from a single thread, you can, on average, issue 50+ API requests per second (a bit fewer for batch API requests since they do more work).</li>
  <li>The throughput scales horizontally, so the more threads and hosts you add, the higher the throughput. Using this scaling model, some AWS customers have queues that process thousands of messages every second.</li>
</ul>

<p><strong>Queue Operations, Unique IDs, and Metadata</strong></p>

<ul>
  <li>SQS Operations: <code>CreateQueue</code>, <code>ListQueues</code>, <code>DeleteQueue</code>, <code>SendMessage</code>, <code>SendMessageBatch</code>, <code>ReceiveMessage</code>, <code>DeleteMessage</code>, <code>DeleteMessageBatch</code>, <code>PurgeQueue</code>, <code>ChangeMessageVisibility</code>, <code>ChangeMessageVisibilityBatch</code>, <code>SetQueueAttributes</code>, <code>GetQueueAttributes</code>, <code>GetQueueUrl</code>, <code>ListDeadLetterSourceQueues</code>, <code>AddPermission</code>, and <code>RemovePermission</code>.</li>
  <li>Only the AWS account owner or an AWS identity that has been granted the proper permissions can perform operations.</li>
  <li>Messages are identified via a globally unique ID that SQS returns when the message is delivered to the queue.</li>
  <li>When you receive a message from the queue, the response includes a receipt handle, which you must provide when deleting the message.</li>
</ul>

<p><strong>Queue and Message Identifiers</strong></p>

<p>SQS uses 3 identifiers:</p>

<ul>
  <li>Queue URLs
    <ul>
      <li>When creating a new queue, the queue name must be unique.</li>
      <li>SQS assigns each queue an identifier called a <em>queue URL</em>, which includes the queue name and other components that SQS determines.</li>
      <li>Whenever you want to perform an action on a queue, you must provide its queue URL.</li>
    </ul>
  </li>
  <li>Message IDs
    <ul>
      <li>SQS assigns each message a unique ID - returned as part of the <code>SendMessage</code> response.</li>
      <li>Max length = 100 characters.</li>
    </ul>
  </li>
  <li>Receipt handles.
    <ul>
      <li>Each time you receive a message from a queue, you receive a receipt handle for that message.</li>
      <li>The handle is associated with the act of receiving the message, not with the message itself.</li>
      <li>Receipt handle is needed to delete a message or to change the message visibility</li>
      <li>You must always receive a message before you can delete it (i.e., you can’t put a message into the queue and then recall it).</li>
      <li>Max length = 1,024 characters.</li>
    </ul>
  </li>
</ul>

<p><strong>Message Attributes</strong></p>

<ul>
  <li>SQS provides support for message attributes.</li>
  <li>Message attributes allow you to provide structured metadata items (such as timestamps, geospatial data, signatures, and identifiers) about the message.</li>
  <li>Message attributes are optional and separate from, but sent along with, the message body.</li>
  <li>The receiver of the message can use this information to help decide how to handle the message without having to process the message body first.</li>
  <li>Max 10 attributes allowed per message.</li>
  <li>can be specified via AWS Console, AWS SDKs, or a query API.</li>
</ul>

<p><strong>Long Polling</strong></p>

<ul>
  <li>When an application queries the SQS queue for messages, it calls the function <code>ReceiveMessage</code>.</li>
  <li><code>ReceiveMessage</code> will check for the existence of a message in the queue and return immediately, either with or without a message.</li>
  <li><code>ReceiveMessage</code> burn CPU cycles and tie up a thread. So calling this in a loop repeatdly is problematic.</li>
  <li>With long polling, you send a <code>WaitTimeSeconds</code> argument to <code>ReceiveMessage</code> of up to 20 seconds.</li>
  <li>If there is no message in the queue, then the call will wait up to <code>WaitTimeSeconds</code> for a message to appear before returning.</li>
  <li>If a message appears before the time expires, the call will return the message right away.</li>
  <li>Long polling drastically reduces the amount of load on your client.</li>
</ul>

<p><strong>Dead Letter Queues</strong></p>

<ul>
  <li>SQS provides support for dead letter queues.</li>
  <li>A dead letter queue is a queue that other (source) queues can target to send messages that for some reason could not be successfully processed.</li>
  <li>Benefit: ability to sideline and isolate the unsuccessfully processed messages, then analyze to determine the cause of failure.</li>
  <li>Messages can be sent to and received from a dead letter queue, just like any other Amazon SQS queue.</li>
  <li>can be created from SQS API and the SQS console.</li>
</ul>

<p><strong>Access Control</strong></p>

<ul>
  <li>While IAM can be used to control the interactions of different AWS identities with queues, there are often times when you will want to expose queues to other accounts. For example:
    <ul>
      <li>to grant another AWS account a particular type of access to your queue (for example, <code>SendMessage</code>).</li>
      <li>to grant another AWS account access to your queue for a specific period of time.</li>
      <li>to grant another AWS account access to your queue only if the requests come from your EC2 instances.</li>
      <li>You want to deny another AWS account access to your queue.</li>
    </ul>
  </li>
  <li>While close coordination between accounts may allow these types of actions through the use of IAM roles, that level of coordination is frequently unfeasible.</li>
  <li>SQS Access Control allows you to assign policies to queues that grant specific interactions to other accounts without that account having to assume IAM roles from your account.</li>
  <li>Sample policy below, gives the developer with AWS account number 12345 the SendMessage permission for the queue named 54321/queue1 in the US East region.</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
</pre></td><td class="code"><pre><code class="js"><span class="line"><span class="p">{</span>
</span><span class="line">	<span class="s2">&quot;Version&quot;</span><span class="o">:</span> <span class="s2">&quot;2012&amp;#x02013;10–17&quot;</span><span class="p">,</span>
</span><span class="line">	<span class="s2">&quot;Id&quot;</span><span class="o">:</span> <span class="s2">&quot;Queue1_Policy_UUID&quot;</span><span class="p">,</span>
</span><span class="line">	<span class="s2">&quot;Statement&quot;</span><span class="o">:</span> <span class="p">[</span>
</span><span class="line">		<span class="p">{</span>
</span><span class="line">			<span class="s2">&quot;Sid&quot;</span><span class="o">:</span><span class="s2">&quot;Queue1_SendMessage&quot;</span><span class="p">,</span>
</span><span class="line">			<span class="s2">&quot;Effect&quot;</span><span class="o">:</span> <span class="s2">&quot;Allow&quot;</span><span class="p">,</span>
</span><span class="line">			<span class="s2">&quot;Principal&quot;</span><span class="o">:</span> <span class="p">{</span>
</span><span class="line">				<span class="s2">&quot;AWS&quot;</span><span class="o">:</span> <span class="s2">&quot;12345&quot;</span>
</span><span class="line">			<span class="p">},</span>
</span><span class="line">			<span class="s2">&quot;Action&quot;</span><span class="o">:</span> <span class="s2">&quot;sqs:SendMessage&quot;</span><span class="p">,</span>
</span><span class="line">			<span class="s2">&quot;Resource&quot;</span><span class="o">:</span> <span class="s2">&quot;arn:aws:sqs:us-east-1:54321:queue1&quot;</span>
</span><span class="line">		<span class="p">}</span>
</span><span class="line">	<span class="p">]</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p><strong>Tradeoff Message Durability and Latency</strong></p>

<ul>
  <li>SQS does not return success to a <code>SendMessage</code> API call until the message is durably stored in SQS. This makes the programming model very simple with no doubt about the safety of messages, unlike the situation with an asynchronous messaging model.</li>
  <li>If you don’t need a durable messaging system, however, you can build an asynchronous, client-side batching on top of SQS libraries that delays enqueue of messages to SQS and transmits a set of messages in a batch.</li>
  <li>Caution: With a client-side batching approach, you could potentially lose messages when your client process or client host dies for any reason.</li>
</ul>

<hr />

<h1 id="terminology">Terminology</h1>

<ul>
  <li>Network perimeter - a boundary between two or more portions of a network. It can refer to the boundary between your VPC and your network, it could refer to the boundary between what you manage versus what AWS manages.</li>
  <li>
    <p>WAF (Web Application Firewall)</p>
  </li>
  <li>NAT (Network Address Translation) instances and NAT Gateways - allows EC2 instances deployed in private subnets to gain Internet access</li>
</ul>

<h1 id="references">References</h1>

<ul>
  <li>Books
    <ul>
      <li>AWS Certified Solution Architect - Study Guide</li>
    </ul>
  </li>
  <li>CloudAcademy.com
    <ul>
      <li><a href="https://cloudacademy.com/amazon-web-services/aws-technical-fundamentals-aws-110-course">AWS Technical Foundation 110</a></li>
      <li>AWS Technical Foundation 120</li>
      <li>AWS Technical Foundation 140</li>
      <li>AWS Technical Foundation 160</li>
      <li>AWS Technical Foundation 180</li>
      <li>AWS Technical Foundation 190</li>
      <li><a href="https://cloudacademy.com/amazon-web-services/foundations-for-solutions-architect-associate-on-aws-course/">Foundations for Solutions Architect Associate on AWS course</a></li>
    </ul>
  </li>
</ul>

  
    <footer>
      
      
        <div class="sharing">
  
  <a href="//twitter.com/share" class="twitter-share-button" data-url="http://fizalihsan.github.io/technology/aws.html" data-via="fizalihsan" data-counturl="http://fizalihsan.github.io/technology/aws.html" >Tweet</a>
  
  
  
</div>

      
    </footer>
  
</article>

</div>

<aside class="sidebar">
  
    <section>

	<br/><br/>
	<ul>
		<h2>Technical</h2>

		<li>
			<h3>Architecture</h3>
			<a href="/technology/architecture.html">Overview</a>  |  <a href="/technology/coding-principles.html">Coding Principles</a>  |  <a href="/technology/design-principles.html">Design Principles</a>  |  <a href="/technology/lambda-architecture.html">Lambda Architecture</a>  |  <a href="/technology/logging-and-monitoring.html">Logging &amp; Monitoring</a>  |  <a href="/technology/patterns.html">Patterns</a>  |  <a href="/technology/sdlc.html">SDLC</a>
			  |  <a href="/technology/scalability.html">Scalability</a>  |  <a href="/technology/security.html">Security</a>  |  <a href="/technology/transaction.html">Transactions</a>
		</li>

		<li>
			<h3>BigData</h3>
			<a href="/technology/bigdata.html">Concepts</a>  |  <a href="/technology/distributedcomputing.html">Distributed Computing</a>  |  <a href="/technology/elasticsearch.html">ElasticSearch</a>  |  <a href="/technology/bigdata-frameworks.html">Frameworks</a>  |  <a href="/technology/hadoop-ecosystem.html">Hadoop Ecosystem</a>  |  <a href="/technology/nosql.html">NoSQL</a>  |  <a href="/technology/concurrency-parallelism.html">Parallel Computing</a>
		</li>

		<li>
			<h3>Cloud Computing</h3>
			<a href="/technology/cloud-computing.html">Concepts</a>  |  <a href="/technology/aws.html">AWS</a>
		</li>

		<li>
			<h3>Database</h3>
			<a href="/technology/rdbms.html">Concepts</a>  |  <a href="/technology/db-design.html">Design</a>  |  <a href="/technology/db-performance.html">Performance</a>  |  <a href="/technology/sql.html">SQL</a>
		</li>

		<li>
			<h3>Functional Programming</h3>
			<a href="/technology/functionalprogramming.html">Concepts</a>
		</li>

		<li>
			<h3>Java</h3>
			<a href="/technology/java.html">Core Concepts</a>  |  <a href="/technology/java-collections.html">Collections</a>  |  <a href="/technology/java-concurrency.html">Concurrency</a>  |  <a href="/technology/java-database.html">Database</a>  |  <a href="/technology/java-jmx.html">JMX</a>  |  <a href="/technology/java-jndi.html">JNDI</a>  |  <a href="/technology/java-io.html">I/O</a>  |  <a href="/technology/java-performance.html">Performance</a>  |  <a href="/technology/java-testing.html">Testing</a>  |  <a href="/technology/java-web.html">Web</a>  |  <a href="/technology/java-newsletters.html">Java Specialist Newsletters</a>
		</li>

		<li>
			<h3>Languages</h3>
			<a href="/technology/groovy.html">Groovy</a>  |  <a href="/technology/python.html">Python</a>  |  <a href="/technology/scala.html">Scala</a>
		</li>

		<li>
			<h3>SOA</h3>
			<a href="/technology/soa.html">Concepts</a>  |  <a href="/technology/camel.html">Camel</a>  |  <a href="/technology/messaging.html">Messaging</a>   |  <a href="/technology/rest-services.html">REST</a>  |  <a href="/technology/soap-services.html">SOAP</a>  |  <a href="/technology/webservices.html">Web Services</a> |  <a href="/technology/xml.html">XML</a>
		</li>

		<li>
			<h3>Spring</h3>
			<a href="/technology/spring-batch.html">Batch</a>  |  <a href="/technology/spring.html">Core</a>  |  <a href="/technology/spring-mvc.html">MVC</a>
		</li>

		<li>
			<h3>Web Programming</h3>
			<a href="/technology/webconcepts.html">Concepts</a> | <a href="/technology/js-ecosystem.html">JS Ecosystem</a>
		</li>

		<li>
			<h3>General Concepts</h3>
			<a href="/technology/algorithms.html">Algorithms</a>  |  <a href="/softskills/brain-teasers.html">Brain Teasers</a>  |  <a href="/technology/datastructures.html">Data Structures</a>  |  <a href="/technology/design-faqs.html">Design FAQs</a>  |  <a href="/technology/networking.html">Networking</a>  |  <a href="/technology/oops.html">OOPS</a>  |  <a href="/technology/os.html">Operating Systems</a>  |  <a href="/technology/vcs.html">VCS</a>  |  <a href="/technology/coding_bibliography.html">Coding Bibliography</a>
		</li>

		<h2>Non-Techninal</h2>

		<li>
			<h3>Data Science</h3>

			<a href="/datascience/datascience.html">Overview</a>  |  <a href="/datascience/statistics.html">Statistics</a>  |  <a href="/datascience/r.html">R</a>
		</li>

		<li>
			<h3>Domain Knowledge</h3>
			<a href="/domain/retirement.html">Retirement</a>
		</li>

		<li>
			<h3>Soft Skills</h3>
			<a href="/softskills/presentation.html">Presentation</a>  |  <a href="/softskills/productivity.html">Productivity</a>  |  <a href="/softskills/written.html">Written Communication</a>
		</li>
	</ul>

  <!-- <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
  </ul> &#8211;>
  <!-- <ul>
    <li><a href="/datascience/statistics.html">Statistics</a></li>
    <li><a href="/datascience/r.html">R Programming</a></li>
  </ul> &#8211;>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/fizalihsan">@fizalihsan</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'fizalihsan',
            count: 0,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>





  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2017 - Mohamed Fizal Ihsan Mohamed -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
